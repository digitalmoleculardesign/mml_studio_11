{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqjbxeS9ZTrD"
   },
   "source": [
    "# Graphs networks on molecules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqHZtPgaZg7t"
   },
   "source": [
    "## Setup custom stuff for the colab (packages, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2860
    },
    "id": "uXp4AeaNTauu",
    "outputId": "7f973fe4-d576-47d4-d441-6c35612aacc0"
   },
   "outputs": [],
   "source": "# Modern setup for PyTorch, PyG, RDKit, and W&B\n# No miniconda installation needed - using native Colab Python\n\n# Install modern packages\n!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n!pip install -q torch-geometric torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.1.0+cu118.html\n!pip install -q pytorch-lightning==2.1.0\n!pip install -q rdkit==2024.3.6\n!pip install -q wandb\n# Pin bayesian-optimization to 1.4.3 to avoid UtilityFunction import error\n# (v2.0.0+ replaced UtilityFunction with AcquisitionFunction)\n!pip install -q bayesian-optimization==1.4.3\n!pip install -q torch-uncertainty\n!pip install -q scikit-learn pandas matplotlib seaborn tqdm\n\nprint('=' * 80)\nprint('Installation complete!')\nprint('=' * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsa9EoHiZqnO"
   },
   "source": [
    "## Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184
    },
    "id": "xFiTTx7MV-xx",
    "outputId": "ae25ff02-a68d-4e1b-f014-46fbc700e42b"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from collections import OrderedDict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Scientific Python stack\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Cheminformatics\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "\n",
    "# Machine Learning / Deep Learning\n",
    "import sklearn\n",
    "from sklearn import model_selection, metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PyTorch ecosystem\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# PyTorch Geometric\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import MessagePassing, global_add_pool, global_mean_pool\n",
    "from torch_geometric.utils import add_self_loops\n",
    "\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# Bayesian optimization\n",
    "from bayes_opt import BayesianOptimization, UtilityFunction\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    pl.seed_everything(seed, workers=True)\n",
    "    \n",
    "set_seed(42)\n",
    "\n",
    "# Gomes Group color palette\n",
    "GOMES_COLORS = {\n",
    "    'teal': '#00D9FF',\n",
    "    'coral': '#FF6B6B', \n",
    "    'navy': '#0A1628',\n",
    "    'light_teal': '#7FECFF',\n",
    "    'light_coral': '#FF9999'\n",
    "}\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette([GOMES_COLORS['teal'], GOMES_COLORS['coral'], GOMES_COLORS['navy']])\n",
    "\n",
    "# Print versions\n",
    "print('RDKit: {}'.format(rdkit.__version__))\n",
    "print('PyTorch: {}'.format(torch.__version__))\n",
    "print('PyTorch Geometric: {}'.format(torch_geometric.__version__))\n",
    "print('PyTorch Lightning: {}'.format(pl.__version__))\n",
    "print('CUDA available: {}'.format(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5TzvybqPPWx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEJAnwEFbPrh"
   },
   "source": [
    "# Task and data: predict solubility (logP) ðŸ’§\n",
    "\n",
    "Datatset source: Delaney's solubility dataset from ESOL:â€‰ Estimating Aqueous Solubility Directly from Molecular Structure\n",
    "(https://pubs.acs.org/doi/10.1021/ci034243x)\n",
    "\n",
    "\n",
    "#### Machine learning and deep learning in a nutshell\n",
    "\n",
    "![](https://github.com/beangoben/chemistry_ml_colab/blob/master/images/ml_dl.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UwXokv8aKTa"
   },
   "source": [
    "## Let's load some molecular data with ðŸ¼s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "mu2cR-u_aGkp",
    "outputId": "596a799c-9180-42b3-c351-f3a76d80f96f"
   },
   "outputs": [],
   "source": [
    "# Load ESOL solubility dataset\n",
    "# Check if data directory exists, if not download from repo\n",
    "import os\n",
    "if not os.path.exists('data/solubility.csv'):\n",
    "    !mkdir -p data\n",
    "    !wget -q https://raw.githubusercontent.com/beangoben/chemistry_ml_colab/master/data/solubility.csv -O data/solubility.csv\n",
    "\n",
    "df = pd.read_csv('data/solubility.csv')\n",
    "print('Dataset shape:', df.shape)\n",
    "print('\\nColumns:', list(df.columns))\n",
    "\n",
    "# Define column names\n",
    "smiles_column = 'smiles'\n",
    "target = 'measured log solubility in mols per litre'\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axIqULda9xkU"
   },
   "source": [
    "## Let's prepare our y data and our splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "fmlN2Lvg9w99",
    "outputId": "c89c5152-ca02-46c3-f2c0-52a48abe0070"
   },
   "outputs": [],
   "source": [
    "# Prepare target values and train/test splits\n",
    "y = df[target].values.reshape(-1, 1)\n",
    "print('Target shape:', y.shape)\n",
    "\n",
    "# Create train/test split with fixed random state for reproducibility\n",
    "indices = df.index.tolist()\n",
    "train_index, test_index = model_selection.train_test_split(\n",
    "    indices, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "print(f'Train samples: {len(train_index)}, Test samples: {len(test_index)}')\n",
    "\n",
    "# Visualize target distribution with Gomes Group colors\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.histplot(y.ravel(), bins=30, kde=True, color=GOMES_COLORS['teal'], ax=ax)\n",
    "ax.set_xlabel(target, fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Distribution of Log Solubility', fontsize=14, fontweight='bold')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Basic statistics\n",
    "print(f'\\nTarget statistics:')\n",
    "print(f'  Mean: {y.mean():.3f}')\n",
    "print(f'  Std: {y.std():.3f}')\n",
    "print(f'  Min: {y.min():.3f}')\n",
    "print(f'  Max: {y.max():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxlIrTjS9qjQ"
   },
   "source": [
    "## Let's prepare out molecular graphs (x)\n",
    "\n",
    "There are two steps to understand:\n",
    "\n",
    "1) Generate atomic and bond features\n",
    "\n",
    "2) For each molecule, compute features and store this information as tensors\n",
    "\n",
    "### Molecular features\n",
    "\n",
    "\n",
    "![](https://github.com/beangoben/chemistry_ml_colab/blob/master/images/mol_tensors.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_VkFP_K9mRq"
   },
   "outputs": [],
   "source": [
    "# Modern molecular featurization using OGB-style approach\n",
    "# Based on: https://github.com/snap-stanford/ogb/blob/master/ogb/utils/features.py\n",
    "\n",
    "# Allowable atom features\n",
    "allowable_features = {\n",
    "    'possible_atomic_num_list': list(range(1, 119)) + ['misc'],\n",
    "    'possible_chirality_list': [\n",
    "        'CHI_UNSPECIFIED',\n",
    "        'CHI_TETRAHEDRAL_CW',\n",
    "        'CHI_TETRAHEDRAL_CCW',\n",
    "        'CHI_OTHER'\n",
    "    ],\n",
    "    'possible_degree_list': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 'misc'],\n",
    "    'possible_formal_charge_list': [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 'misc'],\n",
    "    'possible_numH_list': [0, 1, 2, 3, 4, 5, 6, 7, 8, 'misc'],\n",
    "    'possible_number_radical_e_list': [0, 1, 2, 3, 4, 'misc'],\n",
    "    'possible_hybridization_list': [\n",
    "        'SP', 'SP2', 'SP3', 'SP3D', 'SP3D2', 'misc'\n",
    "    ],\n",
    "    'possible_is_aromatic_list': [False, True],\n",
    "    'possible_is_in_ring_list': [False, True]\n",
    "}\n",
    "\n",
    "# Allowable bond features\n",
    "allowable_bond_features = {\n",
    "    'possible_bond_type_list': [\n",
    "        'SINGLE',\n",
    "        'DOUBLE',\n",
    "        'TRIPLE',\n",
    "        'AROMATIC',\n",
    "        'misc'\n",
    "    ],\n",
    "    'possible_bond_stereo_list': [\n",
    "        'STEREONONE',\n",
    "        'STEREOZ',\n",
    "        'STEREOE',\n",
    "        'STEREOCIS',\n",
    "        'STEREOTRANS',\n",
    "        'STEREOANY',\n",
    "    ],\n",
    "    'possible_is_conjugated_list': [False, True],\n",
    "}\n",
    "\n",
    "\n",
    "def safe_index(l, e):\n",
    "    \"\"\"\n",
    "    Return index of element e in list l. If e is not present, return the last index\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return l.index(e)\n",
    "    except:\n",
    "        return len(l) - 1\n",
    "\n",
    "\n",
    "def atom_to_feature_vector(atom):\n",
    "    \"\"\"\n",
    "    Converts RDKit atom object to feature list of indices\n",
    "    \"\"\"\n",
    "    atom_feature = [\n",
    "        safe_index(allowable_features['possible_atomic_num_list'], atom.GetAtomicNum()),\n",
    "        allowable_features['possible_chirality_list'].index(str(atom.GetChiralTag())),\n",
    "        safe_index(allowable_features['possible_degree_list'], atom.GetTotalDegree()),\n",
    "        safe_index(allowable_features['possible_formal_charge_list'], atom.GetFormalCharge()),\n",
    "        safe_index(allowable_features['possible_numH_list'], atom.GetTotalNumHs()),\n",
    "        safe_index(allowable_features['possible_number_radical_e_list'], atom.GetNumRadicalElectrons()),\n",
    "        safe_index(allowable_features['possible_hybridization_list'], str(atom.GetHybridization())),\n",
    "        allowable_features['possible_is_aromatic_list'].index(atom.GetIsAromatic()),\n",
    "        allowable_features['possible_is_in_ring_list'].index(atom.IsInRing()),\n",
    "    ]\n",
    "    return atom_feature\n",
    "\n",
    "\n",
    "def bond_to_feature_vector(bond):\n",
    "    \"\"\"\n",
    "    Converts RDKit bond object to feature list of indices\n",
    "    \"\"\"\n",
    "    bond_feature = [\n",
    "        safe_index(allowable_bond_features['possible_bond_type_list'], str(bond.GetBondType())),\n",
    "        allowable_bond_features['possible_bond_stereo_list'].index(str(bond.GetStereo())),\n",
    "        allowable_bond_features['possible_is_conjugated_list'].index(bond.GetIsConjugated()),\n",
    "    ]\n",
    "    return bond_feature\n",
    "\n",
    "\n",
    "def atom_feature_vector_to_dict(atom_feature):\n",
    "    \"\"\"Helper function to convert atom feature vector to dictionary\"\"\"\n",
    "    feature_names = ['atomic_num', 'chirality', 'degree', 'formal_charge', \n",
    "                     'num_h', 'num_rad_e', 'hybridization', 'is_aromatic', 'is_in_ring']\n",
    "    return {k: v for k, v in zip(feature_names, atom_feature)}\n",
    "\n",
    "\n",
    "def bond_feature_vector_to_dict(bond_feature):\n",
    "    \"\"\"Helper function to convert bond feature vector to dictionary\"\"\"  \n",
    "    feature_names = ['bond_type', 'bond_stereo', 'is_conjugated']\n",
    "    return {k: v for k, v in zip(feature_names, bond_feature)}\n",
    "\n",
    "\n",
    "# Print feature dimensions\n",
    "print(f'Atom feature dimension: {len(atom_to_feature_vector(Chem.MolFromSmiles(\"C\").GetAtomWithIdx(0)))}')\n",
    "print(f'Bond feature dimension: {len(bond_to_feature_vector(Chem.MolFromSmiles(\"CC\").GetBondWithIdx(0)))}')\n",
    "\n",
    "# Example: Extract features for a simple molecule\n",
    "mol = Chem.MolFromSmiles('CCO')\n",
    "atom_features = [atom_to_feature_vector(atom) for atom in mol.GetAtoms()]\n",
    "print(f'\\nExample molecule: CCO (ethanol)')\n",
    "print(f'Number of atoms: {len(atom_features)}')\n",
    "print(f'First atom features: {atom_features[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6x22lFCAjF-"
   },
   "source": [
    "### Mol Tensorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7E0pHVLa-WWB"
   },
   "outputs": [],
   "source": [
    "def smiles_to_pyg_graph(smiles_string):\n",
    "    \"\"\"\n",
    "    Convert SMILES string to PyTorch Geometric Data object\n",
    "    Returns a graph with node features, edge indices, and edge features\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles_string)\n",
    "    \n",
    "    if mol is None:\n",
    "        return None\n",
    "    \n",
    "    # Get atom features\n",
    "    atom_features_list = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        atom_features_list.append(atom_to_feature_vector(atom))\n",
    "    x = torch.tensor(atom_features_list, dtype=torch.long)\n",
    "    \n",
    "    # Get bond features and edge indices\n",
    "    num_bond_features = 3  # bond type, bond stereo, is_conjugated\n",
    "    \n",
    "    if len(mol.GetBonds()) > 0:  # molecule has bonds\n",
    "        edges_list = []\n",
    "        edge_features_list = []\n",
    "        \n",
    "        for bond in mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "            \n",
    "            edge_feature = bond_to_feature_vector(bond)\n",
    "            \n",
    "            # Add both directions since molecular graphs are undirected\n",
    "            edges_list.append((i, j))\n",
    "            edge_features_list.append(edge_feature)\n",
    "            edges_list.append((j, i))\n",
    "            edge_features_list.append(edge_feature)\n",
    "        \n",
    "        # Convert to tensor format: shape [2, num_edges]\n",
    "        edge_index = torch.tensor(edges_list, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_features_list, dtype=torch.long)\n",
    "    else:  # molecule has no bonds (single atom)\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        edge_attr = torch.empty((0, num_bond_features), dtype=torch.long)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# Example: Convert a few molecules to graph format\n",
    "print('Converting SMILES to PyTorch Geometric graphs...\\n')\n",
    "\n",
    "example_smiles = ['C', 'CCO', 'c1ccccc1']  # methane, ethanol, benzene\n",
    "example_names = ['Methane', 'Ethanol', 'Benzene']\n",
    "\n",
    "for name, smi in zip(example_names, example_smiles):\n",
    "    graph = smiles_to_pyg_graph(smi)\n",
    "    print(f'{name} ({smi}):')\n",
    "    print(f'  Nodes: {graph.num_nodes}')\n",
    "    print(f'  Edges: {graph.num_edges}')\n",
    "    print(f'  Node feature shape: {graph.x.shape}')\n",
    "    print(f'  Edge feature shape: {graph.edge_attr.shape}')\n",
    "    print(f'  Edge index shape: {graph.edge_index.shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQf8ThfDAsLy"
   },
   "source": [
    "### Ready to tensorize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 150
    },
    "id": "AVxffcUQ_KkI",
    "outputId": "7c307911-1afd-4a65-a73a-7f639ed86d08"
   },
   "outputs": [],
   "source": [
    "# Convert all molecules to PyG graphs\n",
    "print('Converting ESOL dataset to PyG graphs...')\n",
    "\n",
    "smiles_list = df[smiles_column].tolist()\n",
    "graph_list = []\n",
    "\n",
    "for i, smi in enumerate(tqdm(smiles_list)):\n",
    "    graph = smiles_to_pyg_graph(smi)\n",
    "    if graph is not None:\n",
    "        # Add target value to graph\n",
    "        graph.y = torch.tensor([y[i, 0]], dtype=torch.float)\n",
    "        graph_list.append(graph)\n",
    "    else:\n",
    "        print(f'Warning: Could not parse SMILES at index {i}: {smi}')\n",
    "\n",
    "print(f'\\nSuccessfully converted {len(graph_list)} molecules to graphs')\n",
    "print(f'Example graph:')\n",
    "print(graph_list[0])\n",
    "\n",
    "# Create train and test graph lists\n",
    "train_graphs = [graph_list[i] for i in train_index if i < len(graph_list)]\n",
    "test_graphs = [graph_list[i] for i in test_index if i < len(graph_list)]\n",
    "\n",
    "print(f'\\nTrain graphs: {len(train_graphs)}')\n",
    "print(f'Test graphs: {len(test_graphs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUjxfn0IA_Hw"
   },
   "source": [
    "# Graph Convolution Networks\n",
    "\n",
    "We need to define:\n",
    "\n",
    "* Input layer, what data are we feeding to the network? (how do we represent a molecular graph)\n",
    "* GraphConvolution Layer , how do we learn a new node-level representation?\n",
    "* GraphEmbedding Layer, how do we project our graph to a vector?\n",
    "* (optionally) FC layers (Neural network), to increase prediction performance.\n",
    "* Prediction layer , aka a Generalized Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M57MItHmCeGs"
   },
   "source": [
    "## Input layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RhhU7Jo6_Nar"
   },
   "outputs": [],
   "source": [
    "# Create PyTorch Dataset for molecular graphs\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "class MoleculeDataset(TorchDataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for molecular graphs\n",
    "    \"\"\"\n",
    "    def __init__(self, graph_list):\n",
    "        self.graphs = graph_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx]\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MoleculeDataset(train_graphs)\n",
    "test_dataset = MoleculeDataset(test_graphs)\n",
    "\n",
    "print(f'Train dataset size: {len(train_dataset)}')\n",
    "print(f'Test dataset size: {len(test_dataset)}')\n",
    "print(f'\\nExample data point:')\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Qqu4Z9jDZIM"
   },
   "source": [
    "## Graph convolution layer\n",
    "\n",
    "\n",
    "![](https://github.com/beangoben/chemistry_ml_colab/blob/master/images/gcn_one.png?raw=true)\n",
    "![](https://github.com/beangoben/chemistry_ml_colab/blob/master/images/gcn_two.png?raw=true)\n",
    "![](https://github.com/beangoben/chemistry_ml_colab/blob/master/images/gcn_layers.png?raw=true)\n",
    "![](https://github.com/beangoben/chemistry_ml_colab/blob/master/images/gcn_connected.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dgZhkb7BE2O"
   },
   "outputs": [],
   "source": [
    "# Graph Convolution Layer using PyTorch Geometric MessagePassing\n",
    "# This replaces the custom TensorFlow GraphConvolution layer\n",
    "\n",
    "class GCNConv(MessagePassing):\n",
    "    \"\"\"\n",
    "    Graph Convolutional Network layer that aggregates neighbor information\n",
    "    Similar to the original TensorFlow implementation but using PyG's MessagePassing\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, edge_dim, activation='relu'):\n",
    "        super(GCNConv, self).__init__(aggr='add')  # Use sum aggregation\n",
    "        \n",
    "        # Combine atom and bond features\n",
    "        self.lin = nn.Linear(in_channels + edge_dim, out_channels)\n",
    "        \n",
    "        # Activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'softplus':\n",
    "            self.activation = nn.Softplus()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        else:\n",
    "            self.activation = nn.Identity()\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        \"\"\"\n",
    "        x: Node feature matrix [num_nodes, in_channels]\n",
    "        edge_index: Graph connectivity [2, num_edges]\n",
    "        edge_attr: Edge feature matrix [num_edges, edge_dim]\n",
    "        \"\"\"\n",
    "        # Add self-loops to edge_index\n",
    "        edge_index, edge_attr = add_self_loops(\n",
    "            edge_index, edge_attr, \n",
    "            fill_value=0.0, \n",
    "            num_nodes=x.size(0)\n",
    "        )\n",
    "        \n",
    "        # Start propagating messages\n",
    "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "        \n",
    "        return self.activation(out)\n",
    "    \n",
    "    def message(self, x_j, edge_attr):\n",
    "        \"\"\"\n",
    "        x_j: Features of source nodes [num_edges, in_channels]\n",
    "        edge_attr: Edge features [num_edges, edge_dim]\n",
    "        \"\"\"\n",
    "        # Concatenate node and edge features\n",
    "        combined = torch.cat([x_j, edge_attr], dim=-1)\n",
    "        return self.lin(combined)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "print('GCN Convolution Layer Example:')\n",
    "print('Input: atom features + edge features')\n",
    "print('Output: updated atom features after message passing')\n",
    "print('\\nThe layer performs:')\n",
    "print('1. Gather features from neighboring atoms')\n",
    "print('2. Concatenate with bond (edge) features') \n",
    "print('3. Transform through linear layer')\n",
    "print('4. Apply activation function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "os-BmQW3FDw-"
   },
   "source": [
    "## GraphEmbedding layer\n",
    "\n",
    "![](https://github.com/beangoben/chemistry_ml_colab/blob/master/images/GCN_emb.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cy-Z8YJyD8L4"
   },
   "outputs": [],
   "source": [
    "# Graph Embedding Layer for global pooling\n",
    "# This replaces the custom TensorFlow GraphEmbedding layer\n",
    "\n",
    "class GraphPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Pools node-level features to graph-level representation\n",
    "    Uses global sum pooling (can also use mean, max, or attention-based pooling)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, embedding_dim, activation='softmax'):\n",
    "        super(GraphPooling, self).__init__()\n",
    "        \n",
    "        self.transform = nn.Linear(in_channels, embedding_dim)\n",
    "        \n",
    "        # Activation function  \n",
    "        if activation == 'softmax':\n",
    "            self.activation = nn.Softmax(dim=-1)\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'softplus':\n",
    "            self.activation = nn.Softplus()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        else:\n",
    "            self.activation = nn.Identity()\n",
    "    \n",
    "    def forward(self, x, batch):\n",
    "        \"\"\"\n",
    "        x: Node features [num_nodes_in_batch, in_channels]\n",
    "        batch: Batch vector [num_nodes_in_batch] indicating which graph each node belongs to\n",
    "        \"\"\"\n",
    "        # Transform node features\n",
    "        x = self.activation(self.transform(x))\n",
    "        \n",
    "        # Global pooling: sum node features for each graph\n",
    "        out = global_add_pool(x, batch)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# Example usage\n",
    "print('Graph Pooling Layer Example:')\n",
    "print('Input: node-level features from all atoms')\n",
    "print('Output: single fixed-size vector per molecule')\n",
    "print('\\nThe layer performs:')\n",
    "print('1. Transform each atom feature through linear layer')\n",
    "print('2. Apply activation function')\n",
    "print('3. Sum (pool) all atom features to get molecule representation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ML2T2heNFnWQ"
   },
   "source": [
    "## Layers.... assemble!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "HUv7eiesFlrg",
    "outputId": "643ef36f-2c57-4442-a238-ca90a97af12d"
   },
   "outputs": [],
   "source": [
    "# Complete GNN Model using PyTorch Lightning\n",
    "# This combines GCN layers, graph pooling, and prediction in one model\n",
    "\n",
    "class MoleculeGNN(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Graph Neural Network for molecular property prediction\n",
    "    Architecture: Atom Embedding -> GCN Layers -> Graph Pooling -> MLP -> Prediction\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_atom_features=9,\n",
    "        num_bond_features=3,\n",
    "        hidden_channels=64,\n",
    "        num_conv_layers=4,\n",
    "        embedding_dim=128,\n",
    "        dropout=0.0,\n",
    "        conv_activation='relu',\n",
    "        pool_activation='softmax',\n",
    "        learning_rate=0.001\n",
    "    ):\n",
    "        super(MoleculeGNN, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Atom feature embedding (convert categorical features to continuous)\n",
    "        self.atom_embedding = nn.Embedding(\n",
    "            num_embeddings=119,  # Max atomic number\n",
    "            embedding_dim=hidden_channels\n",
    "        )\n",
    "        \n",
    "        # GCN layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        in_channels = hidden_channels\n",
    "        \n",
    "        for i in range(num_conv_layers):\n",
    "            out_channels = hidden_channels * (i + 1)\n",
    "            self.convs.append(\n",
    "                GCNConv(in_channels, out_channels, num_bond_features, conv_activation)\n",
    "            )\n",
    "            in_channels = out_channels\n",
    "        \n",
    "        # Graph pooling layer\n",
    "        self.pool = GraphPooling(in_channels, embedding_dim, pool_activation)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Prediction head\n",
    "        self.predictor = nn.Linear(embedding_dim, 1)\n",
    "        \n",
    "        # Normalization (will be set during training)\n",
    "        self.register_buffer('y_mean', torch.tensor([0.0]))\n",
    "        self.register_buffer('y_std', torch.tensor([1.0]))\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        Forward pass through the network\n",
    "        \"\"\"\n",
    "        x, edge_index, edge_attr, batch_idx = batch.x, batch.edge_index, batch.edge_attr, batch.batch\n",
    "        \n",
    "        # Embed atom features (only use atomic number for embedding)\n",
    "        x = self.atom_embedding(x[:, 0])  # Use first feature (atomic number)\n",
    "        \n",
    "        # Convert edge attributes to float\n",
    "        edge_attr = edge_attr.float()\n",
    "        \n",
    "        # Apply GCN layers\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        # Pool to graph-level representation\n",
    "        x = self.pool(x, batch_idx)\n",
    "        \n",
    "        # Predict\n",
    "        out = self.predictor(x)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_pred = self(batch)\n",
    "        y_true = batch.y.view(-1, 1)\n",
    "        \n",
    "        loss = F.mse_loss(y_pred, y_true)\n",
    "        mae = F.l1_loss(y_pred, y_true)\n",
    "        \n",
    "        self.log('train_loss', loss, batch_size=len(batch.y))\n",
    "        self.log('train_mae', mae, batch_size=len(batch.y))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_pred = self(batch)\n",
    "        y_true = batch.y.view(-1, 1)\n",
    "        \n",
    "        loss = F.mse_loss(y_pred, y_true)\n",
    "        mae = F.l1_loss(y_pred, y_true)\n",
    "        \n",
    "        self.log('val_loss', loss, batch_size=len(batch.y), prog_bar=True)\n",
    "        self.log('val_mae', mae, batch_size=len(batch.y), prog_bar=True)\n",
    "        \n",
    "        return {'val_loss': loss, 'val_mae': mae, 'y_pred': y_pred, 'y_true': y_true}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        y_pred = self(batch)\n",
    "        y_true = batch.y.view(-1, 1)\n",
    "        \n",
    "        loss = F.mse_loss(y_pred, y_true)\n",
    "        mae = F.l1_loss(y_pred, y_true)\n",
    "        \n",
    "        self.log('test_loss', loss, batch_size=len(batch.y))\n",
    "        self.log('test_mae', mae, batch_size=len(batch.y))\n",
    "        \n",
    "        return {'test_loss': loss, 'test_mae': mae, 'y_pred': y_pred, 'y_true': y_true}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, \n",
    "            mode='min', \n",
    "            factor=0.5, \n",
    "            patience=10, \n",
    "            min_lr=1e-6\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss'\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "# Create a simple model\n",
    "model = MoleculeGNN(\n",
    "    num_atom_features=9,\n",
    "    num_bond_features=3,\n",
    "    hidden_channels=32,\n",
    "    num_conv_layers=3,\n",
    "    embedding_dim=128,\n",
    "    dropout=0.1,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "print('Model architecture:')\n",
    "print(model)\n",
    "print(f'\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 808
    },
    "id": "5ktYn_VqbhST",
    "outputId": "0d0788c7-018a-4439-ad7d-90793b8d0615"
   },
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "\n",
    "# DataLoader for PyG graphs\n",
    "train_loader = PyGDataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = PyGDataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f'Number of training batches: {len(train_loader)}')\n",
    "print(f'Number of test batches: {len(test_loader)}')\n",
    "\n",
    "# Check a batch\n",
    "example_batch = next(iter(train_loader))\n",
    "print(f'\\nExample batch:')\n",
    "print(f'  Batch size: {example_batch.num_graphs}')\n",
    "print(f'  Total nodes in batch: {example_batch.num_nodes}')\n",
    "print(f'  Total edges in batch: {example_batch.num_edges}')\n",
    "print(f'  Node features shape: {example_batch.x.shape}')\n",
    "print(f'  Edge features shape: {example_batch.edge_attr.shape}')\n",
    "print(f'  Target shape: {example_batch.y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1996
    },
    "id": "yu5_6yscGQ4D",
    "outputId": "525eeee3-0a51-45b4-881a-47e2193e805b"
   },
   "outputs": [],
   "source": [
    "# Training setup with PyTorch Lightning\n",
    "\n",
    "# Create model\n",
    "model = MoleculeGNN(\n",
    "    num_atom_features=9,\n",
    "    num_bond_features=3,\n",
    "    hidden_channels=32,\n",
    "    num_conv_layers=4,\n",
    "    embedding_dim=128,\n",
    "    dropout=0.05,\n",
    "    conv_activation='relu',\n",
    "    pool_activation='softmax',\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "# Setup callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='checkpoints',\n",
    "    filename='gnn-{epoch:02d}-{val_loss:.4f}',\n",
    "    save_top_k=1,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    mode='min',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "# Create trainer (without W&B for simplicity)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback, lr_monitor],\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    log_every_n_steps=10,\n",
    "    enable_progress_bar=True,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print('Starting training...\\n')\n",
    "trainer.fit(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8Co_itwfSrI"
   },
   "source": [
    "### Some training stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 838
    },
    "id": "44AdCwCocJkO",
    "outputId": "ec8233fc-60b5-4359-f923-672af2896d10"
   },
   "outputs": [],
   "source": [
    "# Evaluate model and visualize results\n",
    "# Test the best model\n",
    "test_results = trainer.test(model, test_loader, ckpt_path='best')\n",
    "\n",
    "# Get predictions\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        preds = model(batch)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(batch.y.cpu().numpy())\n",
    "\n",
    "y_pred = np.concatenate(all_preds, axis=0).ravel()\n",
    "y_test = np.concatenate(all_targets, axis=0).ravel()\n",
    "\n",
    "# Calculate metrics\n",
    "r2 = metrics.r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f'Test Set Performance:')\n",
    "print(f'  R2 Score: {r2:.4f}')\n",
    "print(f'  RMSE: {rmse:.4f}')\n",
    "print(f'  MAE: {mae:.4f}')\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Parity plot\n",
    "axes[0].scatter(y_test, y_pred, alpha=0.5, c=GOMES_COLORS['teal'], s=30)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "             'r--', lw=2, label='Perfect prediction')\n",
    "axes[0].set_xlabel('True Log Solubility', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Predicted Log Solubility', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title(f'Parity Plot (RÂ²={r2:.3f})', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals plot\n",
    "residuals = y_test - y_pred\n",
    "axes[1].scatter(y_pred, residuals, alpha=0.5, c=GOMES_COLORS['coral'], s=30)\n",
    "axes[1].axhline(y=0, color='k', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted Log Solubility', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Residuals', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title(f'Residuals Plot (MAE={mae:.3f})', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uFMlUvVSYQK"
   },
   "source": [
    "## A good template for a GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkdnK9qSg0nK"
   },
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"fp_depth\": 5,\n",
    "    \"fp_layer_size\": 270,\n",
    "    \"fp_activation\": \"softmax\",\n",
    "    #\"fp_l1\": 1.9624592815875186e-05,\n",
    "    #\"fp_l2\": 0.00021787629651043338,\n",
    "    \"fp_dropout\": 0.0,\n",
    "    \"fp_batchnorm\": false,\n",
    "    \"fp_merge_mode\": \"residual\",\n",
    "    \"conv_base_layer_size\": 28,\n",
    "    \"conv_layer_size_ratio\": 1.51,\n",
    "    \"conv_activation\": \"relu\",\n",
    "\n",
    "    #\"conv_l1\": 8.217857229994724e-06,\n",
    "    #\"conv_l2\": 0.014418374965506497,\n",
    "    \"conv_dropout\": 0.05227018814431583,\n",
    "    \"conv_batchnorm\": true,\n",
    "    #\"conv_graphpool\": \"none\",\n",
    "    #\"conv_n_layers\": 1,\n",
    "\n",
    "    \"net_depth\": 0,\n",
    "    \"net_base_layer_size\": 128,\n",
    "    \"net_layer_size_ratio\": 1.1,\n",
    "    \"net_activation\": \"relu\",\n",
    "    \"net_use_bias\": false,\n",
    "    \"net_l1\": 0.0,\n",
    "    \"net_l2\": 0.0,\n",
    "    \"net_dropout\": 0.25,\n",
    "    \"net_batchnorm\": false,\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 64,\n",
    "    \"n_folds\": 1,\n",
    "    \"learning_rate\": 0.00016344199030918218,\n",
    "    \"lr_scheduler\": \"decay_restart\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uC_G3zmcSrPo"
   },
   "outputs": [],
   "source": [
    "# Advanced GNN with residual connections and deeper architecture\n",
    "# This implements a more sophisticated model similar to the original cells 31-34\n",
    "\n",
    "class AdvancedMoleculeGNN(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Advanced GNN with:\n",
    "    - Multiple GCN layers with increasing hidden dimensions\n",
    "    - Residual/skip connections between layers\n",
    "    - Graph-level embeddings from multiple layers\n",
    "    - Batch normalization and dropout\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_atom_features=9,\n",
    "        num_bond_features=3,\n",
    "        base_hidden_channels=28,\n",
    "        layer_size_ratio=1.5,\n",
    "        num_conv_layers=5,\n",
    "        embedding_dim=270,\n",
    "        dropout=0.05,\n",
    "        use_batch_norm=True,\n",
    "        conv_activation='relu',\n",
    "        pool_activation='softmax',\n",
    "        learning_rate=0.001\n",
    "    ):\n",
    "        super(AdvancedMoleculeGNN, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Atom embedding\n",
    "        self.atom_embedding = nn.Embedding(119, base_hidden_channels)\n",
    "        \n",
    "        # GCN layers with increasing dimensions\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList() if use_batch_norm else None\n",
    "        self.pools = nn.ModuleList()  # Pool after each conv layer\n",
    "        \n",
    "        in_channels = base_hidden_channels\n",
    "        \n",
    "        for i in range(num_conv_layers):\n",
    "            out_channels = int(base_hidden_channels * (layer_size_ratio ** i))\n",
    "            \n",
    "            self.convs.append(\n",
    "                GCNConv(in_channels, out_channels, num_bond_features, conv_activation)\n",
    "            )\n",
    "            \n",
    "            if use_batch_norm:\n",
    "                self.batch_norms.append(nn.BatchNorm1d(out_channels))\n",
    "            \n",
    "            # Add pooling layer for this level\n",
    "            self.pools.append(\n",
    "                GraphPooling(out_channels, embedding_dim, pool_activation)\n",
    "            )\n",
    "            \n",
    "            in_channels = out_channels\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Final predictor (takes concatenated embeddings from all layers)\n",
    "        self.predictor = nn.Linear(embedding_dim, 1)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        x, edge_index, edge_attr, batch_idx = batch.x, batch.edge_index, batch.edge_attr, batch.batch\n",
    "        \n",
    "        # Embed atom features\n",
    "        x = self.atom_embedding(x[:, 0])\n",
    "        edge_attr = edge_attr.float()\n",
    "        \n",
    "        # Collect embeddings from each layer (residual-like connections)\n",
    "        embeddings = []\n",
    "        \n",
    "        for i, (conv, pool) in enumerate(zip(self.convs, self.pools)):\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            \n",
    "            if self.batch_norms is not None:\n",
    "                x = self.batch_norms[i](x)\n",
    "            \n",
    "            x = self.dropout(x)\n",
    "            \n",
    "            # Get graph-level embedding at this layer\n",
    "            emb = pool(x, batch_idx)\n",
    "            embeddings.append(emb)\n",
    "        \n",
    "        # Sum embeddings from all layers (residual-like aggregation)\n",
    "        final_embedding = torch.stack(embeddings).sum(dim=0)\n",
    "        \n",
    "        # Predict\n",
    "        out = self.predictor(final_embedding)\n",
    "        \n",
    "        return out, final_embedding  # Return embedding for visualization\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_pred, _ = self(batch)\n",
    "        y_true = batch.y.view(-1, 1)\n",
    "        \n",
    "        loss = F.mse_loss(y_pred, y_true)\n",
    "        mae = F.l1_loss(y_pred, y_true)\n",
    "        \n",
    "        self.log('train_loss', loss, batch_size=len(batch.y))\n",
    "        self.log('train_mae', mae, batch_size=len(batch.y))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_pred, _ = self(batch)\n",
    "        y_true = batch.y.view(-1, 1)\n",
    "        \n",
    "        loss = F.mse_loss(y_pred, y_true)\n",
    "        mae = F.l1_loss(y_pred, y_true)\n",
    "        \n",
    "        self.log('val_loss', loss, batch_size=len(batch.y), prog_bar=True)\n",
    "        self.log('val_mae', mae, batch_size=len(batch.y), prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        y_pred, _ = self(batch)\n",
    "        y_true = batch.y.view(-1, 1)\n",
    "        \n",
    "        loss = F.mse_loss(y_pred, y_true)\n",
    "        mae = F.l1_loss(y_pred, y_true)\n",
    "        \n",
    "        self.log('test_loss', loss, batch_size=len(batch.y))\n",
    "        self.log('test_mae', mae, batch_size=len(batch.y))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=10, min_lr=1e-6\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {'scheduler': scheduler, 'monitor': 'val_loss'}\n",
    "        }\n",
    "\n",
    "\n",
    "print('Advanced GNN Model with:')\n",
    "print('- Residual-like connections')\n",
    "print('- Multi-scale embeddings')\n",
    "print('- Batch normalization')\n",
    "print('- Increasing layer dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PXwDLxzBA6FH"
   },
   "outputs": [],
   "source": [
    "# Train the advanced model (optional - can skip for faster notebook execution)\n",
    "# Uncomment to train:\n",
    "\n",
    "# advanced_model = AdvancedMoleculeGNN(\n",
    "#     base_hidden_channels=28,\n",
    "#     layer_size_ratio=1.5,\n",
    "#     num_conv_layers=5,\n",
    "#     embedding_dim=270,\n",
    "#     dropout=0.05,\n",
    "#     use_batch_norm=True,\n",
    "#     learning_rate=0.0001\n",
    "# )\n",
    "# \n",
    "# advanced_trainer = pl.Trainer(\n",
    "#     max_epochs=150,\n",
    "#     callbacks=[checkpoint_callback, early_stop_callback, lr_monitor],\n",
    "#     accelerator='auto',\n",
    "#     devices=1,\n",
    "#     enable_progress_bar=True,\n",
    "#     deterministic=True\n",
    "# )\n",
    "# \n",
    "# advanced_trainer.fit(advanced_model, train_loader, test_loader)\n",
    "# advanced_test_results = advanced_trainer.test(advanced_model, test_loader, ckpt_path='best')\n",
    "\n",
    "print('Advanced model training cell (commented out for speed)')\n",
    "print('Uncomment to train a deeper, more sophisticated model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79-HdP54BITN"
   },
   "outputs": [],
   "source": [
    "# Quick performance check with the basic model\n",
    "model.eval()\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        preds, _ = model(batch)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "\n",
    "y_pred = np.concatenate(all_preds, axis=0).ravel()\n",
    "r2 = metrics.r2_score(y_test, y_pred)\n",
    "print(f'Basic model RÂ² score: {r2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vllF4fCJStkl"
   },
   "source": [
    "## Graph embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2u8Ovb3KSvhG"
   },
   "outputs": [],
   "source": [
    "# Extract graph embeddings from the trained model\n",
    "print('Extracting graph embeddings...')\n",
    "\n",
    "# Create a dataloader for all molecules\n",
    "all_loader = PyGDataLoader(graph_list, batch_size=64, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "all_embeddings = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(all_loader):\n",
    "        _, embeddings = model(batch)\n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "        all_targets.append(batch.y.cpu().numpy())\n",
    "\n",
    "embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "targets = np.concatenate(all_targets, axis=0).ravel()\n",
    "\n",
    "print(f'\\nEmbeddings shape: {embeddings.shape}')\n",
    "print(f'Embedding dimension: {embeddings.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTW4BBwjCNa1"
   },
   "outputs": [],
   "source": [
    "# Visualize embeddings with PCA\n",
    "print('Performing PCA dimensionality reduction...')\n",
    "\n",
    "# Standardize embeddings\n",
    "scaler = StandardScaler()\n",
    "embeddings_scaled = scaler.fit_transform(embeddings)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings_scaled)\n",
    "\n",
    "print(f'PCA explained variance ratio: {pca.explained_variance_ratio_}')\n",
    "print(f'Total variance explained: {pca.explained_variance_ratio_.sum():.3f}')\n",
    "\n",
    "# Create visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    embeddings_2d[:, 0], \n",
    "    embeddings_2d[:, 1], \n",
    "    c=targets, \n",
    "    cmap='viridis', \n",
    "    s=30, \n",
    "    alpha=0.6,\n",
    "    edgecolors='k',\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax.set_title('Molecular Embeddings in 2D (PCA)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('Log Solubility', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nThe GNN has learned to organize molecules by their solubility!')\n",
    "print('Similar molecules cluster together in the embedding space.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ir_qJKF5Sw9h"
   },
   "source": [
    "# On Graph neural networks\n",
    "\n",
    "![](https://github.com/beangoben/chemistry_ml_colab/blob/master/images/graph_atributes.png?raw=true)\n",
    "\n",
    "![](https://github.com/beangoben/chemistry_ml_colab/blob/master/images/gn.png?raw=true)\n",
    "\n",
    "https://rusty1s.github.io/pytorch_geometric/build/html/modules/nn.html#torch_geometric.nn.meta.MetaLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XxHThPbvSwIv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-DlaTH1cF87"
   },
   "source": [
    "## How to tune the best architecture\n",
    "\n",
    "![](https://github.com/beangoben/chemistry_ml_colab/blob/master/images/bayesopt.gif?raw=true)\n",
    "\n",
    "\n",
    "* https://github.com/fmfn/BayesianOptimization/blob/master/examples/visualization.ipynb\n",
    "* https://github.com/SheffieldML/GPyOpt\n",
    "* https://github.com/automl/pysmac\n",
    "* http://sheffieldml.github.io/GPyOpt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viQ3gQwzcGT-"
   },
   "outputs": [],
   "source": [
    "# Bayesian Optimization utilities (modernized)\n",
    "# Using the latest bayesian-optimization package\n",
    "\n",
    "from matplotlib import gridspec\n",
    "\n",
    "def posterior(optimizer, x_obs, y_obs, grid):\n",
    "    \"\"\"Calculate posterior mean and std from GP\"\"\"\n",
    "    optimizer._gp.fit(x_obs, y_obs)\n",
    "    mu, sigma = optimizer._gp.predict(grid, return_std=True)\n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "def plot_gp(optimizer, x, y):\n",
    "    \"\"\"\n",
    "    Visualize Gaussian Process and Utility Function\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    steps = len(optimizer.space)\n",
    "    fig.suptitle(\n",
    "        f'Gaussian Process and Utility Function After {steps} Steps',\n",
    "        fontdict={'size': 24, 'weight': 'bold'}\n",
    "    )\n",
    "    \n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[3, 1])\n",
    "    axis = plt.subplot(gs[0])\n",
    "    acq = plt.subplot(gs[1])\n",
    "    \n",
    "    # Get observations\n",
    "    x_obs = np.array([[res[\"params\"][\"x\"]] for res in optimizer.res])\n",
    "    y_obs = np.array([res[\"target\"] for res in optimizer.res])\n",
    "    \n",
    "    # Calculate posterior\n",
    "    mu, sigma = posterior(optimizer, x_obs, y_obs, x)\n",
    "    \n",
    "    # Plot target function\n",
    "    axis.plot(x, y, linewidth=3, label='Target Function', color=GOMES_COLORS['navy'])\n",
    "    \n",
    "    # Plot observations\n",
    "    axis.plot(x_obs.flatten(), y_obs, 'D', markersize=10, \n",
    "             label='Observations', color=GOMES_COLORS['coral'])\n",
    "    \n",
    "    # Plot GP prediction\n",
    "    axis.plot(x, mu, '--', color='k', linewidth=2, label='GP Mean')\n",
    "    \n",
    "    # Plot confidence interval\n",
    "    axis.fill(\n",
    "        np.concatenate([x, x[::-1]]),\n",
    "        np.concatenate([mu - 1.96 * sigma, (mu + 1.96 * sigma)[::-1]]),\n",
    "        alpha=0.3, fc=GOMES_COLORS['teal'], ec='None', \n",
    "        label='95% Confidence Interval'\n",
    "    )\n",
    "    \n",
    "    axis.set_xlim((-2, 10))\n",
    "    axis.set_ylim((None, None))\n",
    "    axis.set_ylabel('f(x)', fontdict={'size': 16})\n",
    "    axis.set_xlabel('x', fontdict={'size': 16})\n",
    "    axis.legend(loc=2, bbox_to_anchor=(1.01, 1), borderaxespad=0., fontsize=12)\n",
    "    axis.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot acquisition function\n",
    "    utility_function = UtilityFunction(kind=\"ucb\", kappa=5, xi=0)\n",
    "    utility = utility_function.utility(x, optimizer._gp, 0)\n",
    "    \n",
    "    acq.plot(x, utility, label='Utility Function', \n",
    "            color=GOMES_COLORS['navy'], linewidth=2)\n",
    "    acq.plot(x[np.argmax(utility)], np.max(utility), '*', markersize=20,\n",
    "            label='Next Sample Point', markerfacecolor='gold', \n",
    "            markeredgecolor='k', markeredgewidth=2)\n",
    "    \n",
    "    acq.set_xlim((-2, 10))\n",
    "    acq.set_ylim((0, np.max(utility) + 0.5))\n",
    "    acq.set_ylabel('Utility', fontdict={'size': 16})\n",
    "    acq.set_xlabel('x', fontdict={'size': 16})\n",
    "    acq.legend(loc=2, bbox_to_anchor=(1.01, 1), borderaxespad=0., fontsize=12)\n",
    "    acq.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "print('Bayesian Optimization utilities loaded')\n",
    "print('Functions: posterior(), plot_gp()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nj9vF4CFm7uT"
   },
   "source": [
    "## black box function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-UxhO9phmjk5"
   },
   "outputs": [],
   "source": [
    "# Define black box function to optimize\n",
    "def target(x):\n",
    "    \"\"\"\n",
    "    Complex function with multiple peaks\n",
    "    Goal: Find the global maximum\n",
    "    \"\"\"\n",
    "    return np.exp(-(x - 2)**2) + np.exp(-(x - 6)**2/10) + 1/ (x**2 + 1)\n",
    "\n",
    "\n",
    "print('Black box function defined: f(x) with multiple peaks')\n",
    "print('Goal: Find x that maximizes f(x) in range [-2, 10]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58evMXG8m9TX"
   },
   "source": [
    "### lets take a gander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "Ay2EhOzwmk3I",
    "outputId": "0703e68a-fb90-4a75-deb3-85f343696bf6"
   },
   "outputs": [],
   "source": [
    "# Visualize the target function\n",
    "x = np.linspace(-2, 10, 10000).reshape(-1, 1)\n",
    "y = target(x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(x, y, linewidth=3, color=GOMES_COLORS['teal'])\n",
    "ax.set_xlabel('x', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('f(x)', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Target Function to Optimize', fontsize=16, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Global maximum appears to be around x â‰ˆ 2.0')\n",
    "print(f'True maximum: f({x[np.argmax(y)][0]:.3f}) = {y.max():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "id": "9XvQmel6mrQ8",
    "outputId": "c9870b01-9469-4ac8-f896-16ab282600d1"
   },
   "outputs": [],
   "source": [
    "# Initialize Bayesian Optimization\n",
    "optimizer = BayesianOptimization(\n",
    "    f=target,\n",
    "    pbounds={'x': (-2, 10)},\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Start with one random sample\n",
    "optimizer.maximize(init_points=1, n_iter=0, kappa=5)\n",
    "\n",
    "print('\\nBayesian Optimization initialized with 1 random sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dcflp6OBm1Fn"
   },
   "source": [
    "## Iterative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "id": "N6_Nfq7bm0Eu",
    "outputId": "96268576-960b-4a7b-c5f7-c771a45878d8"
   },
   "outputs": [],
   "source": [
    "# Iterative Bayesian Optimization\n",
    "# Run multiple iterations and visualize the process\n",
    "\n",
    "print('Running Bayesian Optimization iterations...\\n')\n",
    "\n",
    "for i in range(5):\n",
    "    # Perform one iteration\n",
    "    optimizer.maximize(init_points=0, n_iter=1, kappa=5)\n",
    "    \n",
    "    # Plot current state\n",
    "    print(f'\\n--- After {len(optimizer.space)} evaluations ---')\n",
    "    plot_gp(optimizer, x, y)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print current best\n",
    "    best_params = optimizer.max['params']\n",
    "    best_target = optimizer.max['target']\n",
    "    print(f'Current best: x = {best_params[\"x\"]:.4f}, f(x) = {best_target:.4f}')\n",
    "\n",
    "print('\\nBayesian Optimization complete!')\n",
    "print(f'Final best: x = {optimizer.max[\"params\"][\"x\"]:.4f}')\n",
    "print(f'Final best value: f(x) = {optimizer.max[\"target\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcUhuSrCP5zN"
   },
   "source": [
    "## Uncertainty quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 874
    },
    "id": "T3r7sbUfm4HV",
    "outputId": "e0a9e532-34a3-4721-a57f-bad685e13106"
   },
   "outputs": [],
   "source": [
    "# Uncertainty Quantification through Model Ensembles\n",
    "# Train multiple models with different hyperparameters\n",
    "\n",
    "def create_and_train_model(hparams, train_loader, test_loader, max_epochs=50):\n",
    "    \"\"\"\n",
    "    Helper function to create and train a model with specific hyperparameters\n",
    "    \"\"\"\n",
    "    model = MoleculeGNN(\n",
    "        num_atom_features=9,\n",
    "        num_bond_features=3,\n",
    "        hidden_channels=hparams['hidden_channels'],\n",
    "        num_conv_layers=hparams['num_conv_layers'],\n",
    "        embedding_dim=hparams['embedding_dim'],\n",
    "        dropout=hparams['dropout'],\n",
    "        conv_activation=hparams['conv_activation'],\n",
    "        pool_activation=hparams['pool_activation'],\n",
    "        learning_rate=hparams['learning_rate']\n",
    "    )\n",
    "    \n",
    "    # Quick training without verbose output\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator='auto',\n",
    "        devices=1,\n",
    "        enable_progress_bar=False,\n",
    "        enable_model_summary=False,\n",
    "        logger=False,\n",
    "        enable_checkpointing=False\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model, train_loader, test_loader)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Define ensemble hyperparameters (varied configurations)\n",
    "ensemble_hparams = [\n",
    "    {'hidden_channels': 32, 'num_conv_layers': 3, 'embedding_dim': 128, \n",
    "     'dropout': 0.0, 'conv_activation': 'relu', 'pool_activation': 'softplus', \n",
    "     'learning_rate': 0.001},\n",
    "    \n",
    "    {'hidden_channels': 48, 'num_conv_layers': 4, 'embedding_dim': 150, \n",
    "     'dropout': 0.1, 'conv_activation': 'relu', 'pool_activation': 'sigmoid', \n",
    "     'learning_rate': 0.0005},\n",
    "    \n",
    "    {'hidden_channels': 40, 'num_conv_layers': 3, 'embedding_dim': 140, \n",
    "     'dropout': 0.05, 'conv_activation': 'relu', 'pool_activation': 'softmax', \n",
    "     'learning_rate': 0.002},\n",
    "    \n",
    "    {'hidden_channels': 36, 'num_conv_layers': 4, 'embedding_dim': 120, \n",
    "     'dropout': 0.15, 'conv_activation': 'relu', 'pool_activation': 'sigmoid', \n",
    "     'learning_rate': 0.001},\n",
    "    \n",
    "    {'hidden_channels': 44, 'num_conv_layers': 3, 'embedding_dim': 160, \n",
    "     'dropout': 0.08, 'conv_activation': 'relu', 'pool_activation': 'tanh', \n",
    "     'learning_rate': 0.0015},\n",
    "]\n",
    "\n",
    "print(f'Training ensemble of {len(ensemble_hparams)} models...')\n",
    "print('This may take a few minutes...\\n')\n",
    "\n",
    "ensemble_models = []\n",
    "ensemble_scores = []\n",
    "\n",
    "for i, hparams in enumerate(tqdm(ensemble_hparams)):\n",
    "    model = create_and_train_model(hparams, train_loader, test_loader, max_epochs=30)\n",
    "    ensemble_models.append(model)\n",
    "    \n",
    "    # Evaluate model\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            pred, _ = model(batch)\n",
    "            preds.append(pred.cpu().numpy())\n",
    "    \n",
    "    y_pred = np.concatenate(preds, axis=0).ravel()\n",
    "    r2 = metrics.r2_score(y_test, y_pred)\n",
    "    ensemble_scores.append(r2)\n",
    "    \n",
    "    print(f'Model {i+1}: RÂ² = {r2:.4f}')\n",
    "\n",
    "print(f'\\nEnsemble training complete!')\n",
    "print(f'Average RÂ²: {np.mean(ensemble_scores):.4f} Â± {np.std(ensemble_scores):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "id": "C8LnbSZUXZZr",
    "outputId": "ecf1fd27-8308-40f9-a790-311c47932e69"
   },
   "outputs": [],
   "source": [
    "# Get ensemble predictions and compute uncertainty\n",
    "print('Computing ensemble predictions and uncertainties...')\n",
    "\n",
    "# Collect predictions from all models\n",
    "all_preds = []\n",
    "\n",
    "for model in ensemble_models:\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            pred, _ = model(batch)\n",
    "            preds.append(pred.cpu().numpy())\n",
    "    \n",
    "    y_pred = np.concatenate(preds, axis=0).ravel()\n",
    "    all_preds.append(y_pred)\n",
    "\n",
    "# Convert to array: shape (num_models, num_samples)\n",
    "all_preds = np.array(all_preds)\n",
    "\n",
    "print(f'Predictions shape: {all_preds.shape}')\n",
    "\n",
    "# Compute ensemble statistics\n",
    "ensemble_mean = np.mean(all_preds, axis=0)\n",
    "ensemble_std = np.std(all_preds, axis=0)  # Epistemic uncertainty\n",
    "\n",
    "# Ensemble performance\n",
    "r2_ensemble = metrics.r2_score(y_test, ensemble_mean)\n",
    "mae_ensemble = metrics.mean_absolute_error(y_test, ensemble_mean)\n",
    "\n",
    "print(f'\\nEnsemble Performance:')\n",
    "print(f'  RÂ² Score: {r2_ensemble:.4f}')\n",
    "print(f'  MAE: {mae_ensemble:.4f}')\n",
    "print(f'\\nUncertainty Statistics:')\n",
    "print(f'  Mean uncertainty (std): {ensemble_std.mean():.4f}')\n",
    "print(f'  Min uncertainty: {ensemble_std.min():.4f}')\n",
    "print(f'  Max uncertainty: {ensemble_std.max():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "V5Um0FKJar33",
    "outputId": "a3bca3f8-1077-4411-b61b-b1be7f19a05b"
   },
   "outputs": [],
   "source": [
    "# Visualize uncertainty distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.hist(ensemble_std, bins=30, color=GOMES_COLORS['teal'], alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('Prediction Uncertainty (Std Dev)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Distribution of Predictive Uncertainty', fontsize=14, fontweight='bold')\n",
    "ax.axvline(ensemble_std.mean(), color=GOMES_COLORS['coral'], \n",
    "          linestyle='--', linewidth=2, label=f'Mean: {ensemble_std.mean():.3f}')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Higher uncertainty indicates disagreement among ensemble members')\n",
    "print('This often correlates with difficult predictions or out-of-distribution samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1979
    },
    "id": "ll3PNU_Ab5gk",
    "outputId": "567852f0-e707-4851-e5e4-03f9034a7819"
   },
   "outputs": [],
   "source": [
    "# Alternative: Monte Carlo Dropout for Uncertainty Quantification\n",
    "# This is a modern approach that doesn't require training multiple models\n",
    "\n",
    "class MCDropoutGNN(MoleculeGNN):\n",
    "    \"\"\"\n",
    "    GNN with Monte Carlo Dropout for uncertainty estimation\n",
    "    Dropout is applied during inference to sample from the posterior\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def enable_dropout(self):\n",
    "        \"\"\"Enable dropout during inference\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Dropout):\n",
    "                module.train()\n",
    "    \n",
    "    def mc_predict(self, batch, n_samples=100):\n",
    "        \"\"\"\n",
    "        Perform MC Dropout prediction\n",
    "        Returns: mean and std of predictions\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        self.enable_dropout()\n",
    "        \n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for _ in range(n_samples):\n",
    "                pred, _ = self(batch)\n",
    "                predictions.append(pred.cpu().numpy())\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        mean_pred = predictions.mean(axis=0)\n",
    "        std_pred = predictions.std(axis=0)\n",
    "        \n",
    "        return mean_pred, std_pred\n",
    "\n",
    "\n",
    "print('MC Dropout GNN class defined')\n",
    "print('Use mc_predict() to get uncertainty estimates from dropout sampling')\n",
    "print('\\nNote: For faster notebook execution, this is provided as an example')\n",
    "print('Uncomment the training code below to use MC Dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116
    },
    "id": "Yjqt2kpoh-2H",
    "outputId": "0f19ca7c-c740-4da7-f590-c04b35c46eb2"
   },
   "outputs": [],
   "source": [
    "# Example MC Dropout usage (commented out for speed)\n",
    "# Uncomment to use MC Dropout instead of ensemble\n",
    "\n",
    "# mc_model = MCDropoutGNN(\n",
    "#     num_atom_features=9,\n",
    "#     num_bond_features=3,\n",
    "#     hidden_channels=48,\n",
    "#     num_conv_layers=4,\n",
    "#     embedding_dim=128,\n",
    "#     dropout=0.2,  # Important: use higher dropout for MC Dropout\n",
    "#     learning_rate=0.001\n",
    "# )\n",
    "#\n",
    "# trainer = pl.Trainer(max_epochs=50, accelerator='auto', devices=1)\n",
    "# trainer.fit(mc_model, train_loader, test_loader)\n",
    "#\n",
    "# # Get MC Dropout predictions\n",
    "# mc_model.eval()\n",
    "# mc_means = []\n",
    "# mc_stds = []\n",
    "#\n",
    "# for batch in test_loader:\n",
    "#     mean_pred, std_pred = mc_model.mc_predict(batch, n_samples=50)\n",
    "#     mc_means.append(mean_pred)\n",
    "#     mc_stds.append(std_pred)\n",
    "#\n",
    "# mc_predictions = np.concatenate(mc_means, axis=0).ravel()\n",
    "# mc_uncertainties = np.concatenate(mc_stds, axis=0).ravel()\n",
    "#\n",
    "# r2_mc = metrics.r2_score(y_test, mc_predictions)\n",
    "# print(f'MC Dropout RÂ² Score: {r2_mc:.4f}')\n",
    "\n",
    "print('MC Dropout training code (commented out)')\n",
    "print('This provides an alternative to ensembles for uncertainty estimation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "-m4dPlS-iCrK",
    "outputId": "35664333-116c-4335-d8dd-a53310550b62"
   },
   "outputs": [],
   "source": [
    "# Analyze correlation between prediction error and uncertainty\n",
    "# Key insight: High uncertainty should correlate with high prediction error\n",
    "\n",
    "# Calculate prediction errors\n",
    "prediction_errors = np.abs(y_test - ensemble_mean)\n",
    "\n",
    "# Calculate correlation\n",
    "correlation, p_value = sp.stats.pearsonr(prediction_errors, ensemble_std)\n",
    "\n",
    "print(f'Correlation Analysis:')\n",
    "print(f'  Pearson correlation: {correlation:.4f}')\n",
    "print(f'  P-value: {p_value:.6f}')\n",
    "\n",
    "if correlation > 0.3:\n",
    "    print(f'\\nPositive correlation detected!')\n",
    "    print(f'Higher uncertainty is associated with larger prediction errors.')\n",
    "    print(f'This validates the uncertainty estimates.')\n",
    "else:\n",
    "    print(f'\\nWeak or no correlation.')\n",
    "    print(f'Uncertainty estimates may need improvement.')\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter plot: Error vs Uncertainty\n",
    "axes[0].scatter(prediction_errors, ensemble_std, alpha=0.5, \n",
    "               c=GOMES_COLORS['teal'], s=40, edgecolors='k', linewidth=0.5)\n",
    "axes[0].set_xlabel('Prediction Error (|True - Predicted|)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Prediction Uncertainty (Std Dev)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title(f'Error vs Uncertainty (Ï={correlation:.3f})', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(prediction_errors, ensemble_std, 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0].plot(np.sort(prediction_errors), p(np.sort(prediction_errors)), \n",
    "            \"r--\", linewidth=2, label='Trend line')\n",
    "axes[0].legend()\n",
    "\n",
    "# Histogram: Distribution comparison\n",
    "axes[1].hist(prediction_errors, bins=20, alpha=0.5, \n",
    "            label='Prediction Errors', color=GOMES_COLORS['coral'])\n",
    "axes[1].hist(ensemble_std, bins=20, alpha=0.5, \n",
    "            label='Uncertainties', color=GOMES_COLORS['teal'])\n",
    "axes[1].set_xlabel('Value', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Distributions', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSni5vhRiF6D"
   },
   "outputs": [],
   "source": [
    "# Summary: Uncertainty Quantification Methods\n",
    "\n",
    "print('=' * 80)\n",
    "print('UNCERTAINTY QUANTIFICATION SUMMARY')\n",
    "print('=' * 80)\n",
    "\n",
    "print('\\nMethods Covered:')\n",
    "print('\\n1. Model Ensembles (Epistemic Uncertainty)')\n",
    "print('   - Train multiple models with different hyperparameters')\n",
    "print('   - Variance across models indicates uncertainty')\n",
    "print('   - Pros: Reliable, interpretable')\n",
    "print('   - Cons: Computationally expensive (multiple trainings)')\n",
    "\n",
    "print('\\n2. Monte Carlo Dropout (Aleatoric + Epistemic Uncertainty)')\n",
    "print('   - Single model with dropout enabled during inference')\n",
    "print('   - Multiple forward passes with different dropout masks')\n",
    "print('   - Pros: Efficient, only one model needed')\n",
    "print('   - Cons: Requires careful dropout tuning')\n",
    "\n",
    "print('\\n3. Bayesian Neural Networks (Full Bayesian Treatment)')\n",
    "print('   - Learn distributions over weights instead of point estimates')\n",
    "print('   - Libraries: Pyro, PyTorch Uncertainty, Bayesian-Torch')\n",
    "print('   - Pros: Theoretically principled')\n",
    "print('   - Cons: Complex implementation, expensive inference')\n",
    "\n",
    "print('\\nKey Takeaways:')\n",
    "print('- Uncertainty quantification is crucial for reliable ML in chemistry')\n",
    "print('- High uncertainty â†’ low confidence â†’ need more data or better model')\n",
    "print('- Uncertainty estimates should correlate with prediction errors')\n",
    "print('- Ensembles are simple and effective for most applications')\n",
    "\n",
    "print('\\nFurther Reading:')\n",
    "print('- Gal & Ghahramani (2016): Dropout as Bayesian Approximation')\n",
    "print('- Lakshminarayanan et al. (2017): Simple and Scalable Ensembles')\n",
    "print('- Intel Bayesian-Torch: github.com/IntelLabs/bayesian-torch')\n",
    "print('- TorchUncertainty: github.com/ENSTA-U2IS/torch-uncertainty')\n",
    "\n",
    "print('\\n' + '=' * 80)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}