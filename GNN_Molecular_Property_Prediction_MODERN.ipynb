{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a id='introduction'></a>\n",
    "# 1. Introduction: Why Graphs for Molecules?\n",
    "\n",
    "**\u23f1\ufe0f Expected time:** 10 minutes\n",
    "\n",
    "## The Challenge of Molecular Representation\n",
    "\n",
    "Molecules are complex 3D structures with atoms connected by chemical bonds. How should we represent them for machine learning?\n",
    "\n",
    "### Traditional Approaches\n",
    "\n",
    "1. **SMILES Strings** \u2192 Treat as text sequences\n",
    "2. **Molecular Descriptors** \u2192 Hand-crafted features (MW, logP, TPSA, etc.)\n",
    "3. **Fingerprints** \u2192 Binary vectors (Morgan, MACCS, etc.)\n",
    "\n",
    "### The Graph Representation Advantage\n",
    "\n",
    "A **graph** is a natural way to represent molecules:\n",
    "- **Nodes (Vertices)** = Atoms\n",
    "- **Edges (Links)** = Chemical bonds\n",
    "- **Node features** = Atom properties (element, charge, hybridization)\n",
    "- **Edge features** = Bond properties (type, stereochemistry)\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://github.com/beangoben/chemistry_ml_colab/blob/master/images/Chloroquine-2D-molecular-graph.png?raw=1\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "> **Inductive Bias**: Graphs encode the assumption that molecular properties are determined by:\n",
    "> 1. The types of atoms present\n",
    "> 2. How atoms are connected\n",
    "> 3. Local chemical environments (neighborhoods)\n",
    "\n",
    "This is exactly how chemists think about molecules!"
   ],
   "metadata": {
    "id": "fRRh_9CaMTj8"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0tmdeUEMTj9"
   },
   "source": [
    "# Graph Neural Networks for Molecular Property Prediction\n",
    "\n",
    "**Molecular Machine Learning Course - GNN Studio**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_REPO/GNN_Molecular_Property_Prediction.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. **Understand** why graph representations are natural for molecular data\n",
    "2. **Construct** molecular graphs from SMILES strings with atom and bond features\n",
    "3. **Implement** message-passing neural networks (MPNNs) using PyTorch Geometric\n",
    "4. **Train** and evaluate GNN models for molecular property prediction\n",
    "5. **Use** production-ready tools like Chemprop for real-world applications\n",
    "6. **Debug** common issues in GNN training pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "0. [Setup & Environment](#setup)\n",
    "1. [Introduction: Why Graphs for Molecules?](#introduction)\n",
    "2. [Molecular Graph Construction](#graph-construction)\n",
    "3. [The ESOL Solubility Dataset](#dataset)\n",
    "4. [Message-Passing Neural Networks - Theory](#mpnn-theory)\n",
    "5. [Implementing MPNN with PyTorch Geometric](#mpnn-implementation)\n",
    "6. [Training and Evaluation](#training)\n",
    "7. [Understanding Learned Representations](#representations)\n",
    "8. [Chemprop - Production-Ready GNNs](#chemprop)\n",
    "9. [Common Pitfalls and Debugging](#debugging)\n",
    "10. [Extensions and Next Steps](#extensions)\n",
    "\n",
    "---\n",
    "\n",
    "## Authors and Acknowledgments\n",
    "\n",
    "**Developed by:** Gomes Research Group, CMU  \n",
    "**Based on:** AI4Chem course materials (Schwallergroup)  \n",
    "**Last Updated:** December 2024  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFVvRC7-MTj9"
   },
   "source": [
    "<a id='setup'></a>\n",
    "# 0. Setup & Environment\n",
    "\n",
    "**\u23f1\ufe0f Expected runtime:** 2-3 minutes\n",
    "\n",
    "This section installs all required packages and sets up the environment for reproducible experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_LH0phEMTj9"
   },
   "outputs": [],
   "source": [
    "# Check if running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"\u2713 Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"\u2713 Running in local environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TT_yv-mJMTj9"
   },
   "source": [
    "## 0.1 Package Installation\n",
    "\n",
    "We'll install the latest stable versions of:\n",
    "- **PyTorch 2.x** - Deep learning framework\n",
    "- **PyTorch Geometric 2.6+** - Graph neural network library  \n",
    "- **PyTorch Lightning 2.x** - High-level training framework\n",
    "- **RDKit** - Cheminformatics toolkit\n",
    "- **Chemprop v2.2.1** - Production-ready molecular ML\n",
    "- **Weights & Biases** - Experiment tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijFtwAN4MTj9"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Install uv first for faster package management\n",
    "pip install -q uv\n",
    "\n",
    "# Install PyTorch with CUDA 11.8 support (for Colab T4 GPUs)\n",
    "uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Install PyTorch Geometric and dependencies\n",
    "uv pip install torch-geometric -f https://data.pyg.org/whl/torch-2.1.0+cu118.html\n",
    "\n",
    "# Install other ML packages\n",
    "uv pip install pytorch-lightning==2.1.0 wandb rdkit ogb deepchem scikit-learn pandas matplotlib seaborn plotly tqdm\n",
    "\n",
    "# Install Chemprop v2\n",
    "uv pip install chemprop==2.2.1\n",
    "\n",
    "echo \"\u2713 Package installation complete!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdnNFvZdMTj9"
   },
   "source": [
    "## 0.2 Download Data\n",
    "\n",
    "We'll use the **ESOL (Estimated SOLubility)** dataset, which contains aqueous solubility measurements for 1,128 small organic molecules.\n",
    "\n",
    "**Reference:** Delaney, J. S. (2004). *ESOL: Estimating Aqueous Solubility Directly from Molecular Structure.* J. Chem. Inf. Comput. Sci., 44(3), 1000-1005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZzldQaiiMTj9"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create data directory\n",
    "mkdir -p data\n",
    "\n",
    "# Download ESOL dataset\n",
    "wget -q https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/delaney-processed.csv -O data/esol.csv\n",
    "\n",
    "echo \"\u2713 Data download complete!\"\n",
    "echo \"Files in data directory:\"\n",
    "ls -lh data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlQ-XTOJMTj-"
   },
   "source": [
    "## 0.3 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RW-fCDYeMTj-"
   },
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# PyTorch Geometric\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "from torch_geometric.nn import NNConv, global_add_pool, global_mean_pool, global_max_pool\n",
    "from torch_geometric.nn import MLP, GCNConv, GINConv, GATConv\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "\n",
    "# Cheminformatics\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Draw, Descriptors\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "\n",
    "# OGB utilities\n",
    "from ogb.utils import smiles2graph\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Experiment tracking\n",
    "import wandb\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure RDKit visualization\n",
    "IPythonConsole.ipython_useSVG = True\n",
    "IPythonConsole.molSize = 300, 300\n",
    "\n",
    "print(f\"\u2713 PyTorch version: {torch.__version__}\")\n",
    "print(f\"\u2713 PyTorch Geometric version: {torch_geometric.__version__}\")\n",
    "print(f\"\u2713 PyTorch Lightning version: {pl.__version__}\")\n",
    "print(f\"\u2713 RDKit version: {rdkit.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdzptRMTMTj-"
   },
   "source": [
    "## 0.4 GPU Detection and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Faw3FiGWMTj-"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "print(\"=\" * 60)\n",
    "print(\"HARDWARE DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\u2713 GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  - CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"  - Device Capability: {torch.cuda.get_device_capability(0)}\")\n",
    "    print(f\"  - Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  No GPU detected - using CPU\")\n",
    "    if IN_COLAB:\n",
    "        print(\"\\n\ud83d\udca1 To enable GPU in Colab:\")\n",
    "        print(\"   Runtime \u2192 Change runtime type \u2192 Hardware accelerator \u2192 GPU\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"\\n\u2713 Using device: {device}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABIJIJ_6MTj-"
   },
   "source": [
    "## 0.5 Reproducibility Setup\n",
    "\n",
    "Setting random seeds ensures that:\n",
    "1. Model weights are initialized identically\n",
    "2. Data shuffling is deterministic\n",
    "3. Results are reproducible across runs\n",
    "\n",
    "This is **critical** for debugging and comparing different architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jOS1_aUUMTj-"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Set random seeds for reproducibility across all libraries.\n",
    "\n",
    "    Args:\n",
    "        seed: Random seed value (default: 42)\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Deterministic operations (may reduce performance slightly)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # PyTorch Lightning\n",
    "    pl.seed_everything(seed, workers=True)\n",
    "\n",
    "    # Environment variable for deterministic algorithms\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    print(f\"\u2713 Random seed set to {seed} for reproducibility\")\n",
    "\n",
    "# Set seed for this session\n",
    "RANDOM_SEED = 42\n",
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbU2VbbFMTj-"
   },
   "source": [
    "## 0.6 Visualization Setup (Gomes Group Style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9wR0qygMTj-"
   },
   "outputs": [],
   "source": [
    "# Gomes Group color palette\n",
    "GOMES_COLORS = {\n",
    "    'teal': '#00D9FF',\n",
    "    'coral': '#FF6B6B',\n",
    "    'navy': '#0A1628',\n",
    "    'light_teal': '#7FEFFF',\n",
    "    'dark_teal': '#00A8CC',\n",
    "    'light_coral': '#FF9999',\n",
    "    'dark_coral': '#CC5555',\n",
    "    'gray': '#95A5A6',\n",
    "    'white': '#FFFFFF'\n",
    "}\n",
    "\n",
    "# Configure matplotlib style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['xtick.labelsize'] = 11\n",
    "plt.rcParams['ytick.labelsize'] = 11\n",
    "plt.rcParams['legend.fontsize'] = 12\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "# Seaborn configuration\n",
    "sns.set_palette([GOMES_COLORS['teal'], GOMES_COLORS['coral'],\n",
    "                 GOMES_COLORS['gray'], GOMES_COLORS['navy']])\n",
    "\n",
    "print(\"\u2713 Visualization style configured (Gomes Group aesthetic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfeDGH1oMTj-"
   },
   "source": [
    "## 0.7 Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P_g8ap0lMTj-"
   },
   "outputs": [],
   "source": [
    "def print_section_header(title: str, emoji: str = \"\ud83d\udcda\"):\n",
    "    \"\"\"\n",
    "    Print a formatted section header.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"{emoji}  {title}\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "def print_metrics(metrics: dict, title: str = \"Metrics\"):\n",
    "    \"\"\"\n",
    "    Pretty print evaluation metrics.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{title}:\")\n",
    "    print(\"-\" * 40)\n",
    "    for key, value in metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key:.<30} {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {key:.<30} {value}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "def estimate_runtime(gpu: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Return estimated runtime string based on hardware.\n",
    "    \"\"\"\n",
    "    if gpu:\n",
    "        return \"\u23f1\ufe0f Expected runtime: ~2-3 minutes (GPU)\"\n",
    "    else:\n",
    "        return \"\u23f1\ufe0f Expected runtime: ~8-10 minutes (CPU)\"\n",
    "\n",
    "print(\"\u2713 Utility functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktpVL1GgMTj-"
   },
   "source": [
    "---\n",
    "\n",
    "**\u2705 Setup Complete!**\n",
    "\n",
    "You're now ready to start learning about Graph Neural Networks for molecular property prediction.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZTalutzMTj-"
   },
   "source": [
    "<a id='introduction'></a>\n",
    "# 1. Introduction: Why Graphs for Molecules?\n",
    "\n",
    "**\u23f1\ufe0f Expected time:** 10 minutes\n",
    "\n",
    "## The Challenge of Molecular Representation\n",
    "\n",
    "Molecules are complex 3D structures with atoms connected by chemical bonds. How should we represent them for machine learning?\n",
    "\n",
    "### Traditional Approaches\n",
    "\n",
    "1. **SMILES Strings** \u2192 Treat as text sequences\n",
    "2. **Molecular Descriptors** \u2192 Hand-crafted features (MW, logP, TPSA, etc.)\n",
    "3. **Fingerprints** \u2192 Binary vectors (Morgan, MACCS, etc.)\n",
    "\n",
    "### The Graph Representation Advantage\n",
    "\n",
    "A **graph** is a natural way to represent molecules:\n",
    "- **Nodes (Vertices)** = Atoms\n",
    "- **Edges (Links)** = Chemical bonds\n",
    "- **Node features** = Atom properties (element, charge, hybridization)\n",
    "- **Edge features** = Bond properties (type, stereochemistry)\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://github.com/beangoben/chemistry_ml_colab/blob/master/images/Chloroquine-2D-molecular-graph.png?raw=1\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "> **Inductive Bias**: Graphs encode the assumption that molecular properties are determined by:\n",
    "> 1. The types of atoms present\n",
    "> 2. How atoms are connected\n",
    "> 3. Local chemical environments (neighborhoods)\n",
    "\n",
    "This is exactly how chemists think about molecules!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jtOOj4CMTj-"
   },
   "outputs": [],
   "source": [
    "print_section_header(\"Visualizing Molecular Graphs\", \"\ud83d\udd2c\")\n",
    "\n",
    "# Example molecules of increasing complexity\n",
    "molecules = {\n",
    "    'Methane': 'C',\n",
    "    'Ethanol': 'CCO',\n",
    "    'Benzene': 'c1ccccc1',\n",
    "    'Aspirin': 'CC(=O)Oc1ccccc1C(=O)O'\n",
    "}\n",
    "\n",
    "# Visualize molecules\n",
    "mols = [Chem.MolFromSmiles(smi) for smi in molecules.values()]\n",
    "img = Draw.MolsToGridImage(\n",
    "    mols,\n",
    "    molsPerRow=2,\n",
    "    subImgSize=(300, 300),\n",
    "    legends=list(molecules.keys())\n",
    ")\n",
    "display(img)\n",
    "\n",
    "print(\"\\n\u2713 Each molecule is represented as a graph:\")\n",
    "print(\"  - Nodes: atoms (C, O, H, etc.)\")\n",
    "print(\"  - Edges: bonds (single, double, aromatic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWux6n7YMTj-"
   },
   "source": [
    "## Graph Theory Basics\n",
    "\n",
    "A graph $G = (V, E)$ consists of:\n",
    "\n",
    "- **Vertices (Nodes)**: $V = \\{v_1, v_2, ..., v_n\\}$\n",
    "- **Edges**: $E \\subseteq \\{(i,j) | i,j \\in V, i \\neq j\\}$\n",
    "\n",
    "For molecules, we typically use **undirected graphs** (bonds work both ways).\n",
    "\n",
    "### Graph Properties\n",
    "\n",
    "| Property | Molecular Interpretation |\n",
    "|----------|------------------------|\n",
    "| **Node degree** | Number of bonds an atom forms |\n",
    "| **Path length** | Shortest distance between atoms |\n",
    "| **Cycles** | Rings in molecules |\n",
    "| **Connectivity** | Whether molecule is fragmented |\n",
    "\n",
    "### Adjacency Matrix\n",
    "\n",
    "An $n \\times n$ matrix $A$ where $A_{ij} = 1$ if atoms $i$ and $j$ are bonded.\n",
    "\n",
    "**Note**: For GNNs, we typically use the more efficient **edge index** format instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3_Ex1blMTj-"
   },
   "outputs": [],
   "source": [
    "print_section_header(\"Example: Caffeine Molecular Graph\", \"\u2615\")\n",
    "\n",
    "caffeine_smiles = 'CN1C=NC2=C1C(=O)N(C(=O)N2C)C'\n",
    "mol = Chem.MolFromSmiles(caffeine_smiles)\n",
    "\n",
    "print(f\"Molecule: Caffeine\")\n",
    "print(f\"SMILES: {caffeine_smiles}\")\n",
    "print(f\"Molecular Formula: {Chem.rdMolDescriptors.CalcMolFormula(mol)}\")\n",
    "print(f\"\\nGraph Properties:\")\n",
    "print(f\"  Number of atoms (nodes): {mol.GetNumAtoms()}\")\n",
    "print(f\"  Number of bonds (edges): {mol.GetNumBonds()}\")\n",
    "print(f\"  Number of rings: {Chem.rdMolDescriptors.CalcNumRings(mol)}\")\n",
    "\n",
    "# Display molecule\n",
    "display(mol)\n",
    "\n",
    "# Calculate and display adjacency matrix\n",
    "from rdkit.Chem import GetAdjacencyMatrix\n",
    "adj_matrix = GetAdjacencyMatrix(mol)\n",
    "\n",
    "print(f\"\\nAdjacency Matrix Shape: {adj_matrix.shape}\")\n",
    "print(\"First 5x5 block:\")\n",
    "print(adj_matrix[:5, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6J5FimxgMTj_"
   },
   "source": [
    "<a id='graph-construction'></a>\n",
    "# 2. Molecular Graph Construction\n",
    "\n",
    "**\u23f1\ufe0f Expected time:** 15 minutes\n",
    "\n",
    "Now that we understand why graphs are natural for molecules, let's learn how to convert SMILES strings into graph data structures that neural networks can process.\n",
    "\n",
    "## From SMILES to Graph\n",
    "\n",
    "The conversion process involves three key steps:\n",
    "\n",
    "1. **Extract atom features** \u2192 Convert each atom into a feature vector\n",
    "2. **Extract bond features** \u2192 Convert each bond into a feature vector\n",
    "3. **Build edge index** \u2192 Create connectivity matrix in COO format\n",
    "\n",
    "We'll use the **OGB (Open Graph Benchmark)** featurization standard, which is widely adopted in molecular ML research.\n",
    "\n",
    "### Atom Features\n",
    "\n",
    "For each atom, we extract 9 categorical features:\n",
    "\n",
    "| Feature | Description | Example Values |\n",
    "|---------|-------------|----------------|\n",
    "| Atomic number | Element identity | 1 (H), 6 (C), 7 (N), 8 (O) |\n",
    "| Degree | Number of bonded neighbors | 0, 1, 2, 3, 4 |\n",
    "| Formal charge | Integer charge | -1, 0, +1 |\n",
    "| Chirality | Stereochemistry | R, S, unspecified |\n",
    "| Num H atoms | Total hydrogen count | 0, 1, 2, 3 |\n",
    "| Hybridization | Orbital type | sp, sp2, sp3 |\n",
    "| Is aromatic | Aromaticity flag | True, False |\n",
    "| Is in ring | Ring membership | True, False |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WkM9TqhuMTj_"
   },
   "outputs": [],
   "source": [
    "print_section_header(\"Atom Featurization (OGB Standard)\", \"\u269b\ufe0f\")\n",
    "\n",
    "# Define allowable atom features (from OGB)\n",
    "ATOM_FEATURES = {\n",
    "    'atomic_num': list(range(1, 119)) + ['misc'],\n",
    "    'degree': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 'misc'],\n",
    "    'formal_charge': [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 'misc'],\n",
    "    'chiral_tag': ['CHI_UNSPECIFIED', 'CHI_TETRAHEDRAL_CW', 'CHI_TETRAHEDRAL_CCW', 'CHI_OTHER'],\n",
    "    'num_Hs': [0, 1, 2, 3, 4, 5, 6, 7, 8, 'misc'],\n",
    "    'hybridization': ['SP', 'SP2', 'SP3', 'SP3D', 'SP3D2', 'misc'],\n",
    "    'is_aromatic': [False, True],\n",
    "    'is_in_ring': [False, True]\n",
    "}\n",
    "\n",
    "def safe_index(lst, element):\n",
    "    \"\"\"\n",
    "    Return index of element in list. If not present, return last index (misc).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return lst.index(element)\n",
    "    except ValueError:\n",
    "        return len(lst) - 1\n",
    "\n",
    "def atom_to_feature_vector(atom):\n",
    "    \"\"\"\n",
    "    Convert RDKit atom object to feature vector (list of indices).\n",
    "\n",
    "    Args:\n",
    "        atom: RDKit Atom object\n",
    "\n",
    "    Returns:\n",
    "        list: Feature vector with 8 integer indices\n",
    "    \"\"\"\n",
    "    features = [\n",
    "        safe_index(ATOM_FEATURES['atomic_num'], atom.GetAtomicNum()),\n",
    "        safe_index(ATOM_FEATURES['degree'], atom.GetTotalDegree()),\n",
    "        safe_index(ATOM_FEATURES['formal_charge'], atom.GetFormalCharge()),\n",
    "        ATOM_FEATURES['chiral_tag'].index(str(atom.GetChiralTag())),\n",
    "        safe_index(ATOM_FEATURES['num_Hs'], atom.GetTotalNumHs()),\n",
    "        safe_index(ATOM_FEATURES['hybridization'], str(atom.GetHybridization())),\n",
    "        ATOM_FEATURES['is_aromatic'].index(atom.GetIsAromatic()),\n",
    "        ATOM_FEATURES['is_in_ring'].index(atom.IsInRing()),\n",
    "    ]\n",
    "    return features\n",
    "\n",
    "# Test on ethanol\n",
    "mol = Chem.MolFromSmiles('CCO')\n",
    "print(\"Ethanol (CCO) Atom Features:\")\n",
    "print(\"-\" * 50)\n",
    "for i, atom in enumerate(mol.GetAtoms()):\n",
    "    features = atom_to_feature_vector(atom)\n",
    "    print(f\"Atom {i} ({atom.GetSymbol()}):\")\n",
    "    print(f\"  Atomic number: {atom.GetAtomicNum()}\")\n",
    "    print(f\"  Degree: {atom.GetTotalDegree()}\")\n",
    "    print(f\"  Hybridization: {atom.GetHybridization()}\")\n",
    "    print(f\"  Feature vector: {features}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPsDl18YMTj_"
   },
   "source": [
    "### Bond Features\n",
    "\n",
    "For each bond, we extract 3 categorical features:\n",
    "\n",
    "| Feature | Description | Example Values |\n",
    "|---------|-------------|----------------|\n",
    "| Bond type | Chemical bond order | SINGLE, DOUBLE, TRIPLE, AROMATIC |\n",
    "| Stereo | Stereochemistry | E/Z, cis/trans, none |\n",
    "| Conjugated | Part of conjugated system | True, False |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s3duB8mpMTj_"
   },
   "outputs": [],
   "source": [
    "print_section_header(\"Bond Featurization\", \"\ud83d\udd17\")\n",
    "\n",
    "BOND_FEATURES = {\n",
    "    'bond_type': ['SINGLE', 'DOUBLE', 'TRIPLE', 'AROMATIC', 'misc'],\n",
    "    'stereo': ['STEREONONE', 'STEREOZ', 'STEREOE', 'STEREOCIS', 'STEREOTRANS', 'STEREOANY'],\n",
    "    'is_conjugated': [False, True],\n",
    "}\n",
    "\n",
    "def bond_to_feature_vector(bond):\n",
    "    \"\"\"\n",
    "    Convert RDKit bond object to feature vector.\n",
    "\n",
    "    Args:\n",
    "        bond: RDKit Bond object\n",
    "\n",
    "    Returns:\n",
    "        list: Feature vector with 3 integer indices\n",
    "    \"\"\"\n",
    "    features = [\n",
    "        safe_index(BOND_FEATURES['bond_type'], str(bond.GetBondType())),\n",
    "        BOND_FEATURES['stereo'].index(str(bond.GetStereo())),\n",
    "        BOND_FEATURES['is_conjugated'].index(bond.GetIsConjugated()),\n",
    "    ]\n",
    "    return features\n",
    "\n",
    "# Test on ethanol bonds\n",
    "mol = Chem.MolFromSmiles('CCO')\n",
    "print(\"Ethanol Bond Features:\")\n",
    "print(\"-\" * 50)\n",
    "for i, bond in enumerate(mol.GetBonds()):\n",
    "    features = bond_to_feature_vector(bond)\n",
    "    begin = bond.GetBeginAtom().GetSymbol()\n",
    "    end = bond.GetEndAtom().GetSymbol()\n",
    "    bond_type = bond.GetBondType()\n",
    "    print(f\"Bond {i} ({begin}-{end}):\")\n",
    "    print(f\"  Type: {bond_type}\")\n",
    "    print(f\"  Feature vector: {features}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XUM6AI8MTj_"
   },
   "source": [
    "### Edge Index Format (COO)\n",
    "\n",
    "PyTorch Geometric uses **Coordinate (COO) format** for graph connectivity:\n",
    "\n",
    "```\n",
    "edge_index = [[source_nodes],\n",
    "              [target_nodes]]\n",
    "```\n",
    "\n",
    "Shape: `[2, num_edges]`\n",
    "\n",
    "**Important**: For undirected graphs (molecules), we add edges in **both directions**:\n",
    "- If atom 0 bonds to atom 1, we add edges (0\u21921) AND (1\u21920)\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://github.com/beangoben/chemistry_ml_colab/blob/master/images/mol_tensors.png?raw=1\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2rra94xMTj_"
   },
   "outputs": [],
   "source": [
    "print_section_header(\"Complete SMILES \u2192 PyG Graph Conversion\", \"\ud83d\udd04\")\n",
    "\n",
    "def molecule_to_graph_data(smiles):\n",
    "    \"\"\"\n",
    "    Convert SMILES to PyTorch Geometric Data object.\n",
    "\n",
    "    Args:\n",
    "        smiles: SMILES string\n",
    "\n",
    "    Returns:\n",
    "        Data: PyG Data object with x, edge_index, edge_attr\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "\n",
    "    # Node features\n",
    "    atom_features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        atom_features.append(atom_to_feature_vector(atom))\n",
    "    x = torch.tensor(atom_features, dtype=torch.long)\n",
    "\n",
    "    # Edge features and edge index\n",
    "    edge_indices = []\n",
    "    edge_features = []\n",
    "\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        bond_feat = bond_to_feature_vector(bond)\n",
    "\n",
    "        # Add both directions (undirected graph)\n",
    "        edge_indices.append([i, j])\n",
    "        edge_features.append(bond_feat)\n",
    "        edge_indices.append([j, i])\n",
    "        edge_features.append(bond_feat)\n",
    "\n",
    "    # Handle single-atom molecules (no bonds)\n",
    "    if len(edge_indices) == 0:\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        edge_attr = torch.empty((0, 3), dtype=torch.long)\n",
    "    else:\n",
    "        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_features, dtype=torch.long)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    return data\n",
    "\n",
    "# Test on multiple molecules\n",
    "examples = [\n",
    "    ('Methane', 'C'),\n",
    "    ('Ethanol', 'CCO'),\n",
    "    ('Benzene', 'c1ccccc1'),\n",
    "    ('Aspirin', 'CC(=O)Oc1ccccc1C(=O)O')\n",
    "]\n",
    "\n",
    "print(\"Molecular Graph Statistics:\")\n",
    "print(\"=\" * 70)\n",
    "for name, smiles in examples:\n",
    "    graph = molecule_to_graph_data(smiles)\n",
    "    print(f\"{name:12s} | Nodes: {graph.num_nodes:3d} | Edges: {graph.num_edges:3d}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n\u2713 Featurization functions ready!\")\n",
    "print(\"  - atom_to_feature_vector()\")\n",
    "print(\"  - bond_to_feature_vector()\")\n",
    "print(\"  - molecule_to_graph_data()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybCh0hXDMTj_"
   },
   "source": [
    "<a id='dataset'></a>\n",
    "# 3. The ESOL Solubility Dataset\n",
    "\n",
    "**\u23f1\ufe0f Expected time:** 10 minutes\n",
    "\n",
    "Now let's load our molecular dataset and prepare it for training.\n",
    "\n",
    "## About ESOL\n",
    "\n",
    "The **ESOL (Estimated SOLubility)** dataset contains:\n",
    "- **1,128 molecules** with measured aqueous solubility\n",
    "- **Target**: log(solubility) in mols per litre\n",
    "- **SMILES** representations for each molecule\n",
    "\n",
    "**Reference:** Delaney, J. S. (2004). ESOL: Estimating Aqueous Solubility Directly from Molecular Structure. *J. Chem. Inf. Comput. Sci.*, 44(3), 1000-1005.\n",
    "\n",
    "### Why Solubility Matters\n",
    "\n",
    "Aqueous solubility is a critical property in drug discovery:\n",
    "- Poor solubility \u2192 Poor bioavailability\n",
    "- Predicting solubility early saves time and money\n",
    "- Traditional methods (wet lab) are slow and expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8V9oqlshMTj_"
   },
   "outputs": [],
   "source": [
    "print_section_header(\"Loading ESOL Dataset\", \"\ud83d\udca7\")\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/esol.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "for col in df.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Define column names\n",
    "smiles_column = 'smiles'\n",
    "target_column = 'measured log solubility in mols per litre'\n",
    "\n",
    "# Display first few rows\n",
    "print(f\"\\nFirst 5 molecules:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4e1MAbyMTj_"
   },
   "outputs": [],
   "source": [
    "print_section_header(\"Exploratory Data Analysis\", \"\ud83d\udcca\")\n",
    "\n",
    "# Extract target values\n",
    "y = df[target_column].values\n",
    "\n",
    "# Basic statistics\n",
    "print(\"Target Statistics:\")\n",
    "print(f\"  Mean: {y.mean():.3f}\")\n",
    "print(f\"  Std: {y.std():.3f}\")\n",
    "print(f\"  Min: {y.min():.3f}\")\n",
    "print(f\"  Max: {y.max():.3f}\")\n",
    "print(f\"  Median: {np.median(y):.3f}\")\n",
    "\n",
    "# Visualize distribution with Gomes colors\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(y, bins=30, color=GOMES_COLORS['teal'], alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(y.mean(), color=GOMES_COLORS['coral'], linestyle='--', linewidth=2, label=f'Mean: {y.mean():.2f}')\n",
    "axes[0].set_xlabel('Log Solubility (mol/L)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Distribution of Log Solubility', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(y, vert=True, patch_artist=True,\n",
    "                boxprops=dict(facecolor=GOMES_COLORS['teal'], alpha=0.7),\n",
    "                medianprops=dict(color=GOMES_COLORS['coral'], linewidth=2))\n",
    "axes[1].set_ylabel('Log Solubility (mol/L)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Box Plot', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2713 Dataset loaded and explored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUSNM10MMTj_"
   },
   "outputs": [],
   "source": [
    "print_section_header(\"Converting Molecules to Graphs\", \"\ud83d\udd04\")\n",
    "\n",
    "# Convert all SMILES to PyG graphs\n",
    "smiles_list = df[smiles_column].tolist()\n",
    "targets = df[target_column].values\n",
    "\n",
    "graph_list = []\n",
    "failed_indices = []\n",
    "\n",
    "print(\"Converting SMILES to graphs...\")\n",
    "for i, smi in enumerate(tqdm(smiles_list)):\n",
    "    graph = molecule_to_graph_data(smi)\n",
    "    if graph is not None:\n",
    "        # Add target value to graph\n",
    "        graph.y = torch.tensor([targets[i]], dtype=torch.float)\n",
    "        graph_list.append(graph)\n",
    "    else:\n",
    "        failed_indices.append(i)\n",
    "        print(f\"  Warning: Could not parse SMILES at index {i}: {smi}\")\n",
    "\n",
    "print(f\"\\n\u2713 Successfully converted {len(graph_list)} molecules to graphs\")\n",
    "print(f\"  Failed: {len(failed_indices)} molecules\")\n",
    "\n",
    "# Show example graph\n",
    "print(f\"\\nExample graph (first molecule):\")\n",
    "print(graph_list[0])\n",
    "print(f\"  Node features shape: {graph_list[0].x.shape}\")\n",
    "print(f\"  Edge index shape: {graph_list[0].edge_index.shape}\")\n",
    "print(f\"  Edge features shape: {graph_list[0].edge_attr.shape}\")\n",
    "print(f\"  Target value: {graph_list[0].y.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T77XK-t_MTj_"
   },
   "outputs": [],
   "source": [
    "print_section_header(\"Train/Val/Test Split (70/10/20)\", \"\u2702\ufe0f\")\n",
    "\n",
    "# Create indices\n",
    "n_samples = len(graph_list)\n",
    "indices = list(range(n_samples))\n",
    "\n",
    "# Split: train (70%), temp (30%)\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    indices, test_size=0.3, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Split temp: val (10%), test (20%) -> 10/30 = 1/3, 20/30 = 2/3\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx, test_size=2/3, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"Split sizes:\")\n",
    "print(f\"  Train: {len(train_idx)} samples ({len(train_idx)/n_samples*100:.1f}%)\")\n",
    "print(f\"  Val:   {len(val_idx)} samples ({len(val_idx)/n_samples*100:.1f}%)\")\n",
    "print(f\"  Test:  {len(test_idx)} samples ({len(test_idx)/n_samples*100:.1f}%)\")\n",
    "\n",
    "# Create dataset lists\n",
    "train_graphs = [graph_list[i] for i in train_idx]\n",
    "val_graphs = [graph_list[i] for i in val_idx]\n",
    "test_graphs = [graph_list[i] for i in test_idx]\n",
    "\n",
    "# Normalize targets (critical for training!)\n",
    "print(f\"\\nNormalizing targets...\")\n",
    "train_targets = np.array([g.y.item() for g in train_graphs])\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_targets.reshape(-1, 1))\n",
    "\n",
    "# Apply normalization to all graphs\n",
    "for g in train_graphs:\n",
    "    g.y = torch.tensor(scaler.transform([[g.y.item()]])[0], dtype=torch.float)\n",
    "for g in val_graphs:\n",
    "    g.y = torch.tensor(scaler.transform([[g.y.item()]])[0], dtype=torch.float)\n",
    "for g in test_graphs:\n",
    "    g.y = torch.tensor(scaler.transform([[g.y.item()]])[0], dtype=torch.float)\n",
    "\n",
    "print(f\"  Mean: {scaler.mean_[0]:.3f}\")\n",
    "print(f\"  Std: {scaler.scale_[0]:.3f}\")\n",
    "\n",
    "print(f\"\\n\u2713 Data preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DMnBVWCMTj_"
   },
   "source": [
    "<a id='mpnn-theory'></a>\n",
    "# 4. Message-Passing Neural Networks - Theory\n",
    "\n",
    "**\u23f1\ufe0f Expected time:** 15 minutes\n",
    "\n",
    "## The Key Idea\n",
    "\n",
    "**Message-Passing Neural Networks (MPNNs)** learn molecular representations by iteratively propagating information between connected atoms.\n",
    "\n",
    "### How Atoms \"Talk\" to Each Other\n",
    "\n",
    "Think of each atom in a molecule as a node in a communication network:\n",
    "1. Each atom has a **hidden state** (feature vector)\n",
    "2. Atoms **send messages** to their neighbors through bonds\n",
    "3. Atoms **aggregate** messages from all neighbors\n",
    "4. Atoms **update** their hidden states based on received messages\n",
    "5. This process repeats for T iterations\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Let's formalize the message passing process:\n",
    "\n",
    "**1. Initialization**\n",
    "\n",
    "$$h_i^0 = I(x_i), \\quad \\forall i \\in V$$\n",
    "\n",
    "where $h_i^0$ is the initial hidden state of atom $i$, and $I$ is an embedding function.\n",
    "\n",
    "**2. Message Generation** (at iteration $t+1$)\n",
    "\n",
    "$$m_{j \\rightarrow i}^{t+1} = M(h_i^t, h_j^t, e_{ij})$$\n",
    "\n",
    "where $m_{j \\rightarrow i}^{t+1}$ is the message from atom $j$ to atom $i$, and $M$ is a learnable message function that considers:\n",
    "- $h_i^t$: current state of receiving atom\n",
    "- $h_j^t$: current state of sending atom  \n",
    "- $e_{ij}$: bond features between atoms $i$ and $j$\n",
    "\n",
    "**3. Message Aggregation**\n",
    "\n",
    "$$m_i^{t+1} = \\sum_{j \\in N(i)} m_{j \\rightarrow i}^{t+1}$$\n",
    "\n",
    "where $N(i)$ is the set of neighboring atoms of atom $i$.\n",
    "\n",
    "**4. State Update**\n",
    "\n",
    "$$h_i^{t+1} = U(h_i^t, m_i^{t+1})$$\n",
    "\n",
    "where $U$ is a learnable update function (often a GRU or LSTM).\n",
    "\n",
    "**5. Readout** (Graph-level prediction)\n",
    "\n",
    "$$\\hat{y} = R(\\{h_i^T | i \\in V\\})$$\n",
    "\n",
    "where $R$ is a readout function that aggregates all atom states into a single molecular representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jyH3YmkMTj_"
   },
   "source": [
    "### Visual Intuition: Message Passing\n",
    "\n",
    "Let's see how information flows through a molecular graph:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://github.com/beangoben/chemistry_ml_colab/blob/master/images/gcn_one.png?raw=1\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://github.com/beangoben/chemistry_ml_colab/blob/master/images/gcn_two.png?raw=1\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "### Multiple Iterations = Larger Receptive Field\n",
    "\n",
    "With each message passing iteration, atoms can \"see\" further in the molecular graph:\n",
    "\n",
    "- **Iteration 1**: Atoms know about immediate neighbors\n",
    "- **Iteration 2**: Atoms know about neighbors-of-neighbors  \n",
    "- **Iteration 3**: Atoms know about atoms 3 bonds away\n",
    "- **Iteration T**: Atoms know about atoms T bonds away\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://github.com/beangoben/chemistry_ml_colab/blob/master/images/gcn_layers.png?raw=1\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "### Connected Layers\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://github.com/beangoben/chemistry_ml_colab/blob/master/images/gcn_connected.png?raw=1\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-wYlcJWMTj_"
   },
   "source": [
    "### Key Properties of MPNNs\n",
    "\n",
    "1. **Permutation Invariance**: Output doesn't depend on atom ordering in the input\n",
    "2. **Variable Size**: Can handle molecules with different numbers of atoms\n",
    "3. **Local-to-Global**: Build molecular representations from local atomic environments\n",
    "4. **Learnable**: All functions ($I$, $M$, $U$, $R$) are neural networks trained end-to-end\n",
    "\n",
    "### Why This Works for Chemistry\n",
    "\n",
    "- **Chemical intuition**: Molecular properties are determined by local atomic environments\n",
    "- **Scalability**: Efficient on large molecules (linear in number of atoms/bonds)\n",
    "- **Flexibility**: Same architecture works for different properties\n",
    "- **Interpretability**: Can visualize learned representations and attention weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7UlzqvEMTj_"
   },
   "source": [
    "<a id='mpnn-implementation'></a>\n",
    "# 5. Implementing MPNN with PyTorch Geometric\n",
    "\n",
    "**\u23f1\ufe0f Expected time:** 20 minutes\n",
    "\n",
    "Now let's implement a complete MPNN using PyTorch Geometric and PyTorch Lightning!\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "Our MPNN will have the following components:\n",
    "\n",
    "1. **Atom Encoder**: Embed atom features into continuous space\n",
    "2. **Bond Encoder**: Embed bond features into continuous space\n",
    "3. **Message Passing Layers**: NNConv (Neural Network Convolution) with GRU updates\n",
    "4. **Global Pooling**: Aggregate atom representations to molecular representation\n",
    "5. **Prediction Head**: MLP to predict target property\n",
    "\n",
    "### Why NNConv?\n",
    "\n",
    "**NNConv** uses edge features (bond information) to modulate message passing:\n",
    "- Standard GCN: Ignores bond types\n",
    "- NNConv: Uses a neural network to compute edge-specific transformations\n",
    "- Perfect for molecules where bond types matter!\n",
    "\n",
    "### Why GRU?\n",
    "\n",
    "**GRU (Gated Recurrent Unit)** for state updates:\n",
    "- Handles iterative refinement of atom states\n",
    "- Prevents gradient vanishing\n",
    "- Learns when to keep vs. update information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixUBQLD0MTj_"
   },
   "outputs": [],
   "source": [
    "print_section_header(\"MPNN Implementation\", \"\ud83e\udde0\")\n",
    "\n",
    "from torch.nn import GRU\n",
    "\n",
    "class MPNN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim,\n",
    "        out_dim,\n",
    "        std,\n",
    "        train_data,\n",
    "        valid_data,\n",
    "        test_data,\n",
    "        batch_size=32,\n",
    "        lr=1e-3,\n",
    "        num_message_passing_steps=3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['train_data', 'valid_data', 'test_data'])\n",
    "\n",
    "        # Store dataset references\n",
    "        self.std = std\n",
    "        self.train_data = train_data\n",
    "        self.valid_data = valid_data\n",
    "        self.test_data = test_data\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.num_mp_steps = num_message_passing_steps\n",
    "\n",
    "        # Initial embedding layers (from OGB)\n",
    "        self.atom_emb = AtomEncoder(emb_dim=hidden_dim)\n",
    "        self.bond_emb = BondEncoder(emb_dim=hidden_dim)\n",
    "\n",
    "        # Message passing layers\n",
    "        # NNConv: Neural Network Convolution with edge features\n",
    "        nn_layer = MLP([hidden_dim, hidden_dim*2, hidden_dim*hidden_dim])\n",
    "        self.conv = NNConv(hidden_dim, hidden_dim, nn_layer, aggr='mean')\n",
    "\n",
    "        # GRU for node state updates\n",
    "        self.gru = GRU(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Readout layer (graph-level representation)\n",
    "        self.mlp = MLP([hidden_dim, int(hidden_dim/2), out_dim])\n",
    "\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        # Initialization: embed atom and bond features\n",
    "        x = self.atom_emb(data.x)  # [num_atoms, hidden_dim]\n",
    "        h = x.unsqueeze(0)  # [1, num_atoms, hidden_dim] for GRU\n",
    "        edge_attr = self.bond_emb(data.edge_attr)  # [num_edges, hidden_dim]\n",
    "\n",
    "        # Message passing iterations\n",
    "        for i in range(self.num_mp_steps):\n",
    "            # 1. Generate and aggregate messages\n",
    "            m = F.relu(self.conv(x, data.edge_index, edge_attr))\n",
    "\n",
    "            # 2. Update node states with GRU\n",
    "            x, h = self.gru(m.unsqueeze(0), h)\n",
    "            x = x.squeeze(0)\n",
    "\n",
    "        # Readout: aggregate node features to graph-level\n",
    "        x = global_add_pool(x, data.batch)  # [num_graphs, hidden_dim]\n",
    "\n",
    "        # Prediction\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x.view(-1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        out = self.forward(batch, mode=\"train\")\n",
    "        loss = F.mse_loss(out, batch.y)\n",
    "        self.log(\"train_loss\", loss, batch_size=len(batch.y), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        out = self.forward(batch, mode=\"valid\")\n",
    "        # Denormalize for meaningful metrics\n",
    "        loss = F.mse_loss(out * self.std, batch.y * self.std)\n",
    "        self.log(\"val_mse\", loss, batch_size=len(batch.y), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        out = self.forward(batch, mode=\"test\")\n",
    "        # Denormalize for meaningful metrics\n",
    "        loss = F.mse_loss(out * self.std, batch.y * self.std)\n",
    "        self.log(\"test_mse\", loss, batch_size=len(batch.y))\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=10,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_mse'\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return PyGDataLoader(self.train_data, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return PyGDataLoader(self.valid_data, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return PyGDataLoader(self.test_data, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "print(\"\u2713 MPNN class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGUv5jvcMTkC"
   },
   "outputs": [],
   "source": [
    "print_section_header(\"Create Model Instance\", \"\ud83c\udfd7\ufe0f\")\n",
    "\n",
    "# Create model\n",
    "model = MPNN(\n",
    "    hidden_dim=64,\n",
    "    out_dim=1,\n",
    "    std=scaler.scale_[0],\n",
    "    train_data=train_graphs,\n",
    "    valid_data=val_graphs,\n",
    "    test_data=test_graphs,\n",
    "    lr=0.001,\n",
    "    batch_size=32,\n",
    "    num_message_passing_steps=3\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model created successfully!\")\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Hidden dimension: {model.hparams.hidden_dim}\")\n",
    "print(f\"  Message passing steps: {model.num_mp_steps}\")\n",
    "print(f\"  Learning rate: {model.lr}\")\n",
    "print(f\"  Batch size: {model.batch_size}\")\n",
    "\n",
    "print(f\"\\n\u2713 Ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-I_jUvGnMTkC"
   },
   "source": [
    "<a id='training'></a>\n",
    "# 6. Training and Evaluation\n",
    "\n",
    "**\u23f1\ufe0f Expected time:** 5-10 minutes (depending on hardware)\n",
    "\n",
    "Time to train our MPNN and evaluate its performance!\n",
    "\n",
    "## Training Setup\n",
    "\n",
    "We'll use PyTorch Lightning's Trainer with:\n",
    "- **ModelCheckpoint**: Save the best model based on validation loss\n",
    "- **EarlyStopping**: Stop training if validation loss doesn't improve\n",
    "- **LearningRateMonitor**: Track learning rate changes\n",
    "- **Optional W&B logging**: Track experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eUvwCBRzMTkC"
   },
   "outputs": [],
   "source": [
    "print_section_header(\"Setup Training\", \"\ud83c\udfaf\")\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_mse',\n",
    "    dirpath='checkpoints',\n",
    "    filename='mpnn-{epoch:02d}-{val_mse:.4f}',\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_mse',\n",
    "    patience=20,\n",
    "    mode='min',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "# Create trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback, lr_monitor],\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    log_every_n_steps=10,\n",
    "    enable_progress_bar=True,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "print(f\"\u2713 Trainer configured\")\n",
    "print(f\"  Max epochs: 100\")\n",
    "print(f\"  Early stopping patience: 20 epochs\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jzfmWu4MTkC"
   },
   "outputs": [],
   "source": [
    "print_section_header(\"Training MPNN\", \"\ud83d\ude80\")\n",
    "\n",
    "print(f\"Starting training...\\n\")\n",
    "print(estimate_runtime(torch.cuda.is_available()))\n",
    "print()\n",
    "\n",
    "# Train model\n",
    "trainer.fit(model)\n",
    "\n",
    "print(f\"\\n\u2713 Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5h1BGeEQMTkC"
   },
   "outputs": [],
   "source": [
    "print_section_header(\"Test Set Evaluation\", \"\ud83d\udcc8\")\n",
    "\n",
    "# Test the best model\n",
    "test_results = trainer.test(model, ckpt_path='best')\n",
    "\n",
    "# Get predictions on test set\n",
    "model.eval()\n",
    "test_loader = PyGDataLoader(test_graphs, batch_size=64, shuffle=False)\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        preds = model(batch)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(batch.y.cpu().numpy())\n",
    "\n",
    "y_pred = np.concatenate(all_preds)\n",
    "y_true = np.concatenate(all_targets)\n",
    "\n",
    "# Denormalize predictions\n",
    "y_pred_denorm = y_pred * scaler.scale_[0] + scaler.mean_[0]\n",
    "y_true_denorm = y_true * scaler.scale_[0] + scaler.mean_[0]\n",
    "\n",
    "# Calculate metrics\n",
    "r2 = r2_score(y_true_denorm, y_pred_denorm)\n",
    "rmse = np.sqrt(mean_squared_error(y_true_denorm, y_pred_denorm))\n",
    "mae = mean_absolute_error(y_true_denorm, y_pred_denorm)\n",
    "\n",
    "metrics = {\n",
    "    'R\u00b2 Score': r2,\n",
    "    'RMSE': rmse,\n",
    "    'MAE': mae\n",
    "}\n",
    "\n",
    "print_metrics(metrics, \"Test Set Performance\")\n",
    "\n",
    "print(f\"\\n\u2713 Model achieves R\u00b2 = {r2:.4f} on test set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DreH7GODMTkC"
   },
   "outputs": [],
   "source": [
    "print_section_header(\"Visualize Results\", \"\ud83d\udcca\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Parity plot\n",
    "axes[0].scatter(y_true_denorm, y_pred_denorm, alpha=0.5, c=GOMES_COLORS['teal'], s=40, edgecolors='k', linewidth=0.5)\n",
    "axes[0].plot([y_true_denorm.min(), y_true_denorm.max()],\n",
    "             [y_true_denorm.min(), y_true_denorm.max()],\n",
    "             'r--', lw=2, label='Perfect prediction')\n",
    "axes[0].set_xlabel('True Log Solubility', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Predicted Log Solubility', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title(f'Parity Plot (R\u00b2={r2:.3f})', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals plot\n",
    "residuals = y_true_denorm - y_pred_denorm\n",
    "axes[1].scatter(y_pred_denorm, residuals, alpha=0.5, c=GOMES_COLORS['coral'], s=40, edgecolors='k', linewidth=0.5)\n",
    "axes[1].axhline(y=0, color='k', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted Log Solubility', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Residuals (True - Predicted)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title(f'Residuals Plot (MAE={mae:.3f})', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2713 The model shows good predictive performance!\")\n",
    "print(\"  - Points close to diagonal line = good predictions\")\n",
    "print(\"  - Residuals centered around 0 = unbiased predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHPUN_NAMTkC"
   },
   "source": [
    "<a id='representations'></a>\n",
    "# 7. Understanding Learned Representations\n",
    "\n",
    "**\u23f1\ufe0f Expected time:** 5 minutes\n",
    "\n",
    "Let's visualize what the GNN has learned by extracting and analyzing molecular embeddings!\n",
    "\n",
    "## What are Embeddings?\n",
    "\n",
    "Before making predictions, our MPNN creates a **fixed-size vector representation** for each molecule. These embeddings encode chemical information learned during training.\n",
    "\n",
    "We'll use **PCA (Principal Component Analysis)** to project high-dimensional embeddings to 2D for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LOjT3irNMTkC"
   },
   "outputs": [],
   "source": [
    "print_section_header(\"Extract Graph Embeddings\", \"\ud83c\udfa8\")\n",
    "\n",
    "# Modify forward pass to return embeddings\n",
    "def forward_with_embedding(model, data):\n",
    "    \"\"\"Modified forward pass that returns both prediction and embedding\"\"\"\n",
    "    # Initialization\n",
    "    x = model.atom_emb(data.x)\n",
    "    h = x.unsqueeze(0)\n",
    "    edge_attr = model.bond_emb(data.edge_attr)\n",
    "\n",
    "    # Message passing\n",
    "    for i in range(model.num_mp_steps):\n",
    "        m = F.relu(model.conv(x, data.edge_index, edge_attr))\n",
    "        x, h = model.gru(m.unsqueeze(0), h)\n",
    "        x = x.squeeze(0)\n",
    "\n",
    "    # Readout - this is our embedding!\n",
    "    embedding = global_add_pool(x, data.batch)\n",
    "\n",
    "    # Prediction\n",
    "    pred = model.mlp(embedding)\n",
    "\n",
    "    return pred.view(-1), embedding\n",
    "\n",
    "# Extract embeddings for all molecules\n",
    "model.eval()\n",
    "all_loader = PyGDataLoader(graph_list, batch_size=64, shuffle=False)\n",
    "\n",
    "all_embeddings = []\n",
    "all_targets = []\n",
    "\n",
    "print(\"Extracting embeddings...\")\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(all_loader):\n",
    "        batch = batch.to(device)\n",
    "        _, embeddings = forward_with_embedding(model, batch)\n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "        # Denormalize targets for visualization\n",
    "        targets_denorm = batch.y.cpu().numpy() * scaler.scale_[0] + scaler.mean_[0]\n",
    "        all_targets.append(targets_denorm)\n",
    "\n",
    "embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "print(f\"\\n\u2713 Extracted embeddings\")\n",
    "print(f\"  Shape: {embeddings.shape}\")\n",
    "print(f\"  Embedding dimension: {embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bWzY4idTMTkC"
   },
   "outputs": [],
   "source": [
    "print_section_header(\"PCA Visualization\", \"\ud83d\udd0d\")\n",
    "\n",
    "# Standardize embeddings before PCA\n",
    "from sklearn.preprocessing import StandardScaler as Scaler\n",
    "emb_scaler = Scaler()\n",
    "embeddings_scaled = emb_scaler.fit_transform(embeddings)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings_scaled)\n",
    "\n",
    "print(f\"PCA Results:\")\n",
    "print(f\"  PC1 explains {pca.explained_variance_ratio_[0]:.2%} of variance\")\n",
    "print(f\"  PC2 explains {pca.explained_variance_ratio_[1]:.2%} of variance\")\n",
    "print(f\"  Total explained: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# Create visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    embeddings_2d[:, 0],\n",
    "    embeddings_2d[:, 1],\n",
    "    c=targets,\n",
    "    cmap='viridis',\n",
    "    s=40,\n",
    "    alpha=0.6,\n",
    "    edgecolors='k',\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)',\n",
    "              fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)',\n",
    "              fontsize=12, fontweight='bold')\n",
    "ax.set_title('Molecular Embeddings in 2D (PCA)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('Log Solubility (mol/L)', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2713 Embedding Visualization Complete!\")\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"  - Similar molecules cluster together\")\n",
    "print(\"  - Color gradient shows solubility patterns\")\n",
    "print(\"  - GNN learned chemically meaningful representations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMDbMbKsMTkC"
   },
   "source": [
    "<a id='chemprop'></a>\n",
    "# 8. Chemprop - Production-Ready GNNs\n",
    "\n",
    "**\u23f1\ufe0f Expected time:** 10 minutes\n",
    "\n",
    "While building custom GNNs is educational, in production you often want battle-tested tools. Enter **Chemprop**!\n",
    "\n",
    "## What is Chemprop?\n",
    "\n",
    "**Chemprop** is a production-ready molecular property prediction framework developed at MIT:\n",
    "- **Optimized D-MPNN**: Directed message passing with extensive hyperparameter tuning\n",
    "- **Easy CLI**: Train models with a single command\n",
    "- **Uncertainty quantification**: Built-in ensemble and dropout methods\n",
    "- **Transfer learning**: Pre-trained models for low-data scenarios\n",
    "- **Well-tested**: Used in numerous drug discovery projects\n",
    "\n",
    "**Reference:** Yang et al. (2019). Analyzing Learned Molecular Representations for Property Prediction. *J. Chem. Inf. Model.*\n",
    "\n",
    "### When to Use Custom GNNs vs. Chemprop?\n",
    "\n",
    "**Custom GNNs (like our MPNN)**:\n",
    "- Research and experimentation\n",
    "- Novel architectures\n",
    "- Custom featurization\n",
    "- Learning and teaching\n",
    "\n",
    "**Chemprop**:\n",
    "- Production deployments\n",
    "- Quick baselines\n",
    "- Limited ML expertise\n",
    "- Proven reliability needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjPUO7TtMTkC"
   },
   "outputs": [],
   "source": [
    "print_section_header(\"Chemprop Training\", \"\u26a1\")\n",
    "\n",
    "print(\"Chemprop v2.2.1 CLI Training\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"Basic training command:\")\n",
    "print()\n",
    "print(\"  !chemprop train \\\\\")\n",
    "print(\"      --data-path data/esol.csv \\\\\")\n",
    "print(\"      --task-type regression \\\\\")\n",
    "print(\"      --output-dir chemprop_output \\\\\")\n",
    "print(\"      --epochs 50 \\\\\")\n",
    "print(\"      --batch-size 50 \\\\\")\n",
    "print(\"      --split-type random \\\\\")\n",
    "print(\"      --split-sizes 0.7 0.1 0.2\")\n",
    "print()\n",
    "print(\"Prediction command:\")\n",
    "print()\n",
    "print(\"  !chemprop predict \\\\\")\n",
    "print(\"      --model-path chemprop_output/model.pt \\\\\")\n",
    "print(\"      --data-path test.csv \\\\\")\n",
    "print(\"      --output-path predictions.csv\")\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"Note: Chemprop training is commented out to keep notebook runtime short.\")\n",
    "print(\"Uncomment the commands above to train a Chemprop model!\")\n",
    "print()\n",
    "print(\"Expected performance: R\u00b2 ~ 0.90-0.92 (slightly better than our MPNN)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLqSV7oHMTkC"
   },
   "source": [
    "### Chemprop Advanced Features\n",
    "\n",
    "**Ensemble Models**:\n",
    "```bash\n",
    "chemprop train \\\n",
    "    --data-path data/esol.csv \\\n",
    "    --ensemble-size 5  # Train 5 models for uncertainty\n",
    "```\n",
    "\n",
    "**Hyperparameter Optimization**:\n",
    "```bash\n",
    "chemprop hyper opt\\\n",
    "    --data-path data/esol.csv \\\n",
    "    --num-iters 20\n",
    "```\n",
    "\n",
    "**Transfer Learning**:\n",
    "```bash\n",
    "chemprop train \\\n",
    "    --data-path small_dataset.csv \\\n",
    "    --checkpoint-path pretrained_model.pt  # Fine-tune\n",
    "```\n",
    "\n",
    "**Interpretability**:\n",
    "```bash\n",
    "chemprop interpret \\\n",
    "    --model-path model.pt \\\n",
    "    --data-path molecules.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crNWuOm5MTkC"
   },
   "source": [
    "<a id='debugging'></a>\n",
    "# 9. Common Pitfalls and Debugging\n",
    "\n",
    "**\u23f1\ufe0f Expected time:** 5 minutes\n",
    "\n",
    "GNN training can be tricky! Here are common issues and solutions.\n",
    "\n",
    "## Issue 1: GPU Not Detected\n",
    "\n",
    "**Symptom**: Model trains on CPU (very slow)\n",
    "\n",
    "**Solution**:\n",
    "1. In Colab: Runtime \u2192 Change runtime type \u2192 Hardware accelerator \u2192 GPU\n",
    "2. Check `torch.cuda.is_available()` returns `True`\n",
    "3. Restart runtime if needed\n",
    "\n",
    "## Issue 2: CUDA Out of Memory\n",
    "\n",
    "**Symptom**: `RuntimeError: CUDA out of memory`\n",
    "\n",
    "**Solutions**:\n",
    "```python\n",
    "# 1. Reduce batch size\n",
    "model = MPNN(..., batch_size=16)  # Instead of 32\n",
    "\n",
    "# 2. Reduce model size\n",
    "model = MPNN(hidden_dim=32, ...)  # Instead of 64\n",
    "\n",
    "# 3. Use gradient accumulation\n",
    "trainer = pl.Trainer(accumulate_grad_batches=2)\n",
    "```\n",
    "\n",
    "## Issue 3: Model Not Learning (High Loss)\n",
    "\n",
    "**Symptom**: Validation loss stays constant or very high\n",
    "\n",
    "**Solutions**:\n",
    "```python\n",
    "# 1. Check target normalization\n",
    "print(f\"Target mean: {y_train.mean()}, std: {y_train.std()}\")\n",
    "# Should be close to 0 and 1 after normalization\n",
    "\n",
    "# 2. Increase learning rate\n",
    "model = MPNN(..., lr=0.01)  # Try 10x higher\n",
    "\n",
    "# 3. Check for NaN values\n",
    "print(f\"NaN in data: {torch.isnan(batch.y).sum()}\")\n",
    "```\n",
    "\n",
    "## Issue 4: Overfitting\n",
    "\n",
    "**Symptom**: Train loss << validation loss\n",
    "\n",
    "**Solutions**:\n",
    "```python\n",
    "# 1. Add dropout\n",
    "self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "# 2. Reduce model capacity\n",
    "model = MPNN(hidden_dim=32, num_message_passing_steps=2)\n",
    "\n",
    "# 3. Use weight decay\n",
    "optimizer = torch.optim.Adam(params, lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# 4. More data augmentation\n",
    "# SMILES enumeration, scaffold splitting, etc.\n",
    "```\n",
    "\n",
    "## Issue 5: Slow Training\n",
    "\n",
    "**Solutions**:\n",
    "```python\n",
    "# 1. Use DataLoader num_workers\n",
    "loader = PyGDataLoader(dataset, batch_size=32, num_workers=4)\n",
    "\n",
    "# 2. Mixed precision training\n",
    "trainer = pl.Trainer(precision='16-mixed')\n",
    "\n",
    "# 3. Compile model (PyTorch 2.0+)\n",
    "model = torch.compile(model)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tB_VGE7rMTkC"
   },
   "outputs": [],
   "source": [
    "print_section_header(\"Debugging Checklist\", \"\ud83d\udd27\")\n",
    "\n",
    "print(\"Run this cell to check your setup:\\n\")\n",
    "\n",
    "# 1. Device check\n",
    "print(f\"1. Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   \u2713 GPU available\")\n",
    "else:\n",
    "    print(f\"   \u26a0\ufe0f  CPU only (training will be slow)\")\n",
    "\n",
    "# 2. Data check\n",
    "print(f\"\\n2. Data:\")\n",
    "print(f\"   Train samples: {len(train_graphs)}\")\n",
    "print(f\"   Val samples: {len(val_graphs)}\")\n",
    "print(f\"   Test samples: {len(test_graphs)}\")\n",
    "\n",
    "# 3. Normalization check\n",
    "train_targets_norm = np.array([g.y.item() for g in train_graphs])\n",
    "print(f\"\\n3. Target normalization:\")\n",
    "print(f\"   Mean: {train_targets_norm.mean():.3f} (should be ~0)\")\n",
    "print(f\"   Std: {train_targets_norm.std():.3f} (should be ~1)\")\n",
    "if abs(train_targets_norm.mean()) < 0.1 and abs(train_targets_norm.std() - 1) < 0.1:\n",
    "    print(f\"   \u2713 Targets properly normalized\")\n",
    "else:\n",
    "    print(f\"   \u26a0\ufe0f  Check normalization!\")\n",
    "\n",
    "# 4. Model check\n",
    "print(f\"\\n4. Model:\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   Device: {next(model.parameters()).device}\")\n",
    "\n",
    "# 5. Batch check\n",
    "print(f\"\\n5. Data loading:\")\n",
    "test_loader = PyGDataLoader(train_graphs[:10], batch_size=5)\n",
    "test_batch = next(iter(test_loader))\n",
    "print(f\"   Batch size: {test_batch.num_graphs}\")\n",
    "print(f\"   Total nodes: {test_batch.num_nodes}\")\n",
    "print(f\"   Total edges: {test_batch.num_edges}\")\n",
    "print(f\"   \u2713 DataLoader working\")\n",
    "\n",
    "print(f\"\\n\u2713 All checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6bZFi6aMTkC"
   },
   "source": [
    "<a id='extensions'></a>\n",
    "# 10. Extensions and Next Steps\n",
    "\n",
    "**\u23f1\ufe0f Expected time:** 15-20 minutes (optional advanced topics)\n",
    "\n",
    "Congratulations on completing the core tutorial! Here are directions for further exploration.\n",
    "\n",
    "## 10.1 Advanced GNN Architectures\n",
    "\n",
    "### Graph Attention Networks (GAT)\n",
    "- Learn attention weights for neighbor aggregation\n",
    "- More interpretable than standard MPNNs\n",
    "- Implementation: `from torch_geometric.nn import GATConv`\n",
    "\n",
    "### Graph Isomorphism Networks (GIN)\n",
    "- Maximally expressive GNN architecture\n",
    "- Theoretical guarantees on distinguishing graphs\n",
    "- Implementation: `from torch_geometric.nn import GINConv`\n",
    "\n",
    "### Equivariant GNNs (E(3)-GNN, SchNet, DimeNet)\n",
    "- Incorporate 3D molecular geometry\n",
    "- Respect physical symmetries\n",
    "- Better for quantum properties\n",
    "\n",
    "## 10.2 Hyperparameter Optimization\n",
    "\n",
    "Use Bayesian Optimization to find best hyperparameters:\n",
    "```python\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "def train_and_evaluate(hidden_dim, lr, num_mp_steps):\n",
    "    model = MPNN(\n",
    "        hidden_dim=int(hidden_dim),\n",
    "        lr=lr,\n",
    "        num_message_passing_steps=int(num_mp_steps),\n",
    "        ...\n",
    "    )\n",
    "    trainer = pl.Trainer(max_epochs=30)\n",
    "    trainer.fit(model)\n",
    "    results = trainer.test(model)\n",
    "    return -results[0]['test_mse']  # Negative because we maximize\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=train_and_evaluate,\n",
    "    pbounds={\n",
    "        'hidden_dim': (32, 128),\n",
    "        'lr': (0.0001, 0.01),\n",
    "        'num_mp_steps': (2, 5)\n",
    "    }\n",
    ")\n",
    "\n",
    "optimizer.maximize(n_iter=20)\n",
    "```\n",
    "\n",
    "## 10.3 Uncertainty Quantification\n",
    "\n",
    "### Ensemble Methods\n",
    "Train multiple models with different initializations:\n",
    "```python\n",
    "ensemble = []\n",
    "for i in range(5):\n",
    "    set_seed(42 + i)\n",
    "    model = MPNN(...)\n",
    "    trainer.fit(model)\n",
    "    ensemble.append(model)\n",
    "\n",
    "# Predict with ensemble\n",
    "predictions = [model(batch) for model in ensemble]\n",
    "mean_pred = torch.stack(predictions).mean(dim=0)\n",
    "std_pred = torch.stack(predictions).std(dim=0)  # Uncertainty!\n",
    "```\n",
    "\n",
    "### MC Dropout\n",
    "Enable dropout during inference:\n",
    "```python\n",
    "def predict_with_uncertainty(model, batch, n_samples=50):\n",
    "    model.train()  # Enable dropout\n",
    "    predictions = [model(batch) for _ in range(n_samples)]\n",
    "    model.eval()\n",
    "    return torch.stack(predictions).mean(dim=0), torch.stack(predictions).std(dim=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6 Model Explainability with Captum\n",
    "\n",
    "**Understanding WHY your GNN makes specific predictions**\n",
    "\n",
    "While achieving high predictive accuracy is important, understanding *why* a model makes certain predictions is equally crucial in drug discovery and molecular design. This section demonstrates how to use **Captum** and **PyTorch Geometric's explainability tools** to identify which atoms and bonds are most important for predictions.\n",
    "\n",
    "### Why Explainability Matters in Chemistry\n",
    "\n",
    "- **Trust**: Validate that the model learns chemically meaningful patterns\n",
    "- **Discovery**: Identify key structural motifs responsible for properties\n",
    "- **Safety**: Detect when models rely on spurious correlations\n",
    "- **Design**: Guide molecular optimization by understanding structure-activity relationships\n",
    "\n",
    "### Explainability Approaches\n",
    "\n",
    "We'll cover three complementary methods:\n",
    "\n",
    "1. **Saliency Maps**: Gradient-based attribution showing which features matter\n",
    "2. **Integrated Gradients**: More robust gradient-based method with theoretical guarantees\n",
    "3. **Visual Analysis**: Mapping attributions back to molecular structures\n",
    "\n",
    "### Recent Research\n",
    "\n",
    "Recent studies (2024-2025) have shown that combining Captum's Integrated Gradients with GNN-specific explainers improves both accuracy and interpretability in drug discovery applications [[Nature Scientific Reports, 2024](https://www.nature.com/articles/s41598-024-83090-3)].\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Install and configure Captum for graph neural networks\n",
    "- Compute atom and bond importance scores\n",
    "- Visualize explanations on molecular structures\n",
    "- Validate explanations against chemical intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Installing Captum for GNN Explainability\", \"\ud83d\udd0d\")\n",
    "\n",
    "# Install Captum if not already installed\n",
    "try:\n",
    "    import captum\n",
    "    print(f\"\u2713 Captum already installed (version {captum.__version__})\")\n",
    "except ImportError:\n",
    "    print(\"Installing Captum...\")\n",
    "    !uv pip install -q captum\n",
    "    import captum\n",
    "    print(f\"\u2713 Captum installed (version {captum.__version__})\")\n",
    "\n",
    "# Import explainability modules\n",
    "from captum.attr import Saliency, IntegratedGradients\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"\\n\u2713 Explainability tools ready!\")\n",
    "print(\"  - Saliency: Fast gradient-based attribution\")\n",
    "print(\"  - IntegratedGradients: Robust attribution with theoretical guarantees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory: Feature Attribution for GNNs\n",
    "\n",
    "**Goal**: For a given prediction $\\hat{y} = f(G)$, determine the importance of each edge (bond) in the molecular graph $G$.\n",
    "\n",
    "#### Method 1: Saliency (Gradient-Based Attribution)\n",
    "\n",
    "The **Saliency method** uses the gradient of the model output with respect to edge weights:\n",
    "\n",
    "$$\n",
    "\\text{Attribution}_{e_i} = \\left| \\frac{\\partial f(G)}{\\partial w_{e_i}} \\right|\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $f(G)$ is the GNN prediction for graph $G$\n",
    "- $w_{e_i}$ is the weight of edge $e_i$ (initially 1.0 for all edges)\n",
    "- The absolute value gives us the magnitude of importance\n",
    "\n",
    "**Intuition**: If a small change in edge $e_i$ causes a large change in the prediction, that edge is important.\n",
    "\n",
    "#### Method 2: Integrated Gradients\n",
    "\n",
    "**Integrated Gradients** is more robust, accumulating gradients along a path from a baseline to the actual input:\n",
    "\n",
    "$$\n",
    "\\text{Attribution}_{e_i} = (w_{e_i} - w_{e_i}^{\\text{baseline}}) \\times \\int_{\\alpha=0}^{1} \\frac{\\partial f(G_{\\alpha})}{\\partial w_{e_i}} d\\alpha\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $G_{\\alpha}$ is a graph with edge weights interpolated: $w_{e_i}^{\\alpha} = w_{e_i}^{\\text{baseline}} + \\alpha \\cdot (w_{e_i} - w_{e_i}^{\\text{baseline}})$\n",
    "- Baseline: graph with all edge weights = 0 (no edges)\n",
    "- Actual input: graph with all edge weights = 1 (all edges present)\n",
    "- The integral is approximated using $n$ discrete steps\n",
    "\n",
    "**Advantages of Integrated Gradients:**\n",
    "- **Axiom 1 (Completeness)**: Attributions sum to difference between baseline and prediction\n",
    "- **Axiom 2 (Sensitivity)**: If a feature affects output, it gets non-zero attribution\n",
    "- More stable than vanilla gradients, less susceptible to saturation\n",
    "\n",
    "**Reference**: [Sundararajan et al. (2017). Axiomatic Attribution for Deep Networks. ICML.](https://arxiv.org/abs/1703.01365)\n",
    "\n",
    "#### PyTorch Geometric + Captum Integration\n",
    "\n",
    "PyTorch Geometric (v2.6+) provides native integration with Captum through:\n",
    "- `CaptumExplainer`: Wrapper for Captum attribution methods\n",
    "- `to_captum_model`: Converts PyG models to Captum-compatible format\n",
    "- Supports both homogeneous and heterogeneous graphs\n",
    "\n",
    "For molecular graphs, we'll create custom wrapper functions that:\n",
    "1. Convert edge weights into learnable parameters\n",
    "2. Forward pass through the GNN with edge weight masking\n",
    "3. Compute gradients with respect to edge weights\n",
    "4. Return attributions for each bond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Implementing GNN Explainability Functions\", \"\ud83d\udee0\ufe0f\")\n",
    "\n",
    "def model_forward_for_explainability(edge_mask, data, model, target_idx=0):\n",
    "    \"\"\"\n",
    "    Forward pass through the model with edge masking for explainability.\n",
    "    \n",
    "    Args:\n",
    "        edge_mask: Tensor of shape [num_edges] representing edge importance weights\n",
    "        data: PyG Data object containing molecular graph\n",
    "        model: Trained GNN model\n",
    "        target_idx: Index of target (0 for regression, class index for classification)\n",
    "    \n",
    "    Returns:\n",
    "        Output prediction (scalar for regression)\n",
    "    \"\"\"\n",
    "    # Create batch tensor for single molecule\n",
    "    batch = torch.zeros(data.x.shape[0], dtype=torch.long, device=data.x.device)\n",
    "    \n",
    "    # Create a modified data object with edge weights\n",
    "    data_masked = data.clone()\n",
    "    \n",
    "    # For our MPNN model, we need to handle edge masking\n",
    "    # We'll weight the edge features by the mask\n",
    "    if hasattr(data_masked, 'edge_attr') and data_masked.edge_attr is not None:\n",
    "        # Expand edge_mask to match edge_attr dimensions\n",
    "        edge_mask_expanded = edge_mask.unsqueeze(1).expand_as(\n",
    "            model.bond_emb(data_masked.edge_attr)\n",
    "        )\n",
    "    \n",
    "    # Forward pass with custom edge masking\n",
    "    x = model.atom_emb(data_masked.x)\n",
    "    h = x.unsqueeze(0)\n",
    "    edge_attr = model.bond_emb(data_masked.edge_attr)\n",
    "    \n",
    "    # Apply edge masking to edge features\n",
    "    if hasattr(data_masked, 'edge_attr') and data_masked.edge_attr is not None:\n",
    "        edge_attr = edge_attr * edge_mask.unsqueeze(1)\n",
    "    \n",
    "    # Message passing with masked edges\n",
    "    for i in range(model.num_mp_steps):\n",
    "        m = F.relu(model.conv(x, data_masked.edge_index, edge_attr))\n",
    "        x, h = model.gru(m.unsqueeze(0), h)\n",
    "        x = x.squeeze(0)\n",
    "    \n",
    "    # Readout\n",
    "    x = global_add_pool(x, batch)\n",
    "    out = model.mlp(x)\n",
    "    \n",
    "    return out.squeeze()\n",
    "\n",
    "\n",
    "def explain_molecule(\n",
    "    data,\n",
    "    model,\n",
    "    method='integrated_gradients',\n",
    "    n_steps=50,\n",
    "    device='cpu'\n",
    "):\n",
    "    \"\"\"\n",
    "    Explain a molecular graph prediction using Captum.\n",
    "    \n",
    "    Args:\n",
    "        data: PyG Data object (single molecule)\n",
    "        model: Trained GNN model\n",
    "        method: 'saliency' or 'integrated_gradients'\n",
    "        n_steps: Number of integration steps (for IG)\n",
    "        device: Device to run on\n",
    "    \n",
    "    Returns:\n",
    "        edge_attributions: Numpy array of shape [num_edges] with importance scores\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    data = data.to(device)\n",
    "    \n",
    "    # Create edge mask (all edges present initially)\n",
    "    num_edges = data.edge_index.shape[1]\n",
    "    edge_mask = torch.ones(num_edges, requires_grad=True, device=device)\n",
    "    \n",
    "    # Wrap model forward function\n",
    "    def forward_func(edge_mask):\n",
    "        return model_forward_for_explainability(edge_mask, data, model)\n",
    "    \n",
    "    # Compute attributions\n",
    "    if method == 'saliency':\n",
    "        explainer = Saliency(forward_func)\n",
    "        attributions = explainer.attribute(edge_mask, abs=True)\n",
    "    elif method == 'integrated_gradients':\n",
    "        # Baseline: no edges (edge_mask = 0)\n",
    "        baseline = torch.zeros_like(edge_mask)\n",
    "        explainer = IntegratedGradients(forward_func)\n",
    "        attributions = explainer.attribute(\n",
    "            edge_mask,\n",
    "            baselines=baseline,\n",
    "            n_steps=n_steps\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    # Convert to numpy and normalize\n",
    "    attr = attributions.detach().cpu().numpy()\n",
    "    attr = np.abs(attr)\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    if attr.max() > 0:\n",
    "        attr = attr / attr.max()\n",
    "    \n",
    "    return attr\n",
    "\n",
    "\n",
    "print(\"\u2713 Explainability functions defined:\")\n",
    "print(\"  - model_forward_for_explainability(): Forward pass with edge masking\")\n",
    "print(\"  - explain_molecule(): Compute attributions using Captum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Molecular Visualization with Attributions\", \"\ud83c\udfa8\")\n",
    "\n",
    "def aggregate_edge_directions(edge_attributions, data):\n",
    "    \"\"\"\n",
    "    Aggregate attributions for undirected edges (molecular graphs).\n",
    "    Since we add edges in both directions, average the attributions.\n",
    "    \n",
    "    Args:\n",
    "        edge_attributions: Array of shape [num_edges] with attributions\n",
    "        data: PyG Data object\n",
    "    \n",
    "    Returns:\n",
    "        dict: {(u, v): attribution} where u < v (undirected edges)\n",
    "    \"\"\"\n",
    "    edge_dict = defaultdict(list)\n",
    "    \n",
    "    for i, (u, v) in enumerate(data.edge_index.t().tolist()):\n",
    "        # Make undirected: always use (min, max) order\n",
    "        edge_key = (min(u, v), max(u, v))\n",
    "        edge_dict[edge_key].append(edge_attributions[i])\n",
    "    \n",
    "    # Average attributions for both directions\n",
    "    aggregated = {k: np.mean(v) for k, v in edge_dict.items()}\n",
    "    return aggregated\n",
    "\n",
    "\n",
    "def draw_molecule_with_attributions(\n",
    "    smiles,\n",
    "    edge_attributions_dict,\n",
    "    title=\"Molecular Attribution Map\",\n",
    "    figsize=(12, 8)\n",
    "):\n",
    "    \"\"\"\n",
    "    Draw molecule with edges colored by attribution scores.\n",
    "    \n",
    "    Args:\n",
    "        smiles: SMILES string\n",
    "        edge_attributions_dict: {(u, v): attribution_score}\n",
    "        title: Plot title\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        print(f\"Could not parse SMILES: {smiles}\")\n",
    "        return\n",
    "    \n",
    "    # Create NetworkX graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes with atom labels\n",
    "    for atom in mol.GetAtoms():\n",
    "        G.add_node(atom.GetIdx(), label=atom.GetSymbol())\n",
    "    \n",
    "    # Add edges\n",
    "    for bond in mol.GetBonds():\n",
    "        u = bond.GetBeginAtomIdx()\n",
    "        v = bond.GetEndAtomIdx()\n",
    "        G.add_edge(u, v)\n",
    "    \n",
    "    # Generate layout\n",
    "    pos = nx.spring_layout(G, seed=42, iterations=50)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Draw nodes\n",
    "    node_colors = [GOMES_COLORS['navy']] * len(G.nodes())\n",
    "    nx.draw_networkx_nodes(\n",
    "        G, pos,\n",
    "        node_color=node_colors,\n",
    "        node_size=600,\n",
    "        ax=ax,\n",
    "        alpha=0.9\n",
    "    )\n",
    "    \n",
    "    # Draw node labels\n",
    "    labels = {n: G.nodes[n]['label'] for n in G.nodes()}\n",
    "    nx.draw_networkx_labels(\n",
    "        G, pos,\n",
    "        labels=labels,\n",
    "        font_size=12,\n",
    "        font_color='white',\n",
    "        font_weight='bold',\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    # Draw edges with attribution-based coloring\n",
    "    edge_colors = []\n",
    "    edge_widths = []\n",
    "    \n",
    "    for (u, v) in G.edges():\n",
    "        edge_key = (min(u, v), max(u, v))\n",
    "        attr = edge_attributions_dict.get(edge_key, 0.0)\n",
    "        edge_colors.append(attr)\n",
    "        edge_widths.append(1 + 8 * attr)  # Width proportional to importance\n",
    "    \n",
    "    edges = nx.draw_networkx_edges(\n",
    "        G, pos,\n",
    "        width=edge_widths,\n",
    "        edge_color=edge_colors,\n",
    "        edge_cmap=plt.cm.Reds,\n",
    "        edge_vmin=0,\n",
    "        edge_vmax=1,\n",
    "        ax=ax,\n",
    "        alpha=0.8\n",
    "    )\n",
    "    \n",
    "    # Add colorbar\n",
    "    sm = plt.cm.ScalarMappable(\n",
    "        cmap=plt.cm.Reds,\n",
    "        norm=plt.Normalize(vmin=0, vmax=1)\n",
    "    )\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Edge Importance', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax.set_title(title, fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"\u2713 Visualization functions ready:\")\n",
    "print(\"  - aggregate_edge_directions(): Merge bidirectional edges\")\n",
    "print(\"  - draw_molecule_with_attributions(): Visualize importance on structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Select Example Molecules for Explanation\", \"\ud83e\uddea\")\n",
    "\n",
    "# Get examples with different predicted solubilities\n",
    "# High solubility, medium, and low\n",
    "\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "# Get predictions for test set\n",
    "test_predictions = []\n",
    "test_smiles = []\n",
    "\n",
    "for idx in test_idx:\n",
    "    graph = graph_list[idx]\n",
    "    smiles = df.iloc[idx][smiles_column]\n",
    "    \n",
    "    # Get prediction\n",
    "    graph_device = graph.clone().to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(graph_device).cpu().item()\n",
    "        # Denormalize\n",
    "        pred_denorm = pred * scaler.scale_[0] + scaler.mean_[0]\n",
    "    \n",
    "    test_predictions.append(pred_denorm)\n",
    "    test_smiles.append((idx, smiles, pred_denorm))\n",
    "\n",
    "# Sort by prediction\n",
    "test_smiles.sort(key=lambda x: x[2])\n",
    "\n",
    "# Select examples: low, medium, high solubility\n",
    "examples_to_explain = [\n",
    "    test_smiles[0],  # Lowest predicted solubility\n",
    "    test_smiles[len(test_smiles)//2],  # Medium\n",
    "    test_smiles[-1],  # Highest predicted solubility\n",
    "]\n",
    "\n",
    "print(\"Selected molecules for explanation:\\n\")\n",
    "for i, (idx, smiles, pred) in enumerate(examples_to_explain):\n",
    "    true_val = df.iloc[idx][target_column]\n",
    "    print(f\"{i+1}. Low solubility\" if i == 0 else f\"{i+1}. Medium solubility\" if i == 1 else f\"{i+1}. High solubility\")\n",
    "    print(f\"   SMILES: {smiles}\")\n",
    "    print(f\"   True:      {true_val:.3f} log(mol/L)\")\n",
    "    print(f\"   Predicted: {pred:.3f} log(mol/L)\")\n",
    "    print()\n",
    "\n",
    "# Display structures\n",
    "print(\"Molecular structures:\")\n",
    "mols = [Chem.MolFromSmiles(x[1]) for x in examples_to_explain]\n",
    "legends = [f\"Low Solubility\\nPred: {examples_to_explain[0][2]:.2f}\",\n",
    "           f\"Medium Solubility\\nPred: {examples_to_explain[1][2]:.2f}\",\n",
    "           f\"High Solubility\\nPred: {examples_to_explain[2][2]:.2f}\"]\n",
    "\n",
    "img = Draw.MolsToGridImage(\n",
    "    mols,\n",
    "    molsPerRow=3,\n",
    "    subImgSize=(300, 300),\n",
    "    legends=legends\n",
    ")\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Method 1: Saliency-Based Explanations\", \"\ud83d\udcca\")\n",
    "\n",
    "print(\"Computing saliency maps for example molecules...\\n\")\n",
    "\n",
    "for i, (idx, smiles, pred) in enumerate(examples_to_explain):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Example {i+1}: {'Low' if i==0 else 'Medium' if i==1 else 'High'} Solubility Molecule\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Get graph data\n",
    "    data = graph_list[idx].clone()\n",
    "    \n",
    "    # Compute saliency\n",
    "    print(\"Computing saliency attributions...\")\n",
    "    attributions = explain_molecule(\n",
    "        data=data,\n",
    "        model=model,\n",
    "        method='saliency',\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Aggregate for undirected edges\n",
    "    edge_attr_dict = aggregate_edge_directions(attributions, data)\n",
    "    \n",
    "    # Print top important bonds\n",
    "    sorted_edges = sorted(edge_attr_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(f\"\\nTop 5 most important bonds:\")\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    for rank, ((u, v), score) in enumerate(sorted_edges[:5], 1):\n",
    "        # Get atom symbols\n",
    "        atom_u = mol.GetAtomWithIdx(u).GetSymbol()\n",
    "        atom_v = mol.GetAtomWithIdx(v).GetSymbol()\n",
    "        print(f\"  {rank}. Bond {u}-{v} ({atom_u}-{atom_v}): {score:.4f}\")\n",
    "    \n",
    "    # Visualize\n",
    "    draw_molecule_with_attributions(\n",
    "        smiles=smiles,\n",
    "        edge_attributions_dict=edge_attr_dict,\n",
    "        title=f\"Saliency Map: {'Low' if i==0 else 'Medium' if i==1 else 'High'} Solubility (Pred: {pred:.2f})\"\n",
    "    )\n",
    "\n",
    "print(\"\\n\u2713 Saliency analysis complete!\")\n",
    "print(\"\\nInterpretation Guide:\")\n",
    "print(\"  - Darker red edges = More important for prediction\")\n",
    "print(\"  - Thicker edges = Higher attribution score\")\n",
    "print(\"  - Validate against chemical intuition (e.g., polar groups for solubility)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Method 2: Integrated Gradients Explanations\", \"\ud83c\udfaf\")\n",
    "\n",
    "print(\"Computing Integrated Gradients for example molecules...\")\n",
    "print(\"(This is more robust than Saliency but takes longer)\\n\")\n",
    "\n",
    "for i, (idx, smiles, pred) in enumerate(examples_to_explain):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Example {i+1}: {'Low' if i==0 else 'Medium' if i==1 else 'High'} Solubility Molecule\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Get graph data\n",
    "    data = graph_list[idx].clone()\n",
    "    \n",
    "    # Compute Integrated Gradients\n",
    "    print(\"Computing Integrated Gradients (50 steps)...\")\n",
    "    attributions = explain_molecule(\n",
    "        data=data,\n",
    "        model=model,\n",
    "        method='integrated_gradients',\n",
    "        n_steps=50,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Aggregate for undirected edges\n",
    "    edge_attr_dict = aggregate_edge_directions(attributions, data)\n",
    "    \n",
    "    # Print top important bonds\n",
    "    sorted_edges = sorted(edge_attr_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(f\"\\nTop 5 most important bonds:\")\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    for rank, ((u, v), score) in enumerate(sorted_edges[:5], 1):\n",
    "        # Get atom symbols\n",
    "        atom_u = mol.GetAtomWithIdx(u).GetSymbol()\n",
    "        atom_v = mol.GetAtomWithIdx(v).GetSymbol()\n",
    "        print(f\"  {rank}. Bond {u}-{v} ({atom_u}-{atom_v}): {score:.4f}\")\n",
    "    \n",
    "    # Visualize\n",
    "    draw_molecule_with_attributions(\n",
    "        smiles=smiles,\n",
    "        edge_attributions_dict=edge_attr_dict,\n",
    "        title=f\"Integrated Gradients: {'Low' if i==0 else 'Medium' if i==1 else 'High'} Solubility (Pred: {pred:.2f})\"\n",
    "    )\n",
    "\n",
    "print(\"\\n\u2713 Integrated Gradients analysis complete!\")\n",
    "print(\"\\nKey Differences from Saliency:\")\n",
    "print(\"  - IG accumulates gradients along path from baseline (no edges) to actual graph\")\n",
    "print(\"  - More stable and less affected by gradient saturation\")\n",
    "print(\"  - Better theoretical guarantees (completeness + sensitivity axioms)\")\n",
    "print(\"  - Generally produces cleaner, more interpretable attributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Results\n",
    "\n",
    "#### What to Look For:\n",
    "\n",
    "1. **Polar Groups for High Solubility**\n",
    "   - Hydroxyl groups (-OH)\n",
    "   - Carboxylic acids (-COOH)\n",
    "   - Amines (-NH2)\n",
    "   - These should show high attribution scores for high-solubility molecules\n",
    "\n",
    "2. **Hydrophobic Groups for Low Solubility**\n",
    "   - Long alkyl chains\n",
    "   - Large aromatic systems\n",
    "   - These should be highlighted for low-solubility molecules\n",
    "\n",
    "3. **Method Comparison**\n",
    "   - **Saliency**: Faster, but can be noisy\n",
    "   - **Integrated Gradients**: Slower, but more robust and interpretable\n",
    "   - In practice, IG often identifies more chemically meaningful patterns\n",
    "\n",
    "#### Validation Strategies:\n",
    "\n",
    "1. **Chemical Intuition**: Do the highlighted substructures make sense?\n",
    "2. **Literature**: Compare with known SAR (structure-activity relationships)\n",
    "3. **Consistency**: Do similar molecules show similar attribution patterns?\n",
    "4. **Ablation**: Remove highlighted substructures and check prediction change\n",
    "\n",
    "#### Common Pitfalls:\n",
    "\n",
    "- **Over-interpretation**: High attribution \u2260 causal relationship\n",
    "- **Context matters**: Same group can have different importance in different molecules\n",
    "- **Model artifacts**: Sometimes highlights dataset biases rather than true chemistry\n",
    "- **Multiple explanations**: Same prediction can have multiple valid explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices for GNN Explainability\n",
    "\n",
    "#### 1. Use Multiple Attribution Methods\n",
    "\n",
    "No single method is perfect. Compare results across:\n",
    "- Saliency (fast screening)\n",
    "- Integrated Gradients (robust baseline)\n",
    "- GNNExplainer (graph-specific, available in PyG)\n",
    "- Attention weights (for GAT models)\n",
    "\n",
    "#### 2. Aggregate Across Multiple Examples\n",
    "\n",
    "```python\n",
    "# Find common important substructures across a class\n",
    "high_solubility_mols = [...] # Filter by property\n",
    "common_attributions = []\n",
    "\n",
    "for mol in high_solubility_mols:\n",
    "    attr = explain_molecule(mol, model, 'integrated_gradients')\n",
    "    common_attributions.append(attr)\n",
    "\n",
    "# Average attributions to find consensus patterns\n",
    "consensus = np.mean(common_attributions, axis=0)\n",
    "```\n",
    "\n",
    "#### 3. Use Explainability During Training\n",
    "\n",
    "- **Model selection**: Choose models with interpretable explanations\n",
    "- **Debugging**: Identify when model learns spurious correlations\n",
    "- **Regularization**: Add losses that encourage sparse, interpretable explanations\n",
    "\n",
    "#### 4. Combine with Domain Knowledge\n",
    "\n",
    "- Consult medicinal chemists about highlighted substructures\n",
    "- Compare with known pharmacophores\n",
    "- Use as hypothesis generation, not truth\n",
    "\n",
    "#### 5. Quantitative Evaluation\n",
    "\n",
    "Where possible, validate explanations quantitatively:\n",
    "\n",
    "```python\n",
    "# Remove attributed substructure and measure prediction change\n",
    "def evaluate_attribution(mol, important_bonds):\n",
    "    original_pred = model(mol)\n",
    "    \n",
    "    # Create variant without important bonds\n",
    "    mol_modified = remove_bonds(mol, important_bonds)\n",
    "    modified_pred = model(mol_modified)\n",
    "    \n",
    "    # Large change = good attribution\n",
    "    return abs(original_pred - modified_pred)\n",
    "```\n",
    "\n",
    "### Further Resources\n",
    "\n",
    "**Documentation:**\n",
    "- [Captum Documentation](https://captum.ai/)\n",
    "- [PyG Explainability Tutorial](https://pytorch-geometric.readthedocs.io/en/latest/modules/explain.html)\n",
    "\n",
    "**Recent Papers (2024-2025):**\n",
    "- [Drug discovery and mechanism prediction with explainable GNNs](https://www.nature.com/articles/s41598-024-83090-3) - Scientific Reports, 2024\n",
    "- [Explainable AI in Drug Discovery](https://wires.onlinelibrary.wiley.com/doi/10.1002/wcms.70049) - WIREs Computational Molecular Science, 2025\n",
    "\n",
    "**Alternative Approaches:**\n",
    "- **GNNExplainer**: Native PyG implementation optimized for graphs\n",
    "- **PGExplainer**: Learns parameterized explainer model\n",
    "- **SubgraphX**: Monte Carlo Tree Search for subgraph explanations\n",
    "\n",
    "---\n",
    "\n",
    "**You've now learned how to:**\n",
    "- \u2713 Install and configure Captum for GNN explainability\n",
    "- \u2713 Compute Saliency and Integrated Gradients attributions\n",
    "- \u2713 Visualize attributions on molecular structures\n",
    "- \u2713 Interpret results in chemical context\n",
    "- \u2713 Apply best practices for trustworthy explanations\n",
    "\n",
    "**Next steps:** Try explaining your own molecular datasets and validate findings with domain experts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3y59JCbMTkC"
   },
   "source": [
    "## Further Reading\n",
    "\n",
    "### Papers\n",
    "1. **Gilmer et al. (2017)**: Neural Message Passing for Quantum Chemistry\n",
    "2. **Yang et al. (2019)**: Analyzing Learned Molecular Representations (Chemprop)\n",
    "3. **Xu et al. (2019)**: How Powerful are Graph Neural Networks? (GIN)\n",
    "4. **Veli\u010dkovi\u0107 et al. (2018)**: Graph Attention Networks (GAT)\n",
    "5. **Sch\u00fctt et al. (2017)**: SchNet - 3D molecular modeling\n",
    "\n",
    "### Courses and Tutorials\n",
    "- **Stanford CS224W**: Machine Learning with Graphs\n",
    "- **PyG Tutorials**: https://pytorch-geometric.readthedocs.io/\n",
    "- **DeepChem**: https://deepchem.io/\n",
    "- **AI4Chem Course**: https://github.com/schwallergroup/ai4chem_course\n",
    "\n",
    "### Software Libraries\n",
    "- **PyTorch Geometric**: https://pytorch-geometric.readthedocs.io/\n",
    "- **Chemprop**: https://github.com/chemprop/chemprop\n",
    "- **DeepChem**: https://deepchem.io/\n",
    "- **RDKit**: https://www.rdkit.org/\n",
    "- **OGB**: https://ogb.stanford.edu/\n",
    "\n",
    "### Research Groups\n",
    "- **Gomes Group (CMU)**: Molecular machine learning and retrosynthesis\n",
    "- **Coley Group (MIT)**: Chemprop, reaction prediction\n",
    "- **Schwaller Group (EPFL)**: Transformer models for chemistry\n",
    "- **Isayev Group (CMU)**: ANI neural network potentials\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed the GNN Molecular Property Prediction tutorial!\n",
    "\n",
    "### What You've Learned:\n",
    "\u2713 Why graphs are natural for molecules\n",
    "\u2713 How to convert SMILES to graph representations\n",
    "\u2713 Message-passing neural network theory\n",
    "\u2713 Implementing MPNNs with PyTorch Geometric\n",
    "\u2713 Training and evaluating GNN models\n",
    "\u2713 Visualizing learned representations\n",
    "\u2713 Using production tools like Chemprop\n",
    "\u2713 Debugging common issues\n",
    "\u2713 Paths for further exploration\n",
    "\n",
    "### Next Steps:\n",
    "1. Try different molecular properties (QM9, MoleculeNet)\n",
    "2. Experiment with different GNN architectures\n",
    "3. Apply to your own molecular datasets\n",
    "4. Explore 3D equivariant models\n",
    "5. Implement uncertainty quantification\n",
    "6. Deploy models in production\n",
    "\n",
    "**Happy molecule learning!**\n",
    "\n",
    "---\n",
    "\n",
    "*Tutorial developed by the Gomes Research Group at CMU*  \n",
    "*Questions? Open an issue on GitHub or contact the course staff*"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "jl880QjlMXGC"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}