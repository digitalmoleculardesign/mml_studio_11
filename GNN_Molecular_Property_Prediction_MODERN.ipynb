{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "<a id='introduction'></a>\n# 1. Introduction: Why Graphs for Molecules?\n\n**‚è±Ô∏è Expected time:** 10 minutes\n\n## The Challenge of Molecular Representation\n\nMolecules are complex 3D structures with atoms connected by chemical bonds. How should we represent them for machine learning?\n\n### Traditional Approaches\n\n1. **SMILES Strings** ‚Üí Treat as text sequences\n2. **Molecular Descriptors** ‚Üí Hand-crafted features (MW, logP, TPSA, etc.)\n3. **Fingerprints** ‚Üí Binary vectors (Morgan, MACCS, etc.)\n\n### The Graph Representation Advantage\n\nA **graph** is a natural way to represent molecules:\n- **Nodes (Vertices)** = Atoms\n- **Edges (Links)** = Chemical bonds\n- **Node features** = Atom properties (element, charge, hybridization)\n- **Edge features** = Bond properties (type, stereochemistry)\n\n<div align=\"center\">\n<img src=\"https://github.com/beangoben/chemistry_ml_colab/blob/master/images/Chloroquine-2D-molecular-graph.png?raw=1\" width=\"500\"/>\n</div>\n\n### Why This Matters\n\n> **Inductive Bias**: Graphs encode the assumption that molecular properties are determined by:\n> 1. The types of atoms present\n> 2. How atoms are connected\n> 3. Local chemical environments (neighborhoods)\n\nThis is exactly how chemists think about molecules!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Networks for Molecular Property Prediction\n",
    "\n",
    "**Molecular Machine Learning Course - GNN Studio**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_REPO/GNN_Molecular_Property_Prediction.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. **Understand** why graph representations are natural for molecular data\n",
    "2. **Construct** molecular graphs from SMILES strings with atom and bond features\n",
    "3. **Implement** message-passing neural networks (MPNNs) using PyTorch Geometric\n",
    "4. **Train** and evaluate GNN models for molecular property prediction\n",
    "5. **Use** production-ready tools like Chemprop for real-world applications\n",
    "6. **Debug** common issues in GNN training pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "0. [Setup & Environment](#setup)\n",
    "1. [Introduction: Why Graphs for Molecules?](#introduction)\n",
    "2. [Molecular Graph Construction](#graph-construction)\n",
    "3. [The ESOL Solubility Dataset](#dataset)\n",
    "4. [Message-Passing Neural Networks - Theory](#mpnn-theory)\n",
    "5. [Implementing MPNN with PyTorch Geometric](#mpnn-implementation)\n",
    "6. [Training and Evaluation](#training)\n",
    "7. [Understanding Learned Representations](#representations)\n",
    "8. [Chemprop - Production-Ready GNNs](#chemprop)\n",
    "9. [Common Pitfalls and Debugging](#debugging)\n",
    "10. [Extensions and Next Steps](#extensions)\n",
    "\n",
    "---\n",
    "\n",
    "## Authors and Acknowledgments\n",
    "\n",
    "**Developed by:** Gomes Research Group, CMU  \n",
    "**Based on:** AI4Chem course materials (Schwallergroup)  \n",
    "**Last Updated:** December 2024  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "# 0. Setup & Environment\n",
    "\n",
    "**‚è±Ô∏è Expected runtime:** 2-3 minutes\n",
    "\n",
    "This section installs all required packages and sets up the environment for reproducible experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úì Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"‚úì Running in local environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Package Installation\n",
    "\n",
    "We'll install the latest stable versions of:\n",
    "- **PyTorch 2.x** - Deep learning framework\n",
    "- **PyTorch Geometric 2.6+** - Graph neural network library  \n",
    "- **PyTorch Lightning 2.x** - High-level training framework\n",
    "- **RDKit** - Cheminformatics toolkit\n",
    "- **Chemprop v2.2.1** - Production-ready molecular ML\n",
    "- **Weights & Biases** - Experiment tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%bash\n# Install uv first for faster package management\npip install -q uv\n\n# Install PyTorch with CUDA 11.8 support (for Colab T4 GPUs)\nuv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# Install PyTorch Geometric and dependencies\nuv pip install torch-geometric -f https://data.pyg.org/whl/torch-2.1.0+cu118.html\n\n# Install other ML packages\nuv pip install pytorch-lightning==2.1.0 wandb rdkit ogb deepchem scikit-learn pandas matplotlib seaborn plotly tqdm\n\n# Install Chemprop v2\nuv pip install chemprop==2.2.1\n\necho \"‚úì Package installation complete!\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Download Data\n",
    "\n",
    "We'll use the **ESOL (Estimated SOLubility)** dataset, which contains aqueous solubility measurements for 1,128 small organic molecules.\n",
    "\n",
    "**Reference:** Delaney, J. S. (2004). *ESOL: Estimating Aqueous Solubility Directly from Molecular Structure.* J. Chem. Inf. Comput. Sci., 44(3), 1000-1005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create data directory\n",
    "mkdir -p data\n",
    "\n",
    "# Download ESOL dataset\n",
    "wget -q https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/delaney-processed.csv -O data/esol.csv\n",
    "\n",
    "echo \"‚úì Data download complete!\"\n",
    "echo \"Files in data directory:\"\n",
    "ls -lh data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# PyTorch Geometric\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "from torch_geometric.nn import NNConv, global_add_pool, global_mean_pool, global_max_pool\n",
    "from torch_geometric.nn import MLP, GCNConv, GINConv, GATConv\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "\n",
    "# Cheminformatics\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Draw, Descriptors\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "\n",
    "# OGB utilities\n",
    "from ogb.utils import smiles2graph\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Experiment tracking\n",
    "import wandb\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure RDKit visualization\n",
    "IPythonConsole.ipython_useSVG = True\n",
    "IPythonConsole.molSize = 300, 300\n",
    "\n",
    "print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úì PyTorch Geometric version: {torch_geometric.__version__}\")\n",
    "print(f\"‚úì PyTorch Lightning version: {pl.__version__}\")\n",
    "print(f\"‚úì RDKit version: {rdkit.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 GPU Detection and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "print(\"=\" * 60)\n",
    "print(\"HARDWARE DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  - CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"  - Device Capability: {torch.cuda.get_device_capability(0)}\")\n",
    "    print(f\"  - Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU detected - using CPU\")\n",
    "    if IN_COLAB:\n",
    "        print(\"\\nüí° To enable GPU in Colab:\")\n",
    "        print(\"   Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"\\n‚úì Using device: {device}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.5 Reproducibility Setup\n",
    "\n",
    "Setting random seeds ensures that:\n",
    "1. Model weights are initialized identically\n",
    "2. Data shuffling is deterministic\n",
    "3. Results are reproducible across runs\n",
    "\n",
    "This is **critical** for debugging and comparing different architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Set random seeds for reproducibility across all libraries.\n",
    "    \n",
    "    Args:\n",
    "        seed: Random seed value (default: 42)\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # Deterministic operations (may reduce performance slightly)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # PyTorch Lightning\n",
    "    pl.seed_everything(seed, workers=True)\n",
    "    \n",
    "    # Environment variable for deterministic algorithms\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    print(f\"‚úì Random seed set to {seed} for reproducibility\")\n",
    "\n",
    "# Set seed for this session\n",
    "RANDOM_SEED = 42\n",
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.6 Visualization Setup (Gomes Group Style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gomes Group color palette\n",
    "GOMES_COLORS = {\n",
    "    'teal': '#00D9FF',\n",
    "    'coral': '#FF6B6B',\n",
    "    'navy': '#0A1628',\n",
    "    'light_teal': '#7FEFFF',\n",
    "    'dark_teal': '#00A8CC',\n",
    "    'light_coral': '#FF9999',\n",
    "    'dark_coral': '#CC5555',\n",
    "    'gray': '#95A5A6',\n",
    "    'white': '#FFFFFF'\n",
    "}\n",
    "\n",
    "# Configure matplotlib style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['xtick.labelsize'] = 11\n",
    "plt.rcParams['ytick.labelsize'] = 11\n",
    "plt.rcParams['legend.fontsize'] = 12\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "# Seaborn configuration\n",
    "sns.set_palette([GOMES_COLORS['teal'], GOMES_COLORS['coral'], \n",
    "                 GOMES_COLORS['gray'], GOMES_COLORS['navy']])\n",
    "\n",
    "print(\"‚úì Visualization style configured (Gomes Group aesthetic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.7 Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_section_header(title: str, emoji: str = \"üìö\"):\n",
    "    \"\"\"\n",
    "    Print a formatted section header.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"{emoji}  {title}\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "def print_metrics(metrics: dict, title: str = \"Metrics\"):\n",
    "    \"\"\"\n",
    "    Pretty print evaluation metrics.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{title}:\")\n",
    "    print(\"-\" * 40)\n",
    "    for key, value in metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key:.<30} {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {key:.<30} {value}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "def estimate_runtime(gpu: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Return estimated runtime string based on hardware.\n",
    "    \"\"\"\n",
    "    if gpu:\n",
    "        return \"‚è±Ô∏è Expected runtime: ~2-3 minutes (GPU)\"\n",
    "    else:\n",
    "        return \"‚è±Ô∏è Expected runtime: ~8-10 minutes (CPU)\"\n",
    "\n",
    "print(\"‚úì Utility functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**‚úÖ Setup Complete!**\n",
    "\n",
    "You're now ready to start learning about Graph Neural Networks for molecular property prediction.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "# 1. Introduction: Why Graphs for Molecules?\n\n",
    "**‚è±Ô∏è Expected time:** 10 minutes\n\n",
    "## The Challenge of Molecular Representation\n\n",
    "Molecules are complex 3D structures with atoms connected by chemical bonds. How should we represent them for machine learning?\n\n",
    "### Traditional Approaches\n\n",
    "1. **SMILES Strings** ‚Üí Treat as text sequences\n",
    "2. **Molecular Descriptors** ‚Üí Hand-crafted features (MW, logP, TPSA, etc.)\n",
    "3. **Fingerprints** ‚Üí Binary vectors (Morgan, MACCS, etc.)\n\n",
    "### The Graph Representation Advantage\n\n",
    "A **graph** is a natural way to represent molecules:\n",
    "- **Nodes (Vertices)** = Atoms\n",
    "- **Edges (Links)** = Chemical bonds\n",
    "- **Node features** = Atom properties (element, charge, hybridization)\n",
    "- **Edge features** = Bond properties (type, stereochemistry)\n\n",
    "<div align=\"center\">\n",
    "<img src=\"https://github.com/beangoben/chemistry_ml_colab/blob/master/images/Chloroquine-2D-molecular-graph.png?raw=1\" width=\"500\"/>\n",
    "</div>\n\n",
    "### Why This Matters\n\n",
    "> **Inductive Bias**: Graphs encode the assumption that molecular properties are determined by:\n",
    "> 1. The types of atoms present\n",
    "> 2. How atoms are connected\n",
    "> 3. Local chemical environments (neighborhoods)\n\n",
    "This is exactly how chemists think about molecules!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Visualizing Molecular Graphs\", \"üî¨\")\n\n",
    "# Example molecules of increasing complexity\n",
    "molecules = {\n",
    "    'Methane': 'C',\n",
    "    'Ethanol': 'CCO',\n",
    "    'Benzene': 'c1ccccc1',\n",
    "    'Aspirin': 'CC(=O)Oc1ccccc1C(=O)O'\n",
    "}\n\n",
    "# Visualize molecules\n",
    "mols = [Chem.MolFromSmiles(smi) for smi in molecules.values()]\n",
    "img = Draw.MolsToGridImage(\n",
    "    mols,\n",
    "    molsPerRow=2,\n",
    "    subImgSize=(300, 300),\n",
    "    legends=list(molecules.keys())\n",
    ")\n",
    "display(img)\n\n",
    "print(\"\\n‚úì Each molecule is represented as a graph:\")\n",
    "print(\"  - Nodes: atoms (C, O, H, etc.)\")\n",
    "print(\"  - Edges: bonds (single, double, aromatic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Theory Basics\n\n",
    "A graph $G = (V, E)$ consists of:\n\n",
    "- **Vertices (Nodes)**: $V = \\{v_1, v_2, ..., v_n\\}$\n",
    "- **Edges**: $E \\subseteq \\{(i,j) | i,j \\in V, i \\neq j\\}$\n\n",
    "For molecules, we typically use **undirected graphs** (bonds work both ways).\n\n",
    "### Graph Properties\n\n",
    "| Property | Molecular Interpretation |\n",
    "|----------|------------------------|\n",
    "| **Node degree** | Number of bonds an atom forms |\n",
    "| **Path length** | Shortest distance between atoms |\n",
    "| **Cycles** | Rings in molecules |\n",
    "| **Connectivity** | Whether molecule is fragmented |\n\n",
    "### Adjacency Matrix\n\n",
    "An $n \\times n$ matrix $A$ where $A_{ij} = 1$ if atoms $i$ and $j$ are bonded.\n\n",
    "**Note**: For GNNs, we typically use the more efficient **edge index** format instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Example: Caffeine Molecular Graph\", \"‚òï\")\n\n",
    "caffeine_smiles = 'CN1C=NC2=C1C(=O)N(C(=O)N2C)C'\n",
    "mol = Chem.MolFromSmiles(caffeine_smiles)\n\n",
    "print(f\"Molecule: Caffeine\")\n",
    "print(f\"SMILES: {caffeine_smiles}\")\n",
    "print(f\"Molecular Formula: {Chem.rdMolDescriptors.CalcMolFormula(mol)}\")\n",
    "print(f\"\\nGraph Properties:\")\n",
    "print(f\"  Number of atoms (nodes): {mol.GetNumAtoms()}\")\n",
    "print(f\"  Number of bonds (edges): {mol.GetNumBonds()}\")\n",
    "print(f\"  Number of rings: {Chem.rdMolDescriptors.CalcNumRings(mol)}\")\n\n",
    "# Display molecule\n",
    "display(mol)\n\n",
    "# Calculate and display adjacency matrix\n",
    "from rdkit.Chem import GetAdjacencyMatrix\n",
    "adj_matrix = GetAdjacencyMatrix(mol)\n\n",
    "print(f\"\\nAdjacency Matrix Shape: {adj_matrix.shape}\")\n",
    "print(\"First 5x5 block:\")\n",
    "print(adj_matrix[:5, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='graph-construction'></a>\n",
    "# 2. Molecular Graph Construction\n\n",
    "**‚è±Ô∏è Expected time:** 15 minutes\n\n",
    "Now that we understand why graphs are natural for molecules, let's learn how to convert SMILES strings into graph data structures that neural networks can process.\n\n",
    "## From SMILES to Graph\n\n",
    "The conversion process involves three key steps:\n\n",
    "1. **Extract atom features** ‚Üí Convert each atom into a feature vector\n",
    "2. **Extract bond features** ‚Üí Convert each bond into a feature vector\n",
    "3. **Build edge index** ‚Üí Create connectivity matrix in COO format\n\n",
    "We'll use the **OGB (Open Graph Benchmark)** featurization standard, which is widely adopted in molecular ML research.\n\n",
    "### Atom Features\n\n",
    "For each atom, we extract 9 categorical features:\n\n",
    "| Feature | Description | Example Values |\n",
    "|---------|-------------|----------------|\n",
    "| Atomic number | Element identity | 1 (H), 6 (C), 7 (N), 8 (O) |\n",
    "| Degree | Number of bonded neighbors | 0, 1, 2, 3, 4 |\n",
    "| Formal charge | Integer charge | -1, 0, +1 |\n",
    "| Chirality | Stereochemistry | R, S, unspecified |\n",
    "| Num H atoms | Total hydrogen count | 0, 1, 2, 3 |\n",
    "| Hybridization | Orbital type | sp, sp2, sp3 |\n",
    "| Is aromatic | Aromaticity flag | True, False |\n",
    "| Is in ring | Ring membership | True, False |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Atom Featurization (OGB Standard)\", \"‚öõÔ∏è\")\n\n",
    "# Define allowable atom features (from OGB)\n",
    "ATOM_FEATURES = {\n",
    "    'atomic_num': list(range(1, 119)) + ['misc'],\n",
    "    'degree': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 'misc'],\n",
    "    'formal_charge': [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 'misc'],\n",
    "    'chiral_tag': ['CHI_UNSPECIFIED', 'CHI_TETRAHEDRAL_CW', 'CHI_TETRAHEDRAL_CCW', 'CHI_OTHER'],\n",
    "    'num_Hs': [0, 1, 2, 3, 4, 5, 6, 7, 8, 'misc'],\n",
    "    'hybridization': ['SP', 'SP2', 'SP3', 'SP3D', 'SP3D2', 'misc'],\n",
    "    'is_aromatic': [False, True],\n",
    "    'is_in_ring': [False, True]\n",
    "}\n\n",
    "def safe_index(lst, element):\n",
    "    \"\"\"\n",
    "    Return index of element in list. If not present, return last index (misc).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return lst.index(element)\n",
    "    except ValueError:\n",
    "        return len(lst) - 1\n\n",
    "def atom_to_feature_vector(atom):\n",
    "    \"\"\"\n",
    "    Convert RDKit atom object to feature vector (list of indices).\n",
    "    \n",
    "    Args:\n",
    "        atom: RDKit Atom object\n",
    "    \n",
    "    Returns:\n",
    "        list: Feature vector with 8 integer indices\n",
    "    \"\"\"\n",
    "    features = [\n",
    "        safe_index(ATOM_FEATURES['atomic_num'], atom.GetAtomicNum()),\n",
    "        safe_index(ATOM_FEATURES['degree'], atom.GetTotalDegree()),\n",
    "        safe_index(ATOM_FEATURES['formal_charge'], atom.GetFormalCharge()),\n",
    "        ATOM_FEATURES['chiral_tag'].index(str(atom.GetChiralTag())),\n",
    "        safe_index(ATOM_FEATURES['num_Hs'], atom.GetTotalNumHs()),\n",
    "        safe_index(ATOM_FEATURES['hybridization'], str(atom.GetHybridization())),\n",
    "        ATOM_FEATURES['is_aromatic'].index(atom.GetIsAromatic()),\n",
    "        ATOM_FEATURES['is_in_ring'].index(atom.IsInRing()),\n",
    "    ]\n",
    "    return features\n\n",
    "# Test on ethanol\n",
    "mol = Chem.MolFromSmiles('CCO')\n",
    "print(\"Ethanol (CCO) Atom Features:\")\n",
    "print(\"-\" * 50)\n",
    "for i, atom in enumerate(mol.GetAtoms()):\n",
    "    features = atom_to_feature_vector(atom)\n",
    "    print(f\"Atom {i} ({atom.GetSymbol()}):\")\n",
    "    print(f\"  Atomic number: {atom.GetAtomicNum()}\")\n",
    "    print(f\"  Degree: {atom.GetTotalDegree()}\")\n",
    "    print(f\"  Hybridization: {atom.GetHybridization()}\")\n",
    "    print(f\"  Feature vector: {features}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bond Features\n\n",
    "For each bond, we extract 3 categorical features:\n\n",
    "| Feature | Description | Example Values |\n",
    "|---------|-------------|----------------|\n",
    "| Bond type | Chemical bond order | SINGLE, DOUBLE, TRIPLE, AROMATIC |\n",
    "| Stereo | Stereochemistry | E/Z, cis/trans, none |\n",
    "| Conjugated | Part of conjugated system | True, False |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Bond Featurization\", \"üîó\")\n\n",
    "BOND_FEATURES = {\n",
    "    'bond_type': ['SINGLE', 'DOUBLE', 'TRIPLE', 'AROMATIC', 'misc'],\n",
    "    'stereo': ['STEREONONE', 'STEREOZ', 'STEREOE', 'STEREOCIS', 'STEREOTRANS', 'STEREOANY'],\n",
    "    'is_conjugated': [False, True],\n",
    "}\n\n",
    "def bond_to_feature_vector(bond):\n",
    "    \"\"\"\n",
    "    Convert RDKit bond object to feature vector.\n",
    "    \n",
    "    Args:\n",
    "        bond: RDKit Bond object\n",
    "    \n",
    "    Returns:\n",
    "        list: Feature vector with 3 integer indices\n",
    "    \"\"\"\n",
    "    features = [\n",
    "        safe_index(BOND_FEATURES['bond_type'], str(bond.GetBondType())),\n",
    "        BOND_FEATURES['stereo'].index(str(bond.GetStereo())),\n",
    "        BOND_FEATURES['is_conjugated'].index(bond.GetIsConjugated()),\n",
    "    ]\n",
    "    return features\n\n",
    "# Test on ethanol bonds\n",
    "mol = Chem.MolFromSmiles('CCO')\n",
    "print(\"Ethanol Bond Features:\")\n",
    "print(\"-\" * 50)\n",
    "for i, bond in enumerate(mol.GetBonds()):\n",
    "    features = bond_to_feature_vector(bond)\n",
    "    begin = bond.GetBeginAtom().GetSymbol()\n",
    "    end = bond.GetEndAtom().GetSymbol()\n",
    "    bond_type = bond.GetBondType()\n",
    "    print(f\"Bond {i} ({begin}-{end}):\")\n",
    "    print(f\"  Type: {bond_type}\")\n",
    "    print(f\"  Feature vector: {features}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Index Format (COO)\n\n",
    "PyTorch Geometric uses **Coordinate (COO) format** for graph connectivity:\n\n",
    "```\n",
    "edge_index = [[source_nodes],\n",
    "              [target_nodes]]\n",
    "```\n\n",
    "Shape: `[2, num_edges]`\n\n",
    "**Important**: For undirected graphs (molecules), we add edges in **both directions**:\n",
    "- If atom 0 bonds to atom 1, we add edges (0‚Üí1) AND (1‚Üí0)\n\n",
    "<div align=\"center\">\n",
    "<img src=\"https://github.com/beangoben/chemistry_ml_colab/blob/master/images/mol_tensors.png?raw=1\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Complete SMILES ‚Üí PyG Graph Conversion\", \"üîÑ\")\n\n",
    "def molecule_to_graph_data(smiles):\n",
    "    \"\"\"\n",
    "    Convert SMILES to PyTorch Geometric Data object.\n",
    "    \n",
    "    Args:\n",
    "        smiles: SMILES string\n",
    "    \n",
    "    Returns:\n",
    "        Data: PyG Data object with x, edge_index, edge_attr\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    \n",
    "    # Node features\n",
    "    atom_features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        atom_features.append(atom_to_feature_vector(atom))\n",
    "    x = torch.tensor(atom_features, dtype=torch.long)\n",
    "    \n",
    "    # Edge features and edge index\n",
    "    edge_indices = []\n",
    "    edge_features = []\n",
    "    \n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        bond_feat = bond_to_feature_vector(bond)\n",
    "        \n",
    "        # Add both directions (undirected graph)\n",
    "        edge_indices.append([i, j])\n",
    "        edge_features.append(bond_feat)\n",
    "        edge_indices.append([j, i])\n",
    "        edge_features.append(bond_feat)\n",
    "    \n",
    "    # Handle single-atom molecules (no bonds)\n",
    "    if len(edge_indices) == 0:\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        edge_attr = torch.empty((0, 3), dtype=torch.long)\n",
    "    else:\n",
    "        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_features, dtype=torch.long)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    return data\n\n",
    "# Test on multiple molecules\n",
    "examples = [\n",
    "    ('Methane', 'C'),\n",
    "    ('Ethanol', 'CCO'),\n",
    "    ('Benzene', 'c1ccccc1'),\n",
    "    ('Aspirin', 'CC(=O)Oc1ccccc1C(=O)O')\n",
    "]\n\n",
    "print(\"Molecular Graph Statistics:\")\n",
    "print(\"=\" * 70)\n",
    "for name, smiles in examples:\n",
    "    graph = molecule_to_graph_data(smiles)\n",
    "    print(f\"{name:12s} | Nodes: {graph.num_nodes:3d} | Edges: {graph.num_edges:3d}\")\n",
    "print(\"=\" * 70)\n\n",
    "print(\"\\n‚úì Featurization functions ready!\")\n",
    "print(\"  - atom_to_feature_vector()\")\n",
    "print(\"  - bond_to_feature_vector()\")\n",
    "print(\"  - molecule_to_graph_data()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataset'></a>\n",
    "# 3. The ESOL Solubility Dataset\n\n",
    "**‚è±Ô∏è Expected time:** 10 minutes\n\n",
    "Now let's load our molecular dataset and prepare it for training.\n\n",
    "## About ESOL\n\n",
    "The **ESOL (Estimated SOLubility)** dataset contains:\n",
    "- **1,128 molecules** with measured aqueous solubility\n",
    "- **Target**: log(solubility) in mols per litre\n",
    "- **SMILES** representations for each molecule\n\n",
    "**Reference:** Delaney, J. S. (2004). ESOL: Estimating Aqueous Solubility Directly from Molecular Structure. *J. Chem. Inf. Comput. Sci.*, 44(3), 1000-1005.\n\n",
    "### Why Solubility Matters\n\n",
    "Aqueous solubility is a critical property in drug discovery:\n",
    "- Poor solubility ‚Üí Poor bioavailability\n",
    "- Predicting solubility early saves time and money\n",
    "- Traditional methods (wet lab) are slow and expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Loading ESOL Dataset\", \"üíß\")\n\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/esol.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "for col in df.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Define column names\n",
    "smiles_column = 'smiles'\n",
    "target_column = 'measured log solubility in mols per litre'\n",
    "\n",
    "# Display first few rows\n",
    "print(f\"\\nFirst 5 molecules:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Exploratory Data Analysis\", \"üìä\")\n\n",
    "# Extract target values\n",
    "y = df[target_column].values\n",
    "\n",
    "# Basic statistics\n",
    "print(\"Target Statistics:\")\n",
    "print(f\"  Mean: {y.mean():.3f}\")\n",
    "print(f\"  Std: {y.std():.3f}\")\n",
    "print(f\"  Min: {y.min():.3f}\")\n",
    "print(f\"  Max: {y.max():.3f}\")\n",
    "print(f\"  Median: {np.median(y):.3f}\")\n",
    "\n",
    "# Visualize distribution with Gomes colors\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(y, bins=30, color=GOMES_COLORS['teal'], alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(y.mean(), color=GOMES_COLORS['coral'], linestyle='--', linewidth=2, label=f'Mean: {y.mean():.2f}')\n",
    "axes[0].set_xlabel('Log Solubility (mol/L)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Distribution of Log Solubility', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(y, vert=True, patch_artist=True,\n",
    "                boxprops=dict(facecolor=GOMES_COLORS['teal'], alpha=0.7),\n",
    "                medianprops=dict(color=GOMES_COLORS['coral'], linewidth=2))\n",
    "axes[1].set_ylabel('Log Solubility (mol/L)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Box Plot', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Dataset loaded and explored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Converting Molecules to Graphs\", \"üîÑ\")\n\n",
    "# Convert all SMILES to PyG graphs\n",
    "smiles_list = df[smiles_column].tolist()\n",
    "targets = df[target_column].values\n",
    "\n",
    "graph_list = []\n",
    "failed_indices = []\n",
    "\n",
    "print(\"Converting SMILES to graphs...\")\n",
    "for i, smi in enumerate(tqdm(smiles_list)):\n",
    "    graph = molecule_to_graph_data(smi)\n",
    "    if graph is not None:\n",
    "        # Add target value to graph\n",
    "        graph.y = torch.tensor([targets[i]], dtype=torch.float)\n",
    "        graph_list.append(graph)\n",
    "    else:\n",
    "        failed_indices.append(i)\n",
    "        print(f\"  Warning: Could not parse SMILES at index {i}: {smi}\")\n",
    "\n",
    "print(f\"\\n‚úì Successfully converted {len(graph_list)} molecules to graphs\")\n",
    "print(f\"  Failed: {len(failed_indices)} molecules\")\n",
    "\n",
    "# Show example graph\n",
    "print(f\"\\nExample graph (first molecule):\")\n",
    "print(graph_list[0])\n",
    "print(f\"  Node features shape: {graph_list[0].x.shape}\")\n",
    "print(f\"  Edge index shape: {graph_list[0].edge_index.shape}\")\n",
    "print(f\"  Edge features shape: {graph_list[0].edge_attr.shape}\")\n",
    "print(f\"  Target value: {graph_list[0].y.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Train/Val/Test Split (70/10/20)\", \"‚úÇÔ∏è\")\n\n",
    "# Create indices\n",
    "n_samples = len(graph_list)\n",
    "indices = list(range(n_samples))\n",
    "\n",
    "# Split: train (70%), temp (30%)\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    indices, test_size=0.3, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Split temp: val (10%), test (20%) -> 10/30 = 1/3, 20/30 = 2/3\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx, test_size=2/3, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"Split sizes:\")\n",
    "print(f\"  Train: {len(train_idx)} samples ({len(train_idx)/n_samples*100:.1f}%)\")\n",
    "print(f\"  Val:   {len(val_idx)} samples ({len(val_idx)/n_samples*100:.1f}%)\")\n",
    "print(f\"  Test:  {len(test_idx)} samples ({len(test_idx)/n_samples*100:.1f}%)\")\n",
    "\n",
    "# Create dataset lists\n",
    "train_graphs = [graph_list[i] for i in train_idx]\n",
    "val_graphs = [graph_list[i] for i in val_idx]\n",
    "test_graphs = [graph_list[i] for i in test_idx]\n",
    "\n",
    "# Normalize targets (critical for training!)\n",
    "print(f\"\\nNormalizing targets...\")\n",
    "train_targets = np.array([g.y.item() for g in train_graphs])\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_targets.reshape(-1, 1))\n",
    "\n",
    "# Apply normalization to all graphs\n",
    "for g in train_graphs:\n",
    "    g.y = torch.tensor(scaler.transform([[g.y.item()]])[0], dtype=torch.float)\n",
    "for g in val_graphs:\n",
    "    g.y = torch.tensor(scaler.transform([[g.y.item()]])[0], dtype=torch.float)\n",
    "for g in test_graphs:\n",
    "    g.y = torch.tensor(scaler.transform([[g.y.item()]])[0], dtype=torch.float)\n",
    "\n",
    "print(f\"  Mean: {scaler.mean_[0]:.3f}\")\n",
    "print(f\"  Std: {scaler.scale_[0]:.3f}\")\n",
    "\n",
    "print(f\"\\n‚úì Data preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mpnn-theory'></a>\n",
    "# 4. Message-Passing Neural Networks - Theory\n\n",
    "**‚è±Ô∏è Expected time:** 15 minutes\n\n",
    "## The Key Idea\n\n",
    "**Message-Passing Neural Networks (MPNNs)** learn molecular representations by iteratively propagating information between connected atoms.\n\n",
    "### How Atoms \"Talk\" to Each Other\n\n",
    "Think of each atom in a molecule as a node in a communication network:\n",
    "1. Each atom has a **hidden state** (feature vector)\n",
    "2. Atoms **send messages** to their neighbors through bonds\n",
    "3. Atoms **aggregate** messages from all neighbors\n",
    "4. Atoms **update** their hidden states based on received messages\n",
    "5. This process repeats for T iterations\n\n",
    "### Mathematical Formulation\n\n",
    "Let's formalize the message passing process:\n\n",
    "**1. Initialization**\n\n",
    "$$h_i^0 = I(x_i), \\quad \\forall i \\in V$$\n\n",
    "where $h_i^0$ is the initial hidden state of atom $i$, and $I$ is an embedding function.\n\n",
    "**2. Message Generation** (at iteration $t+1$)\n\n",
    "$$m_{j \\rightarrow i}^{t+1} = M(h_i^t, h_j^t, e_{ij})$$\n\n",
    "where $m_{j \\rightarrow i}^{t+1}$ is the message from atom $j$ to atom $i$, and $M$ is a learnable message function that considers:\n",
    "- $h_i^t$: current state of receiving atom\n",
    "- $h_j^t$: current state of sending atom  \n",
    "- $e_{ij}$: bond features between atoms $i$ and $j$\n\n",
    "**3. Message Aggregation**\n\n",
    "$$m_i^{t+1} = \\sum_{j \\in N(i)} m_{j \\rightarrow i}^{t+1}$$\n\n",
    "where $N(i)$ is the set of neighboring atoms of atom $i$.\n\n",
    "**4. State Update**\n\n",
    "$$h_i^{t+1} = U(h_i^t, m_i^{t+1})$$\n\n",
    "where $U$ is a learnable update function (often a GRU or LSTM).\n\n",
    "**5. Readout** (Graph-level prediction)\n\n",
    "$$\\hat{y} = R(\\{h_i^T | i \\in V\\})$$\n\n",
    "where $R$ is a readout function that aggregates all atom states into a single molecular representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Intuition: Message Passing\n\n",
    "Let's see how information flows through a molecular graph:\n\n",
    "<div align=\"center\">\n",
    "<img src=\"https://github.com/beangoben/chemistry_ml_colab/blob/master/images/gcn_one.png?raw=1\" width=\"600\"/>\n",
    "</div>\n\n",
    "<div align=\"center\">\n",
    "<img src=\"https://github.com/beangoben/chemistry_ml_colab/blob/master/images/gcn_two.png?raw=1\" width=\"600\"/>\n",
    "</div>\n\n",
    "### Multiple Iterations = Larger Receptive Field\n\n",
    "With each message passing iteration, atoms can \"see\" further in the molecular graph:\n\n",
    "- **Iteration 1**: Atoms know about immediate neighbors\n",
    "- **Iteration 2**: Atoms know about neighbors-of-neighbors  \n",
    "- **Iteration 3**: Atoms know about atoms 3 bonds away\n",
    "- **Iteration T**: Atoms know about atoms T bonds away\n\n",
    "<div align=\"center\">\n",
    "<img src=\"https://github.com/beangoben/chemistry_ml_colab/blob/master/images/gcn_layers.png?raw=1\" width=\"600\"/>\n",
    "</div>\n\n",
    "### Connected Layers\n\n",
    "<div align=\"center\">\n",
    "<img src=\"https://github.com/beangoben/chemistry_ml_colab/blob/master/images/gcn_connected.png?raw=1\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Properties of MPNNs\n\n",
    "1. **Permutation Invariance**: Output doesn't depend on atom ordering in the input\n",
    "2. **Variable Size**: Can handle molecules with different numbers of atoms\n",
    "3. **Local-to-Global**: Build molecular representations from local atomic environments\n",
    "4. **Learnable**: All functions ($I$, $M$, $U$, $R$) are neural networks trained end-to-end\n\n",
    "### Why This Works for Chemistry\n\n",
    "- **Chemical intuition**: Molecular properties are determined by local atomic environments\n",
    "- **Scalability**: Efficient on large molecules (linear in number of atoms/bonds)\n",
    "- **Flexibility**: Same architecture works for different properties\n",
    "- **Interpretability**: Can visualize learned representations and attention weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mpnn-implementation'></a>\n",
    "# 5. Implementing MPNN with PyTorch Geometric\n\n",
    "**‚è±Ô∏è Expected time:** 20 minutes\n\n",
    "Now let's implement a complete MPNN using PyTorch Geometric and PyTorch Lightning!\n\n",
    "## Architecture Overview\n\n",
    "Our MPNN will have the following components:\n\n",
    "1. **Atom Encoder**: Embed atom features into continuous space\n",
    "2. **Bond Encoder**: Embed bond features into continuous space\n",
    "3. **Message Passing Layers**: NNConv (Neural Network Convolution) with GRU updates\n",
    "4. **Global Pooling**: Aggregate atom representations to molecular representation\n",
    "5. **Prediction Head**: MLP to predict target property\n\n",
    "### Why NNConv?\n\n",
    "**NNConv** uses edge features (bond information) to modulate message passing:\n",
    "- Standard GCN: Ignores bond types\n",
    "- NNConv: Uses a neural network to compute edge-specific transformations\n",
    "- Perfect for molecules where bond types matter!\n\n",
    "### Why GRU?\n\n",
    "**GRU (Gated Recurrent Unit)** for state updates:\n",
    "- Handles iterative refinement of atom states\n",
    "- Prevents gradient vanishing\n",
    "- Learns when to keep vs. update information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"MPNN Implementation\", \"üß†\")\n\n",
    "from torch.nn import GRU\n\n",
    "class MPNN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim,\n",
    "        out_dim,\n",
    "        std,\n",
    "        train_data,\n",
    "        valid_data,\n",
    "        test_data,\n",
    "        batch_size=32,\n",
    "        lr=1e-3,\n",
    "        num_message_passing_steps=3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['train_data', 'valid_data', 'test_data'])\n",
    "        \n",
    "        # Store dataset references\n",
    "        self.std = std\n",
    "        self.train_data = train_data\n",
    "        self.valid_data = valid_data\n",
    "        self.test_data = test_data\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.num_mp_steps = num_message_passing_steps\n",
    "        \n",
    "        # Initial embedding layers (from OGB)\n",
    "        self.atom_emb = AtomEncoder(emb_dim=hidden_dim)\n",
    "        self.bond_emb = BondEncoder(emb_dim=hidden_dim)\n",
    "        \n",
    "        # Message passing layers\n",
    "        # NNConv: Neural Network Convolution with edge features\n",
    "        nn_layer = MLP([hidden_dim, hidden_dim*2, hidden_dim*hidden_dim])\n",
    "        self.conv = NNConv(hidden_dim, hidden_dim, nn_layer, aggr='mean')\n",
    "        \n",
    "        # GRU for node state updates\n",
    "        self.gru = GRU(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Readout layer (graph-level representation)\n",
    "        self.mlp = MLP([hidden_dim, int(hidden_dim/2), out_dim])\n",
    "    \n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        # Initialization: embed atom and bond features\n",
    "        x = self.atom_emb(data.x)  # [num_atoms, hidden_dim]\n",
    "        h = x.unsqueeze(0)  # [1, num_atoms, hidden_dim] for GRU\n",
    "        edge_attr = self.bond_emb(data.edge_attr)  # [num_edges, hidden_dim]\n",
    "        \n",
    "        # Message passing iterations\n",
    "        for i in range(self.num_mp_steps):\n",
    "            # 1. Generate and aggregate messages\n",
    "            m = F.relu(self.conv(x, data.edge_index, edge_attr))\n",
    "            \n",
    "            # 2. Update node states with GRU\n",
    "            x, h = self.gru(m.unsqueeze(0), h)\n",
    "            x = x.squeeze(0)\n",
    "        \n",
    "        # Readout: aggregate node features to graph-level\n",
    "        x = global_add_pool(x, data.batch)  # [num_graphs, hidden_dim]\n",
    "        \n",
    "        # Prediction\n",
    "        x = self.mlp(x)\n",
    "        \n",
    "        return x.view(-1)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        out = self.forward(batch, mode=\"train\")\n",
    "        loss = F.mse_loss(out, batch.y)\n",
    "        self.log(\"train_loss\", loss, batch_size=len(batch.y), prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        out = self.forward(batch, mode=\"valid\")\n",
    "        # Denormalize for meaningful metrics\n",
    "        loss = F.mse_loss(out * self.std, batch.y * self.std)\n",
    "        self.log(\"val_mse\", loss, batch_size=len(batch.y), prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        out = self.forward(batch, mode=\"test\")\n",
    "        # Denormalize for meaningful metrics\n",
    "        loss = F.mse_loss(out * self.std, batch.y * self.std)\n",
    "        self.log(\"test_mse\", loss, batch_size=len(batch.y))\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=10,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_mse'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return PyGDataLoader(self.train_data, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return PyGDataLoader(self.valid_data, batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return PyGDataLoader(self.test_data, batch_size=self.batch_size, shuffle=False)\n\n",
    "print(\"‚úì MPNN class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Create Model Instance\", \"üèóÔ∏è\")\n\n",
    "# Create model\n",
    "model = MPNN(\n",
    "    hidden_dim=64,\n",
    "    out_dim=1,\n",
    "    std=scaler.scale_[0],\n",
    "    train_data=train_graphs,\n",
    "    valid_data=val_graphs,\n",
    "    test_data=test_graphs,\n",
    "    lr=0.001,\n",
    "    batch_size=32,\n",
    "    num_message_passing_steps=3\n",
    ")\n\n",
    "# Model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n",
    "print(f\"Model created successfully!\")\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Hidden dimension: {model.hparams.hidden_dim}\")\n",
    "print(f\"  Message passing steps: {model.num_mp_steps}\")\n",
    "print(f\"  Learning rate: {model.lr}\")\n",
    "print(f\"  Batch size: {model.batch_size}\")\n",
    "\n",
    "print(f\"\\n‚úì Ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='training'></a>\n",
    "# 6. Training and Evaluation\n\n",
    "**‚è±Ô∏è Expected time:** 5-10 minutes (depending on hardware)\n\n",
    "Time to train our MPNN and evaluate its performance!\n\n",
    "## Training Setup\n\n",
    "We'll use PyTorch Lightning's Trainer with:\n",
    "- **ModelCheckpoint**: Save the best model based on validation loss\n",
    "- **EarlyStopping**: Stop training if validation loss doesn't improve\n",
    "- **LearningRateMonitor**: Track learning rate changes\n",
    "- **Optional W&B logging**: Track experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Setup Training\", \"üéØ\")\n\n",
    "# Define callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_mse',\n",
    "    dirpath='checkpoints',\n",
    "    filename='mpnn-{epoch:02d}-{val_mse:.4f}',\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    "    verbose=True\n",
    ")\n\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_mse',\n",
    "    patience=20,\n",
    "    mode='min',\n",
    "    verbose=True\n",
    ")\n\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n\n",
    "# Create trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback, lr_monitor],\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    log_every_n_steps=10,\n",
    "    enable_progress_bar=True,\n",
    "    deterministic=True\n",
    ")\n\n",
    "print(f\"‚úì Trainer configured\")\n",
    "print(f\"  Max epochs: 100\")\n",
    "print(f\"  Early stopping patience: 20 epochs\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Training MPNN\", \"üöÄ\")\n\n",
    "print(f\"Starting training...\\n\")\n",
    "print(estimate_runtime(torch.cuda.is_available()))\n",
    "print()\n\n",
    "# Train model\n",
    "trainer.fit(model)\n\n",
    "print(f\"\\n‚úì Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Test Set Evaluation\", \"üìà\")\n\n",
    "# Test the best model\n",
    "test_results = trainer.test(model, ckpt_path='best')\n\n",
    "# Get predictions on test set\n",
    "model.eval()\n",
    "test_loader = PyGDataLoader(test_graphs, batch_size=64, shuffle=False)\n\n",
    "all_preds = []\n",
    "all_targets = []\n\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        preds = model(batch)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(batch.y.cpu().numpy())\n\n",
    "y_pred = np.concatenate(all_preds)\n",
    "y_true = np.concatenate(all_targets)\n\n",
    "# Denormalize predictions\n",
    "y_pred_denorm = y_pred * scaler.scale_[0] + scaler.mean_[0]\n",
    "y_true_denorm = y_true * scaler.scale_[0] + scaler.mean_[0]\n\n",
    "# Calculate metrics\n",
    "r2 = r2_score(y_true_denorm, y_pred_denorm)\n",
    "rmse = np.sqrt(mean_squared_error(y_true_denorm, y_pred_denorm))\n",
    "mae = mean_absolute_error(y_true_denorm, y_pred_denorm)\n\n",
    "metrics = {\n",
    "    'R¬≤ Score': r2,\n",
    "    'RMSE': rmse,\n",
    "    'MAE': mae\n",
    "}\n\n",
    "print_metrics(metrics, \"Test Set Performance\")\n",
    "\n",
    "print(f\"\\n‚úì Model achieves R¬≤ = {r2:.4f} on test set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Visualize Results\", \"üìä\")\n\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n",
    "# Parity plot\n",
    "axes[0].scatter(y_true_denorm, y_pred_denorm, alpha=0.5, c=GOMES_COLORS['teal'], s=40, edgecolors='k', linewidth=0.5)\n",
    "axes[0].plot([y_true_denorm.min(), y_true_denorm.max()], \n",
    "             [y_true_denorm.min(), y_true_denorm.max()], \n",
    "             'r--', lw=2, label='Perfect prediction')\n",
    "axes[0].set_xlabel('True Log Solubility', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Predicted Log Solubility', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title(f'Parity Plot (R¬≤={r2:.3f})', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n\n",
    "# Residuals plot\n",
    "residuals = y_true_denorm - y_pred_denorm\n",
    "axes[1].scatter(y_pred_denorm, residuals, alpha=0.5, c=GOMES_COLORS['coral'], s=40, edgecolors='k', linewidth=0.5)\n",
    "axes[1].axhline(y=0, color='k', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted Log Solubility', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Residuals (True - Predicted)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title(f'Residuals Plot (MAE={mae:.3f})', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n\n",
    "plt.tight_layout()\n",
    "plt.show()\n\n",
    "print(\"‚úì The model shows good predictive performance!\")\n",
    "print(\"  - Points close to diagonal line = good predictions\")\n",
    "print(\"  - Residuals centered around 0 = unbiased predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='representations'></a>\n",
    "# 7. Understanding Learned Representations\n\n",
    "**‚è±Ô∏è Expected time:** 5 minutes\n\n",
    "Let's visualize what the GNN has learned by extracting and analyzing molecular embeddings!\n\n",
    "## What are Embeddings?\n\n",
    "Before making predictions, our MPNN creates a **fixed-size vector representation** for each molecule. These embeddings encode chemical information learned during training.\n\n",
    "We'll use **PCA (Principal Component Analysis)** to project high-dimensional embeddings to 2D for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Extract Graph Embeddings\", \"üé®\")\n\n",
    "# Modify forward pass to return embeddings\n",
    "def forward_with_embedding(model, data):\n",
    "    \"\"\"Modified forward pass that returns both prediction and embedding\"\"\"\n",
    "    # Initialization\n",
    "    x = model.atom_emb(data.x)\n",
    "    h = x.unsqueeze(0)\n",
    "    edge_attr = model.bond_emb(data.edge_attr)\n",
    "    \n",
    "    # Message passing\n",
    "    for i in range(model.num_mp_steps):\n",
    "        m = F.relu(model.conv(x, data.edge_index, edge_attr))\n",
    "        x, h = model.gru(m.unsqueeze(0), h)\n",
    "        x = x.squeeze(0)\n",
    "    \n",
    "    # Readout - this is our embedding!\n",
    "    embedding = global_add_pool(x, data.batch)\n",
    "    \n",
    "    # Prediction\n",
    "    pred = model.mlp(embedding)\n",
    "    \n",
    "    return pred.view(-1), embedding\n\n",
    "# Extract embeddings for all molecules\n",
    "model.eval()\n",
    "all_loader = PyGDataLoader(graph_list, batch_size=64, shuffle=False)\n\n",
    "all_embeddings = []\n",
    "all_targets = []\n\n",
    "print(\"Extracting embeddings...\")\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(all_loader):\n",
    "        batch = batch.to(device)\n",
    "        _, embeddings = forward_with_embedding(model, batch)\n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "        # Denormalize targets for visualization\n",
    "        targets_denorm = batch.y.cpu().numpy() * scaler.scale_[0] + scaler.mean_[0]\n",
    "        all_targets.append(targets_denorm)\n\n",
    "embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "targets = np.concatenate(all_targets, axis=0)\n\n",
    "print(f\"\\n‚úì Extracted embeddings\")\n",
    "print(f\"  Shape: {embeddings.shape}\")\n",
    "print(f\"  Embedding dimension: {embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"PCA Visualization\", \"üîç\")\n\n",
    "# Standardize embeddings before PCA\n",
    "from sklearn.preprocessing import StandardScaler as Scaler\n",
    "emb_scaler = Scaler()\n",
    "embeddings_scaled = emb_scaler.fit_transform(embeddings)\n\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings_scaled)\n\n",
    "print(f\"PCA Results:\")\n",
    "print(f\"  PC1 explains {pca.explained_variance_ratio_[0]:.2%} of variance\")\n",
    "print(f\"  PC2 explains {pca.explained_variance_ratio_[1]:.2%} of variance\")\n",
    "print(f\"  Total explained: {pca.explained_variance_ratio_.sum():.2%}\")\n\n",
    "# Create visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n\n",
    "scatter = ax.scatter(\n",
    "    embeddings_2d[:, 0],\n",
    "    embeddings_2d[:, 1],\n",
    "    c=targets,\n",
    "    cmap='viridis',\n",
    "    s=40,\n",
    "    alpha=0.6,\n",
    "    edgecolors='k',\n",
    "    linewidth=0.5\n",
    ")\n\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax.set_title('Molecular Embeddings in 2D (PCA)', fontsize=14, fontweight='bold')\n\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('Log Solubility (mol/L)', fontsize=12, fontweight='bold')\n\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n\n",
    "plt.tight_layout()\n",
    "plt.show()\n\n",
    "print(\"\\n‚úì Embedding Visualization Complete!\")\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"  - Similar molecules cluster together\")\n",
    "print(\"  - Color gradient shows solubility patterns\")\n",
    "print(\"  - GNN learned chemically meaningful representations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='chemprop'></a>\n",
    "# 8. Chemprop - Production-Ready GNNs\n\n",
    "**‚è±Ô∏è Expected time:** 10 minutes\n\n",
    "While building custom GNNs is educational, in production you often want battle-tested tools. Enter **Chemprop**!\n\n",
    "## What is Chemprop?\n\n",
    "**Chemprop** is a production-ready molecular property prediction framework developed at MIT:\n",
    "- **Optimized D-MPNN**: Directed message passing with extensive hyperparameter tuning\n",
    "- **Easy CLI**: Train models with a single command\n",
    "- **Uncertainty quantification**: Built-in ensemble and dropout methods\n",
    "- **Transfer learning**: Pre-trained models for low-data scenarios\n",
    "- **Well-tested**: Used in numerous drug discovery projects\n\n",
    "**Reference:** Yang et al. (2019). Analyzing Learned Molecular Representations for Property Prediction. *J. Chem. Inf. Model.*\n\n",
    "### When to Use Custom GNNs vs. Chemprop?\n\n",
    "**Custom GNNs (like our MPNN)**:\n",
    "- Research and experimentation\n",
    "- Novel architectures\n",
    "- Custom featurization\n",
    "- Learning and teaching\n\n",
    "**Chemprop**:\n",
    "- Production deployments\n",
    "- Quick baselines\n",
    "- Limited ML expertise\n",
    "- Proven reliability needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Chemprop Training\", \"‚ö°\")\n\n",
    "print(\"Chemprop v2.2.1 CLI Training\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"Basic training command:\")\n",
    "print()\n",
    "print(\"  !chemprop train \\\\\")\n",
    "print(\"      --data-path data/esol.csv \\\\\")\n",
    "print(\"      --task-type regression \\\\\")\n",
    "print(\"      --output-dir chemprop_output \\\\\")\n",
    "print(\"      --epochs 50 \\\\\")\n",
    "print(\"      --batch-size 50 \\\\\")\n",
    "print(\"      --split-type random \\\\\")\n",
    "print(\"      --split-sizes 0.7 0.1 0.2\")\n",
    "print()\n",
    "print(\"Prediction command:\")\n",
    "print()\n",
    "print(\"  !chemprop predict \\\\\")\n",
    "print(\"      --model-path chemprop_output/model.pt \\\\\")\n",
    "print(\"      --data-path test.csv \\\\\")\n",
    "print(\"      --output-path predictions.csv\")\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"Note: Chemprop training is commented out to keep notebook runtime short.\")\n",
    "print(\"Uncomment the commands above to train a Chemprop model!\")\n",
    "print()\n",
    "print(\"Expected performance: R¬≤ ~ 0.90-0.92 (slightly better than our MPNN)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chemprop Advanced Features\n\n",
    "**Ensemble Models**:\n",
    "```bash\n",
    "chemprop train \\\n",
    "    --data-path data/esol.csv \\\n",
    "    --ensemble-size 5  # Train 5 models for uncertainty\n",
    "```\n\n",
    "**Hyperparameter Optimization**:\n",
    "```bash\n",
    "chemprop hyper opt\\\n",
    "    --data-path data/esol.csv \\\n",
    "    --num-iters 20\n",
    "```\n\n",
    "**Transfer Learning**:\n",
    "```bash\n",
    "chemprop train \\\n",
    "    --data-path small_dataset.csv \\\n",
    "    --checkpoint-path pretrained_model.pt  # Fine-tune\n",
    "```\n\n",
    "**Interpretability**:\n",
    "```bash\n",
    "chemprop interpret \\\n",
    "    --model-path model.pt \\\n",
    "    --data-path molecules.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='debugging'></a>\n",
    "# 9. Common Pitfalls and Debugging\n\n",
    "**‚è±Ô∏è Expected time:** 5 minutes\n\n",
    "GNN training can be tricky! Here are common issues and solutions.\n\n",
    "## Issue 1: GPU Not Detected\n\n",
    "**Symptom**: Model trains on CPU (very slow)\n\n",
    "**Solution**:\n",
    "1. In Colab: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\n",
    "2. Check `torch.cuda.is_available()` returns `True`\n",
    "3. Restart runtime if needed\n\n",
    "## Issue 2: CUDA Out of Memory\n\n",
    "**Symptom**: `RuntimeError: CUDA out of memory`\n\n",
    "**Solutions**:\n",
    "```python\n",
    "# 1. Reduce batch size\n",
    "model = MPNN(..., batch_size=16)  # Instead of 32\n\n",
    "# 2. Reduce model size\n",
    "model = MPNN(hidden_dim=32, ...)  # Instead of 64\n\n",
    "# 3. Use gradient accumulation\n",
    "trainer = pl.Trainer(accumulate_grad_batches=2)\n",
    "```\n\n",
    "## Issue 3: Model Not Learning (High Loss)\n\n",
    "**Symptom**: Validation loss stays constant or very high\n\n",
    "**Solutions**:\n",
    "```python\n",
    "# 1. Check target normalization\n",
    "print(f\"Target mean: {y_train.mean()}, std: {y_train.std()}\")\n",
    "# Should be close to 0 and 1 after normalization\n\n",
    "# 2. Increase learning rate\n",
    "model = MPNN(..., lr=0.01)  # Try 10x higher\n\n",
    "# 3. Check for NaN values\n",
    "print(f\"NaN in data: {torch.isnan(batch.y).sum()}\")\n",
    "```\n\n",
    "## Issue 4: Overfitting\n\n",
    "**Symptom**: Train loss << validation loss\n\n",
    "**Solutions**:\n",
    "```python\n",
    "# 1. Add dropout\n",
    "self.dropout = nn.Dropout(0.2)\n\n",
    "# 2. Reduce model capacity\n",
    "model = MPNN(hidden_dim=32, num_message_passing_steps=2)\n\n",
    "# 3. Use weight decay\n",
    "optimizer = torch.optim.Adam(params, lr=0.001, weight_decay=1e-5)\n\n",
    "# 4. More data augmentation\n",
    "# SMILES enumeration, scaffold splitting, etc.\n",
    "```\n\n",
    "## Issue 5: Slow Training\n\n",
    "**Solutions**:\n",
    "```python\n",
    "# 1. Use DataLoader num_workers\n",
    "loader = PyGDataLoader(dataset, batch_size=32, num_workers=4)\n\n",
    "# 2. Mixed precision training\n",
    "trainer = pl.Trainer(precision='16-mixed')\n\n",
    "# 3. Compile model (PyTorch 2.0+)\n",
    "model = torch.compile(model)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Debugging Checklist\", \"üîß\")\n\n",
    "print(\"Run this cell to check your setup:\\n\")\n\n",
    "# 1. Device check\n",
    "print(f\"1. Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   ‚úì GPU available\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  CPU only (training will be slow)\")\n",
    "\n",
    "# 2. Data check\n",
    "print(f\"\\n2. Data:\")\n",
    "print(f\"   Train samples: {len(train_graphs)}\")\n",
    "print(f\"   Val samples: {len(val_graphs)}\")\n",
    "print(f\"   Test samples: {len(test_graphs)}\")\n",
    "\n",
    "# 3. Normalization check\n",
    "train_targets_norm = np.array([g.y.item() for g in train_graphs])\n",
    "print(f\"\\n3. Target normalization:\")\n",
    "print(f\"   Mean: {train_targets_norm.mean():.3f} (should be ~0)\")\n",
    "print(f\"   Std: {train_targets_norm.std():.3f} (should be ~1)\")\n",
    "if abs(train_targets_norm.mean()) < 0.1 and abs(train_targets_norm.std() - 1) < 0.1:\n",
    "    print(f\"   ‚úì Targets properly normalized\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Check normalization!\")\n",
    "\n",
    "# 4. Model check\n",
    "print(f\"\\n4. Model:\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   Device: {next(model.parameters()).device}\")\n",
    "\n",
    "# 5. Batch check\n",
    "print(f\"\\n5. Data loading:\")\n",
    "test_loader = PyGDataLoader(train_graphs[:10], batch_size=5)\n",
    "test_batch = next(iter(test_loader))\n",
    "print(f\"   Batch size: {test_batch.num_graphs}\")\n",
    "print(f\"   Total nodes: {test_batch.num_nodes}\")\n",
    "print(f\"   Total edges: {test_batch.num_edges}\")\n",
    "print(f\"   ‚úì DataLoader working\")\n",
    "\n",
    "print(f\"\\n‚úì All checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='extensions'></a>\n",
    "# 10. Extensions and Next Steps\n\n",
    "**‚è±Ô∏è Expected time:** 15-20 minutes (optional advanced topics)\n\n",
    "Congratulations on completing the core tutorial! Here are directions for further exploration.\n\n",
    "## 10.1 Advanced GNN Architectures\n\n",
    "### Graph Attention Networks (GAT)\n",
    "- Learn attention weights for neighbor aggregation\n",
    "- More interpretable than standard MPNNs\n",
    "- Implementation: `from torch_geometric.nn import GATConv`\n\n",
    "### Graph Isomorphism Networks (GIN)\n",
    "- Maximally expressive GNN architecture\n",
    "- Theoretical guarantees on distinguishing graphs\n",
    "- Implementation: `from torch_geometric.nn import GINConv`\n\n",
    "### Equivariant GNNs (E(3)-GNN, SchNet, DimeNet)\n",
    "- Incorporate 3D molecular geometry\n",
    "- Respect physical symmetries\n",
    "- Better for quantum properties\n\n",
    "## 10.2 Hyperparameter Optimization\n\n",
    "Use Bayesian Optimization to find best hyperparameters:\n",
    "```python\n",
    "from bayes_opt import BayesianOptimization\n\n",
    "def train_and_evaluate(hidden_dim, lr, num_mp_steps):\n",
    "    model = MPNN(\n",
    "        hidden_dim=int(hidden_dim),\n",
    "        lr=lr,\n",
    "        num_message_passing_steps=int(num_mp_steps),\n",
    "        ...\n",
    "    )\n",
    "    trainer = pl.Trainer(max_epochs=30)\n",
    "    trainer.fit(model)\n",
    "    results = trainer.test(model)\n",
    "    return -results[0]['test_mse']  # Negative because we maximize\n\n",
    "optimizer = BayesianOptimization(\n",
    "    f=train_and_evaluate,\n",
    "    pbounds={\n",
    "        'hidden_dim': (32, 128),\n",
    "        'lr': (0.0001, 0.01),\n",
    "        'num_mp_steps': (2, 5)\n",
    "    }\n",
    ")\n",
    "\n",
    "optimizer.maximize(n_iter=20)\n",
    "```\n\n",
    "## 10.3 Uncertainty Quantification\n\n",
    "### Ensemble Methods\n",
    "Train multiple models with different initializations:\n",
    "```python\n",
    "ensemble = []\n",
    "for i in range(5):\n",
    "    set_seed(42 + i)\n",
    "    model = MPNN(...)\n",
    "    trainer.fit(model)\n",
    "    ensemble.append(model)\n\n",
    "# Predict with ensemble\n",
    "predictions = [model(batch) for model in ensemble]\n",
    "mean_pred = torch.stack(predictions).mean(dim=0)\n",
    "std_pred = torch.stack(predictions).std(dim=0)  # Uncertainty!\n",
    "```\n\n",
    "### MC Dropout\n",
    "Enable dropout during inference:\n",
    "```python\n",
    "def predict_with_uncertainty(model, batch, n_samples=50):\n",
    "    model.train()  # Enable dropout\n",
    "    predictions = [model(batch) for _ in range(n_samples)]\n",
    "    model.eval()\n",
    "    return torch.stack(predictions).mean(dim=0), torch.stack(predictions).std(dim=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Larger Datasets\n\n",
    "### QM9\n",
    "- 134k organic molecules\n",
    "- 12 quantum properties\n",
    "- `from torch_geometric.datasets import QM9`\n\n",
    "### ZINC\n",
    "- 250k drug-like molecules\n",
    "- Molecular generation benchmark\n",
    "\n",
    "### MoleculeNet\n",
    "- Collection of benchmark datasets\n",
    "- Various prediction tasks\n",
    "- Available through DeepChem\n\n",
    "## 10.5 Transfer Learning\n\n",
    "Pre-train on large dataset, fine-tune on small:\n",
    "```python\n",
    "# 1. Pre-train on large dataset\n",
    "pretrain_model = MPNN(...)\n",
    "pretrain_trainer.fit(pretrain_model, large_dataset)\n\n",
    "# 2. Fine-tune on small target dataset\n",
    "finetuned_model = MPNN.load_from_checkpoint('pretrained.ckpt')\n",
    "finetuned_model.lr = 0.0001  # Lower learning rate\n",
    "finetune_trainer.fit(finetuned_model, small_dataset)\n",
    "```\n\n",
    "## 10.6 Interpretability and Explainability\n\n",
    "### GNN-Explainer\n",
    "Identify important atoms and bonds:\n",
    "```python\n",
    "from torch_geometric.explain import Explainer, GNNExplainer\n\n",
    "explainer = Explainer(\n",
    "    model=model,\n",
    "    algorithm=GNNExplainer(epochs=200),\n",
    "    explanation_type='model',\n",
    "    node_mask_type='attributes',\n",
    "    edge_mask_type='object'\n",
    ")\n\n",
    "explanation = explainer(data.x, data.edge_index)\n",
    "```\n\n",
    "### Attention Visualization\n",
    "For GAT models, visualize learned attention weights\n\n",
    "## 10.7 Production Deployment\n",
    "\n",
    "### Model Export\n",
    "```python\n",
    "# TorchScript (recommended)\n",
    "traced_model = torch.jit.trace(model, example_batch)\n",
    "traced_model.save('model.pt')\n\n",
    "# ONNX\n",
    "torch.onnx.export(model, example_batch, 'model.onnx')\n",
    "```\n\n",
    "### API Deployment\n",
    "Use FastAPI or Flask to serve predictions:\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(smiles: str):\n",
    "    graph = molecule_to_graph_data(smiles)\n",
    "    pred = model(graph)\n",
    "    return {\"solubility\": float(pred)}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n\n",
    "### Papers\n",
    "1. **Gilmer et al. (2017)**: Neural Message Passing for Quantum Chemistry\n",
    "2. **Yang et al. (2019)**: Analyzing Learned Molecular Representations (Chemprop)\n",
    "3. **Xu et al. (2019)**: How Powerful are Graph Neural Networks? (GIN)\n",
    "4. **Veliƒçkoviƒá et al. (2018)**: Graph Attention Networks (GAT)\n",
    "5. **Sch√ºtt et al. (2017)**: SchNet - 3D molecular modeling\n\n",
    "### Courses and Tutorials\n",
    "- **Stanford CS224W**: Machine Learning with Graphs\n",
    "- **PyG Tutorials**: https://pytorch-geometric.readthedocs.io/\n",
    "- **DeepChem**: https://deepchem.io/\n",
    "- **AI4Chem Course**: https://github.com/schwallergroup/ai4chem_course\n\n",
    "### Software Libraries\n",
    "- **PyTorch Geometric**: https://pytorch-geometric.readthedocs.io/\n",
    "- **Chemprop**: https://github.com/chemprop/chemprop\n",
    "- **DeepChem**: https://deepchem.io/\n",
    "- **RDKit**: https://www.rdkit.org/\n",
    "- **OGB**: https://ogb.stanford.edu/\n\n",
    "### Research Groups\n",
    "- **Gomes Group (CMU)**: Molecular machine learning and retrosynthesis\n",
    "- **Coley Group (MIT)**: Chemprop, reaction prediction\n",
    "- **Schwaller Group (EPFL)**: Transformer models for chemistry\n",
    "- **Isayev Group (CMU)**: ANI neural network potentials\n\n",
    "---\n\n",
    "## Congratulations!\n\n",
    "You've completed the GNN Molecular Property Prediction tutorial!\n\n",
    "### What You've Learned:\n",
    "‚úì Why graphs are natural for molecules\n",
    "‚úì How to convert SMILES to graph representations\n",
    "‚úì Message-passing neural network theory\n",
    "‚úì Implementing MPNNs with PyTorch Geometric\n",
    "‚úì Training and evaluating GNN models\n",
    "‚úì Visualizing learned representations\n",
    "‚úì Using production tools like Chemprop\n",
    "‚úì Debugging common issues\n",
    "‚úì Paths for further exploration\n\n",
    "### Next Steps:\n",
    "1. Try different molecular properties (QM9, MoleculeNet)\n",
    "2. Experiment with different GNN architectures\n",
    "3. Apply to your own molecular datasets\n",
    "4. Explore 3D equivariant models\n",
    "5. Implement uncertainty quantification\n",
    "6. Deploy models in production\n\n",
    "**Happy molecule learning!**\n\n",
    "---\n\n",
    "*Tutorial developed by the Gomes Research Group at CMU*  \n",
    "*Questions? Open an issue on GitHub or contact the course staff*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}